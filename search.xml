<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Kaggle猫狗识别Pytorch详细搭建过程</title>
    <url>/2020-04-26-Kaggle%E7%8C%AB%E7%8B%97%E8%AF%86%E5%88%ABPytorch%E8%AF%A6%E7%BB%86%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B.html</url>
    <content><![CDATA[<p><a href="https://github.com/BraveY/AI-with-code/tree/master/dog-vs-cat">文章源码链接</a>，包括Notebook和对应的Pycharm项目。求个Star！！</p>
<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p><a href="https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition?rvi=1">Kaggle比赛链接</a> ,给出猫狗图片，然后预测图片中是狗的概率。</p>
<p>训练集有25,000张图片，测试集12,500 张图片。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1587879395/blog/deep-learning/CatAndDog.jpg" alt=""></p>
<p>自己最开始构思大致框架的时候的一个思维导图：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1587879936/blog/deep-learning/CatAndDog_xmid.jpg" alt=""></p>
<h2 id="包的导入"><a href="#包的导入" class="headerlink" title="包的导入"></a>包的导入</h2><p>需要注意的是将tqdm 改为tqdm.notebook，从而在notebook环境下获得更好的体验。因为导入tqdm的话，会发生进度条打印多次的情况，体验很不好</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms,models,utils</span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># from tqdm import tqdm_notebook as tqdm</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter </span><br><span class="line"><span class="comment"># from torchvision import datasets, transforms,utils</span></span><br></pre></td></tr></table></figure>
<p>相关文件路径配置，在pycharm项目中将相关路径的配置都统一放在config.py中来管理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_path = <span class="string">&#x27;D:/AIdata/dog vs cat/train&#x27;</span></span><br><span class="line">test_path = <span class="string">&#x27;D:/AIdata/dog vs cat/test1&#x27;</span></span><br><span class="line">data_root = <span class="string">&#x27;D:/AIdata/dog vs cat/&#x27;</span></span><br><span class="line">csv_path = <span class="string">&#x27;./submission_valnet.csv&#x27;</span></span><br><span class="line">tensorboard_path=<span class="string">&#x27;C:/Users/BraveY/Documents/BraveY/AI-with-code/dog-vs-cat/tensortboard&#x27;</span></span><br><span class="line">model_save_path = <span class="string">&#x27;C:/Users/BraveY/Documents/BraveY/AI-with-code/dog-vs-cat/modelDict/dogs-vs-cats-notebook.pth&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="数据集的创建"><a href="#数据集的创建" class="headerlink" title="数据集的创建"></a>数据集的创建</h2><p>因为Kaggle官方提供的是<a href="https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data">原始数据</a>，不像之前的手写数字数据集可以从pytorch中直接下载已经处理过的数据集，可以直接将数据放入模型进行训练。因此需要我们自己实现数据集的生成。</p>
<p>数据集生成的总体思路是继承torch.utils.data.Dataset这个类，自己实现<strong>getitem</strong>和<strong>len</strong>这两个私有方法来完成对我们自己数据的读取操作。其中<strong>getitem</strong>这个函数的主要功能是根据样本的索引，返回索引对应的一张图片的图像数据X与对应的标签Y，也就是返回一个对应的训练样本。<strong>len</strong>这个函数的功能比较简单直接返回数据集中样本的个数即可。</p>
<p>具体而言，<strong>getitem</strong>的实现思路比较简单，将索引idx转换为图片的路径，然后用PIL的Image包来读取图片数据，然后将数据用torchvision的transforms转换成tensor并且进行Resize来统一大小（给出的图片尺寸不一致）与归一化，这样一来就可以得到图像数据了。因为训练集中图片的文件名上面带有猫狗的标签，所以标签可以通过对图片文件名split后得到然后转成0,1编码。</p>
<p>在获取标签的时候，因为官方提供的测试数据集中并没有猫狗的标签，所以测试集的标签逻辑稍有不同。我的做法是使用一个train标志来进行区分，对于测试的数据，直接将测试样本的标签变成图片自带的id，这样方便后面输出提交的csv文件。因为测试样本不用计算loss，所以将标签置为id是没问题的。</p>
<p>为了实现将idx索引转换成图片路径，需要在<strong>init</strong>()函数中将所有的图片路径放在一个list中，这可以用os.listdir()来实现，然后就可以根据索引去获得路径了。</p>
<p>需要注意的是，之所以<strong>getitem</strong>()需要根据索引来返回样本，是因为训练数据并不是一次性将所有样本数据加载到内存中，这样太耗内存。而是只用加载对应batch中的一部分数据，所以通过索引来加载送入模型中的一批数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_path:<span class="built_in">str</span>, train=<span class="literal">True</span>, transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.data_path = data_path</span><br><span class="line">        self.train_flag = train</span><br><span class="line">        <span class="keyword">if</span> transform <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.transform = transforms.Compose(</span><br><span class="line">            [</span><br><span class="line">                transforms.Resize(size = (<span class="number">224</span>,<span class="number">224</span>)),<span class="comment">#尺寸规范</span></span><br><span class="line">                transforms.ToTensor(),   <span class="comment">#转化为tensor</span></span><br><span class="line">                transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),</span><br><span class="line">            ])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.transform = transform</span><br><span class="line">        self.path_list = os.listdir(data_path)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx: <span class="built_in">int</span></span>):</span></span><br><span class="line">        <span class="comment"># img to tensor and label to tensor</span></span><br><span class="line">        img_path = self.path_list[idx]</span><br><span class="line">        <span class="keyword">if</span> self.train_flag <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">if</span> img_path.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>] == <span class="string">&#x27;dog&#x27;</span> : </span><br><span class="line">                label = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            label = <span class="built_in">int</span>(img_path.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]) <span class="comment"># split 的是str类型要转换为int</span></span><br><span class="line">        label = torch.as_tensor(label, dtype=torch.int64) <span class="comment"># 必须使用long 类型数据，否则后面训练会报错 expect long</span></span><br><span class="line">        img_path = os.path.join(self.data_path, img_path)</span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">        img = self.transform(img)</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>) -&gt; int:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.path_list)</span><br></pre></td></tr></table></figure>
<p>测试一下，确保Dataset可以正常迭代</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_ds = MyDataset(train_path)</span><br><span class="line">test_ds = MyDataset(test_path,train=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(tqdm(train_ds)):</span><br><span class="line"><span class="comment">#     pass</span></span><br><span class="line">    print(item)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>(tensor([[[ 0.5922,  0.6078,  0.6392,  ...,  0.9216,  0.8902,  0.8745],
         [ 0.5922,  0.6078,  0.6392,  ...,  0.9216,  0.8980,  0.8824],
         [ 0.5922,  0.6078,  0.6392,  ...,  0.9216,  0.9059,  0.8902],
         ...,
         [ 0.2078,  0.2157,  0.2235,  ..., -0.9765, -0.9765, -0.9765],
         [ 0.2000,  0.2000,  0.2078,  ..., -0.9843, -0.9843, -0.9843],
         [ 0.1843,  0.1922,  0.2000,  ..., -0.9922, -0.9922, -0.9922]],

        [[ 0.2863,  0.3020,  0.3333,  ...,  0.6000,  0.5843,  0.5686],
         [ 0.2863,  0.3020,  0.3333,  ...,  0.6000,  0.5922,  0.5765],
         [ 0.2863,  0.3020,  0.3333,  ...,  0.6000,  0.6000,  0.5843],
         ...,
         [-0.0353, -0.0275, -0.0196,  ..., -0.9765, -0.9765, -0.9765],
         [-0.0431, -0.0431, -0.0353,  ..., -0.9843, -0.9843, -0.9843],
         [-0.0588, -0.0510, -0.0431,  ..., -0.9922, -0.9922, -0.9922]],

        [[-0.3176, -0.3020, -0.2706,  ..., -0.0588, -0.0431, -0.0510],
         [-0.3176, -0.3020, -0.2706,  ..., -0.0510, -0.0431, -0.0431],
         [-0.3176, -0.3020, -0.2706,  ..., -0.0431, -0.0275, -0.0353],
         ...,
         [-0.5608, -0.5529, -0.5451,  ..., -0.9922, -0.9922, -0.9922],
         [-0.5686, -0.5686, -0.5608,  ..., -1.0000, -1.0000, -1.0000],
         [-0.5843, -0.5765, -0.5686,  ..., -1.0000, -1.0000, -1.0000]]]), tensor(0))
</code></pre><h2 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h2><p>如前面所述，因为官方测试集没有标签，而且提交结果上去后只有一个log loos来作为分值，没有准确率的结果。所以为了得到准确率这个指标，需要新建个有标签的验证集来查看准确率。</p>
<p>实现思路是使用torch.utils.data.random_split(),来将官方提供训练数据集划分出一部分的验证集。我的比例是80%的训练集，20%的验证集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">full_ds = train_ds</span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(full_ds))</span><br><span class="line">validate_size = <span class="built_in">len</span>(full_ds) - train_size</span><br><span class="line">new_train_ds, validate_ds = torch.utils.data.random_split(full_ds,[train_size, validate_size])<span class="comment">#数据集划分</span></span><br></pre></td></tr></table></figure>
<h2 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h2><p>我们制作的数据集并不能直接放入模型进行训练，还需要使用一个数据加载器，来加载数据集。使用torch.utils.data.DataLoader()来划分每个batch用来后面训练的时候向网络提供输入数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loader = torch.utils.data.DataLoader(train_ds, batch_size=<span class="number">32</span>,</span><br><span class="line">                                            shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(test_ds, batch_size=<span class="number">32</span>,</span><br><span class="line">                                            shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"><span class="comment">## numworkers设置不为0 会报错 Broken pipe Error 网上说是win10上的pytorch bug</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_train_loader = torch.utils.data.DataLoader(new_train_ds, batch_size=<span class="number">32</span>,</span><br><span class="line">                                            shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">validate_loader = torch.utils.data.DataLoader(validate_ds, batch_size=<span class="number">32</span>,</span><br><span class="line">                                            shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>加载过后数据形状从三维变成四维，多的维度是batch_size，这里是32个样本构成一个batch</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line"><span class="comment">#     pass</span></span><br><span class="line">    print(item[<span class="number">0</span>].shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>torch.Size([32, 3, 224, 224])
</code></pre><h3 id="resize后的图像查看"><a href="#resize后的图像查看" class="headerlink" title="resize后的图像查看"></a>resize后的图像查看</h3><p>前面提到过对数据进行了resize和正则化的处理，下面是对处理后的图像的可视化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_PIL_Tensor = train_ds[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">new_img_PIL = transforms.ToPILImage()(img_PIL_Tensor).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">plt.imshow(new_img_PIL)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># print(new_img_PIL.show())</span></span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1587879395/blog/deep-learning/output_16_0.png" alt="png"></p>
<p>​<br>​    </p>
<h2 id="网络搭建"><a href="#网络搭建" class="headerlink" title="网络搭建"></a>网络搭建</h2><p>网络搭建的框架与之前的<a href="https://zhuanlan.zhihu.com/p/112829371">手写数字识别</a>的框架一致，两个卷积层后3个全连接层。需要注意的是参数不能套用之前的参数了，因为之前的手写数字的图片很小，而且数据量不大所以尽管参数比较大，也能在我的机子上跑起来(MX150,2GB显存)。猫狗的数据量显然比之前的大，所以需要将参数变小些，才能跑起来。 我实验了下，如果不将网络参数降低的话，只调整batch_size没有用，依然会报显存不足。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyCNN,self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">8</span>,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>) <span class="comment"># 按照公式计算后经过卷积层不改变尺寸</span></span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>) <span class="comment"># 2*2的池化 池化后size 减半</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">8</span>,<span class="number">16</span>,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">56</span>*<span class="number">56</span>,<span class="number">256</span>)<span class="comment">#两个池化，所以是224/2/2=56</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">256</span>,<span class="number">64</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">64</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#         self.dp = nn.Dropout(p=0.5)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line"><span class="comment">#         print(&quot;input:&quot;, x)</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line"><span class="comment">#         print(&quot;first conv:&quot;, x)</span></span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line"><span class="comment">#         print(&quot;second conv:&quot;, x)</span></span><br><span class="line">             </span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">56</span>* <span class="number">56</span>)<span class="comment">#将数据平整为一维的 </span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))  </span><br><span class="line">        x = self.fc3(x)  </span><br><span class="line"><span class="comment">#         x = F.log_softmax(x,dim=1) NLLLoss()才需要，交叉熵不需要</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line">   </span><br></pre></td></tr></table></figure>
<h3 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h3><p>除了自己手动DIY一个网络，也可以使用Pytorch已经提供的一些性能很好的模型比如VGG16，ResNet50等等，然后微调下网络结构，来得到符合自己的任务的网络架构。还可以直接下载这些模型在ImageNet上的预训练参数，然后在自己的数据集上进行训练。</p>
<p>我在这儿选择了ResNet50网络以及预训练好的权重进行了下实验，我在实验室的机器上面用P100跑的，因为自己的笔记本显卡太垃圾了只有2GB显存。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 直接设置为True的话下载权重太慢了</span></span><br><span class="line"><span class="comment">## 所以手动用浏览器下载好了之后再重新加载</span></span><br><span class="line">resnet50 = models.resnet50(pretrained=<span class="literal">False</span>)  </span><br><span class="line">model_path = <span class="string">&#x27;D:/AIdata/dog vs cat/resnet50-19c8e357.pth&#x27;</span></span><br><span class="line">resnet50.load_state_dict(torch.load(model_path))</span><br><span class="line">resnet50.fc = nn.Linear(<span class="number">2048</span>, <span class="number">2</span>) <span class="comment">#修改最后一层网络将输出调整为两维</span></span><br></pre></td></tr></table></figure>
<h2 id="损失函数和优化函数"><a href="#损失函数和优化函数" class="headerlink" title="损失函数和优化函数"></a>损失函数和优化函数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = MyCNN()</span><br><span class="line"><span class="comment"># net = resnet50</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># criterion = nn.BCELoss()  #二分类交叉熵损失函数</span></span><br><span class="line"><span class="comment"># criterion = nn.BCEWithLogitsLoss() #二分类交叉熵损失函数 带log loss</span></span><br><span class="line"><span class="comment"># criterion = nn.MSELoss()</span></span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment">#也可以选择Adam优化方法</span></span><br><span class="line"><span class="comment"># optimizer = torch.optim.Adam(net.parameters(),lr=1e-2)   </span></span><br></pre></td></tr></table></figure>
<h2 id="训练日志的打印"><a href="#训练日志的打印" class="headerlink" title="训练日志的打印"></a>训练日志的打印</h2><p>在之前的手写数字识别的准确率的计算和画图以日志的打印比较简单，在这更新为topk准确率以及使用tensorboard来画曲线。并且使用tqdm进度条来实时的打印日志。</p>
<p>专门建立一个类来保存和更新准确率的结果，使用类来让代码更加的规范化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AvgrageMeter</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.reset()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.avg = <span class="number">0</span></span><br><span class="line">        self.<span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">        self.cnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, val, n=<span class="number">1</span></span>):</span></span><br><span class="line">        self.<span class="built_in">sum</span> += val * n</span><br><span class="line">        self.cnt += n</span><br><span class="line">        self.avg = self.<span class="built_in">sum</span> / self.cnt</span><br></pre></td></tr></table></figure>
<h2 id="准确率的计算"><a href="#准确率的计算" class="headerlink" title="准确率的计算"></a>准确率的计算</h2><p>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor) 返回某一维度前k个的索引<br>input：一个tensor数据<br>k：指明是得到前k个数据以及其index<br>dim： 指定在哪个维度上排序， 默认是最后一个维度<br>largest：如果为True，按照大到小排序； 如果为False，按照小到大排序<br>sorted：返回的结果按照顺序返回<br>out：可缺省，不要</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## topk的准确率计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">output, label, topk=(<span class="params"><span class="number">1</span>,</span>)</span>):</span></span><br><span class="line">    maxk = <span class="built_in">max</span>(topk) </span><br><span class="line">    batch_size = label.size(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取前K的索引</span></span><br><span class="line">    _, pred = output.topk(maxk, <span class="number">1</span>, <span class="literal">True</span>, <span class="literal">True</span>) <span class="comment">#使用topk来获得前k个的索引</span></span><br><span class="line">    pred = pred.t() <span class="comment"># 进行转置</span></span><br><span class="line">    <span class="comment"># eq按照对应元素进行比较 view(1,-1) 自动转换到行为1,的形状， expand_as(pred) 扩展到pred的shape</span></span><br><span class="line">    <span class="comment"># expand_as 执行按行复制来扩展，要保证列相等</span></span><br><span class="line">    correct = pred.eq(label.view(<span class="number">1</span>, -<span class="number">1</span>).expand_as(pred)) <span class="comment"># 与正确标签序列形成的矩阵相比，生成True/False矩阵</span></span><br><span class="line"><span class="comment">#     print(correct)</span></span><br><span class="line"></span><br><span class="line">    rtn = []</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> topk:</span><br><span class="line">        correct_k = correct[:k].view(-<span class="number">1</span>).<span class="built_in">float</span>().<span class="built_in">sum</span>(<span class="number">0</span>) <span class="comment"># 前k行的数据 然后平整到1维度，来计算true的总个数</span></span><br><span class="line">        rtn.append(correct_k.mul_(<span class="number">100.0</span> / batch_size)) <span class="comment"># mul_() ternsor 的乘法  正确的数目/总的数目 乘以100 变成百分比</span></span><br><span class="line">    <span class="keyword">return</span> rtn</span><br></pre></td></tr></table></figure>
<h3 id="tensorboard画图"><a href="#tensorboard画图" class="headerlink" title="tensorboard画图"></a>tensorboard画图</h3><p>详细的参数讲解参考：<a href="https://www.pytorchtutorial.com/pytorch-builtin-tensorboard/">https://www.pytorchtutorial.com/pytorch-builtin-tensorboard/</a><br>在使用pip install安装tensorboard如果速度很慢经常断线的话可以换个国内的源：<br><code>pip config set global.index-url https://mirrors.ustc.edu.cn/pypi/web/simple</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter </span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./tensortboard/&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>画图的结果是实时，还可以放大放小，曲线的平滑度设置等，比自己写的画图函数要方便很多：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1587879396/blog/deep-learning/tensorboard.jpg" alt=""></p>
<h2 id="迭代训练"><a href="#迭代训练" class="headerlink" title="迭代训练"></a>迭代训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params"> epoch, train_loader, device, model, criterion, optimizer,tensorboard_path</span>):</span></span><br><span class="line">    model = model.to(device)</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">         model.train()</span><br><span class="line">    	top1 = AvgrageMeter()</span><br><span class="line">        train_loss = <span class="number">0.0</span></span><br><span class="line">        train_loader = tqdm(train_loader)  <span class="comment">#转换成tqdm类型 以方便增加日志的输出</span></span><br><span class="line">        train_loader.set_description(<span class="string">&#x27;[%s%04d/%04d %s%f]&#x27;</span> % (<span class="string">&#x27;Epoch:&#x27;</span>, e + <span class="number">1</span>, epoch, <span class="string">&#x27;lr:&#x27;</span>, <span class="number">0.001</span>))</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):  <span class="comment"># 0是下标起始位置默认为0</span></span><br><span class="line">            inputs, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line">            <span class="comment"># 初始为0，清除上个batch的梯度信息</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            loss = criterion(outputs,labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="comment"># topk 准确率计算</span></span><br><span class="line">            prec1, prec2 = accuracy(outputs, labels, topk=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">            n = inputs.size(<span class="number">0</span>)</span><br><span class="line">            top1.update(prec1.item(), n)</span><br><span class="line">            train_loss += loss.item()</span><br><span class="line">            postfix = &#123;<span class="string">&#x27;train_loss&#x27;</span>: <span class="string">&#x27;%.6f&#x27;</span> % (train_loss / (i + <span class="number">1</span>)), <span class="string">&#x27;train_acc&#x27;</span>: <span class="string">&#x27;%.6f&#x27;</span> % top1.avg&#125;</span><br><span class="line">            train_loader.set_postfix(log=postfix)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ternsorboard 曲线绘制</span></span><br><span class="line">            writer = SummaryWriter(tensorboard_path)</span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;Train/Loss&#x27;</span>, loss.item(), epoch)</span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;Train/Accuracy&#x27;</span>, top1.avg, epoch)</span><br><span class="line">            writer.flush()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><p>准确率验证<br>在验证集上面的验证，求网络的的准确率指标</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validate</span>(<span class="params">validate_loader, device, model, criterion</span>):</span></span><br><span class="line">    val_acc = <span class="number">0.0</span></span><br><span class="line">    model = model.to(device)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 进行评测的时候网络不更新梯度</span></span><br><span class="line">        val_top1 = AvgrageMeter()</span><br><span class="line">        validate_loader = tqdm(validate_loader)</span><br><span class="line">        validate_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(validate_loader, <span class="number">0</span>):  <span class="comment"># 0是下标起始位置默认为0</span></span><br><span class="line">            inputs, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line">            <span class="comment">#         inputs,labels = data[0],data[1]</span></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">            prec1, prec2 = accuracy(outputs, labels, topk=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">            n = inputs.size(<span class="number">0</span>)</span><br><span class="line">            val_top1.update(prec1.item(), n)</span><br><span class="line">            validate_loss += loss.item()</span><br><span class="line">            postfix = &#123;<span class="string">&#x27;validate_loss&#x27;</span>: <span class="string">&#x27;%.6f&#x27;</span> % (validate_loss / (i + <span class="number">1</span>)), <span class="string">&#x27;validate_acc&#x27;</span>: <span class="string">&#x27;%.6f&#x27;</span> % val_top1.avg&#125;</span><br><span class="line">            validate_loader.set_postfix(log=postfix)</span><br><span class="line">        val_acc = val_top1.avg</span><br><span class="line">    <span class="keyword">return</span> val_acc</span><br></pre></td></tr></table></figure>
<h2 id="输出测试集的预测结果"><a href="#输出测试集的预测结果" class="headerlink" title="输出测试集的预测结果"></a>输出测试集的预测结果</h2><p>将测试集输入进网络，得到测试集的预测结果，并转换成csv文件，用来提交到Kaggle上进行评分。需要注意的是，因为官网要求给的是图片是狗的概率，需要将网络的输出转成概率值。</p>
<p>但实际上测试的时候网络的输出是一正一负的值，不是概率值。这是因为测试的时候没有计算loss ，而softmax这个过程是在计算交叉熵的时候自动计算的，所以在网络架构中最后一层全连接输出后没有softmax计算。因此需要我们手动增加上softmax的过程，这样经过softmax后就可以变成两个概率值了!将图片是狗的概率保存下来，并转成符合官方要求的提交格式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submission</span>(<span class="params">csv_path,test_loader, device, model</span>):</span></span><br><span class="line">    result_list = []</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    test_loader = tqdm(test_loader)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 进行评测的时候网络不更新梯度</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_loader, <span class="number">0</span>):</span><br><span class="line">            images, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            softmax_func = nn.Softmax(dim=<span class="number">1</span>)  <span class="comment"># dim=1表示行的和为1</span></span><br><span class="line">            soft_output = softmax_func(outputs)</span><br><span class="line">            predicted = soft_output[:, <span class="number">1</span>]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(predicted)):</span><br><span class="line">                result_list.append(&#123;</span><br><span class="line">                    <span class="string">&quot;id&quot;</span>: labels[i].item(),</span><br><span class="line">                    <span class="string">&quot;label&quot;</span>: predicted[i].item()</span><br><span class="line">                &#125;)</span><br><span class="line">    <span class="comment"># 从list转成 dataframe 然后保存为csv文件</span></span><br><span class="line">    columns = result_list[<span class="number">0</span>].keys()</span><br><span class="line">    result_dict = &#123;col: [anno[col] <span class="keyword">for</span> anno <span class="keyword">in</span> result_list] <span class="keyword">for</span> col <span class="keyword">in</span> columns&#125;</span><br><span class="line">    result_df = pd.DataFrame(result_dict)</span><br><span class="line">    result_df = result_df.sort_values(<span class="string">&quot;id&quot;</span>)</span><br><span class="line">    result_df.to_csv(csv_path, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h2 id="完整调用流程"><a href="#完整调用流程" class="headerlink" title="完整调用流程"></a>完整调用流程</h2><p> 损失函数和优化方法的确定</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = MyCNN()</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># criterion = nn.BCELoss()  #二分类交叉熵损失函数</span></span><br><span class="line"><span class="comment"># criterion = nn.BCEWithLogitsLoss() #二分类交叉熵损失函数 带log loss</span></span><br><span class="line"><span class="comment"># criterion = nn.MSELoss()</span></span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment">#也可以选择Adam优化方法</span></span><br><span class="line"><span class="comment"># optimizer = torch.optim.Adam(net.parameters(),lr=1e-2)   </span></span><br></pre></td></tr></table></figure>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a><strong>训练过程</strong></h3><p>需要传入epoch数目，训练数据加载器，设备，网络模型，损失函数，优化方法和tensorboard画图的路径等参数。<br>注意的是如果使用完整的官方训练数据集来训练网络后，用这个网络去在验证集上面验证是没有意义的，因为验证集的数据是从完整训练数据集上面划分出来，所以相当于在用训练数据验证性能。用划分过后的new_train_loader训练的网络在进行验证才有意义。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># train( 1, train_loader, device,net, criterion, optimizer,tensorboard_path) # 完整的训练数据集</span></span><br><span class="line">train( <span class="number">1</span>, new_train_loader, device,net, criterion, optimizer,tensorboard_path) <span class="comment"># 划分80%后的训练数据集</span></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>Finished Training
</code></pre><p>在训练的时候会用tensorboard保存每个时刻的训练数据，需要新打开一个命令端口输入：<br><code>tensorboard --logdir=/path_to_log_dir/ --port 6006</code> 命令，然后通过在浏览器中输入网址<a href="http://localhost:6006/">http://localhost:6006/</a> 来查看</p>
<p>模型的保存和加载</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(net.state_dict(), model_save_path)</span><br><span class="line">val_net = MyCNN()</span><br><span class="line">val_net.load_state_dict(torch.load(<span class="string">&#x27;./dogs-vs-cats_12epoch_valnet.pth&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code>&lt;All keys matched successfully&gt;
</code></pre><h3 id="验证过程"><a href="#验证过程" class="headerlink" title="验证过程"></a><strong>验证过程</strong></h3><p>输入的网络是上面训练过的网络，或者从模型权重保存路径加载的模型。输出模型在自己划分的验证集上面的准确率，结果是98.84%</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">validate(validate_loader,device,val_net,criterion)</span><br></pre></td></tr></table></figure>
<p>​    输出：</p>
<pre><code>73.56
</code></pre><h3 id="输出测试集预测结果"><a href="#输出测试集预测结果" class="headerlink" title="输出测试集预测结果"></a><strong>输出测试集预测结果</strong></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">submission(<span class="string">&#x27;./test.csv&#x27;</span>,test_loader, device, val_net)</span><br></pre></td></tr></table></figure>
<p>最后在Kaggle上提交预测结果csv文件，得到打分。 需要先报名参赛这些操作，而且只有Dogs vs. Cats Redux: Kernels Edition这个才能够提交数据，最开始的那个6年前的提交通道已经关闭了。提交可以下载Kaggle的API在命令行提交，也可以直接在<a href="https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/submit">提交链接</a>提交</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>文章基于Notebook的过程，也构建了对应的<a href="https://github.com/BraveY/AI-with-code/tree/master/dog-vs-cat/pycharm-project/src">Pycharm项目</a>，将整个过程分模块来编码,架构更清晰。</p>
<p>我总共训练了三个网络，其中MyCNN_net_1使用全部的训练数据，MyCNN_net_2使用划分过的训练数据，RestNet50是预训练的模型，使用完整训练数据训练。</p>
<p>Kaggle上的评分是根据log loss来计算的，分数越低代表模型性能越好。然后其他两个网络使用的完整训练数据集是包含验证集的，所以没有计算验证集的准确率。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>网络</th>
<th>epochs</th>
<th>训练数据</th>
<th>得分</th>
<th>验证集准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td>ResNet50</td>
<td>3</td>
<td>完整训练数据集</td>
<td>0.06691</td>
<td>-</td>
</tr>
<tr>
<td>MyCNN_net_1</td>
<td>12</td>
<td>划分的80%训练数据集</td>
<td>0.73358</td>
<td>73.56</td>
</tr>
<tr>
<td>MyCNN_net_2</td>
<td>12</td>
<td>完整训练数据集</td>
<td>0.94158</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
<p>实验结果并不严谨，只进行了一次，所以存在一些随机性。</p>
<p>其中使用预训练的ResNet50的效果非常好，在猫狗数据集上训练微调的时候loss就很低了，所以只训练了3轮。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1587879398/blog/deep-learning/Resnet50.jpg" alt=""></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>自己在搭建网络的时候遇到一个问题就是随便设置的一个网络结构的时候，发现交叉熵的loss会一直维持在0.69，不下降。暂时还没有搞懂问题出在哪儿，后面有时间了研究下，</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://bravey.github.io/2020-03-13-%E4%BD%BF%E7%94%A8Pytorch%E6%A1%86%E6%9E%B6%E7%9A%84CNN%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%EF%BC%88MNIST%EF%BC%89%E8%AF%86%E5%88%AB.html">之前的手写数字识别</a></p>
<p><a href="https://www.pytorchtutorial.com/pytorch-builtin-tensorboard/">tensorboard画图</a><br><a href="https://www.zhihu.com/question/38341743">pip源的更新</a><br><a href="https://www.cnblogs.com/marsggbo/p/10496696.html">数据集的划分</a><br><a href="https://github.com/ShunLu91/Single-Path-One-Shot-NAS">topk的计算</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM与Prophet时间序列预测实验</title>
    <url>/2019-12-20-LSTM%E4%B8%8EProphet%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B%E5%AE%9E%E9%AA%8C.html</url>
    <content><![CDATA[<h1 id="LSTM与Prophet时间序列预测实验"><a href="#LSTM与Prophet时间序列预测实验" class="headerlink" title="LSTM与Prophet时间序列预测实验"></a>LSTM与Prophet时间序列预测实验</h1><p>分别使用Pytorch构建的LSTM网络与Facebook开源的Prophet工具对时间序列进行预测的一个对比小实验，同时作为一个小白也借着这个实验来学习下Pytorch的使用，因为第一次使用，所以会比较详细的注释代码。</p>
<a id="more"></a>
<p>使用的数据为了与Prophet进行对比，因此使用了Prophet官网例子上用到的数据集。该时间序列数据集来自维基百科上面对美国橄榄球运动员佩顿·曼宁（Peyton Williams Manning）的日访问量的记录日志，时间跨度为2007年12月10号到2016年1月20号共2905条数据。 </p>
<p>Jupyter代码与数据集地址在<a href="https://github.com/BraveY/AI-with-code/tree/master/time-series">我的github</a>上，欢迎star。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>LSTM的介绍参考<a href="https://zhuanlan.zhihu.com/p/30465140">夕小瑶</a>与<a href="https://zhuanlan.zhihu.com/p/32085405">陈诚</a>的介绍，代码主要参考<a href="https://blog.csdn.net/baidu_36669549/article/details/85595807">凌空的桨</a>与<a href="https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch/tree/master/chapter5_RNN/time-series">源码链接</a> ，在Pytorch1.3.1的版本上面改了一下，主要是测试的逻辑修改成了使用测试集以及取消了Variable的使用。整体的逻辑是使用前面的两天的数据来预测下一天的数据，网络的结构是使用了两层LSTM与一层线性回归层。</p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>首先是数据的预处理代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据预处理</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;example_wp_log_peyton_manning.csv&#x27;</span>,usecols=[<span class="number">1</span>])</span><br><span class="line">data = data.dropna() <span class="comment">#丢弃空值</span></span><br><span class="line">dataset = data.values</span><br><span class="line">dataset = dataset.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line">max_value = np.<span class="built_in">max</span>(dataset)</span><br><span class="line">min_value = np.<span class="built_in">min</span>(dataset)</span><br><span class="line">scalar = max_value - min_value</span><br><span class="line">dataset = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x/scalar, dataset)) <span class="comment">#将数据归一化到0~1之间</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#划分数据集</span></span><br><span class="line"><span class="comment">#通过前面几条的数据来预测下一条的数据，look_back设置具体的把前面几条的数据作为预测的输入data_X，而输出就是下一条data_Y</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span>(<span class="params">dataset,look_back=<span class="number">2</span></span>):</span> <span class="comment"># 每个的滑动窗口设置为2</span></span><br><span class="line">    dataX, dataY=[], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset)-look_back):</span><br><span class="line">        a=dataset[i:(i+look_back)]</span><br><span class="line">        dataX.append(a)    <span class="comment"># 记录窗口的值</span></span><br><span class="line">        dataY.append(dataset[i+look_back]) <span class="comment"># 记录除了前面两个以外的所有值作为正确的标签</span></span><br><span class="line">    <span class="keyword">return</span> np.array(dataX), np.array(dataY)</span><br><span class="line"><span class="comment">#创建好输入与输出 data_Y作为正确的预测值</span></span><br><span class="line">data_X, data_Y = create_dataset(dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment">#划分训练集和测试集，70%作为训练集</span></span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="built_in">len</span>(data_X) * <span class="number">0.7</span>)</span><br><span class="line">test_size = <span class="built_in">len</span>(data_X)-train_size</span><br><span class="line"></span><br><span class="line">train_X = data_X[:train_size]</span><br><span class="line">train_Y = data_Y[:train_size]</span><br><span class="line"></span><br><span class="line">test_X = data_X[train_size:]</span><br><span class="line">test_Y = data_Y[train_size:]</span><br><span class="line"></span><br><span class="line"><span class="comment">#最后，我们需要将数据改变一下形状，因为 RNN 读入的数据维度是 (seq, batch, feature)，所以要重新改变一下数据的维度，这里只有一个序列，所以 batch 是 1，而输入的 feature 就是我们希望依据的几天，这里我们定的是两个天，所以 feature 就是 2.</span></span><br><span class="line"></span><br><span class="line">train_X = train_X.reshape(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">train_Y = train_Y.reshape(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">test_X = test_X.reshape(-<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转化成torch 的张量</span></span><br><span class="line">train_x = torch.from_numpy(train_X)</span><br><span class="line">train_y = torch.from_numpy(train_Y)</span><br><span class="line">test_x = torch.from_numpy(test_X)</span><br></pre></td></tr></table></figure>
<h3 id="LSTM网络构建"><a href="#LSTM网络构建" class="headerlink" title="LSTM网络构建"></a>LSTM网络构建</h3><p>接着定义好网络模型，模型的第一部分是一个两层的 RNN，每一步模型接受前两天的输入作为特征，得到一个输出特征。接着通过一个线性层将 RNN 的输出回归到流量的具体数值，这里我们需要用 <code>view</code> 来重新排列，因为 <code>nn.Linear</code> 不接受三维的输入，所以我们先将前两维合并在一起，然后经过线性层之后再将其分开，最后输出结果。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#lstm 网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">lstm_reg</span>(<span class="params">nn.Module</span>):</span><span class="comment">#括号中的是python的类继承语法，父类是nn.Module类 不是参数的意思</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,hidden_size, output_size=<span class="number">1</span>,num_layers=<span class="number">2</span></span>):</span> <span class="comment"># 构造函数</span></span><br><span class="line">        <span class="comment">#inpu_size 是输入的样本的特征维度， hidden_size 是LSTM层的神经元个数，</span></span><br><span class="line">        <span class="comment">#output_size是输出的特征维度</span></span><br><span class="line">        <span class="built_in">super</span>(lstm_reg,self).__init__()<span class="comment"># super用于多层继承使用，必须要有的操作</span></span><br><span class="line"> </span><br><span class="line">        self.rnn = nn.LSTM(input_size,hidden_size,num_layers)<span class="comment"># 两层LSTM网络，</span></span><br><span class="line">        self.reg = nn.Linear(hidden_size,output_size)<span class="comment">#把上一层总共hidden_size个的神经元的输出向量作为输入向量，然后回归到output_size维度的输出向量中</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span> <span class="comment">#x是输入的数据</span></span><br><span class="line">        x, _ = self.rnn(x)<span class="comment"># 单个下划线表示不在意的变量，这里是LSTM网络输出的两个隐藏层状态</span></span><br><span class="line">        s,b,h = x.shape</span><br><span class="line">        x = x.view(s*b, h)</span><br><span class="line">        x = self.reg(x)</span><br><span class="line">        x = x.view(s,b,-<span class="number">1</span>)<span class="comment">#使用-1表示第三个维度自动根据原来的shape 和已经定了的s,b来确定</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment">#我使用了GPU加速，如果不用的话需要把.cuda()给注释掉    </span></span><br><span class="line">net = lstm_reg(<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">net = net.cuda()</span><br><span class="line">criterion = nn.MSELoss().cuda()</span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(),lr=<span class="number">1e-2</span>)    </span><br></pre></td></tr></table></figure>
<p>本来打算把网络拓扑也给画出来的，后面发现自己理解的还不够深入，可以先参考<a href="https://www.zhihu.com/question/41949741/answer/318771336">LSTM神经网络输入输出究竟是怎样的？ - Scofield的回答 - 知乎 </a> 和<a href="https://zhuanlan.zhihu.com/p/79064602">LSTM细节分析理解（pytorch版） - ymmy的文章 - 知乎 </a></p>
<p>关于forward函数中为什么每个层可以直接使用输入的数据x这个tensor，而不需要按照构造函数里面的按照形参(input_size,hidden_size,num_layers)来传递参数。以nn.LSTM做例子，官方API为：</p>
<ul>
<li>参数<br>– <strong>input_size</strong><br>– <strong>hidden_size</strong><br>– <strong>num_layers</strong><br>– <strong>bias</strong><br>– <strong>batch_first</strong><br>– <strong>dropout</strong><br>– <strong>bidirectional</strong></li>
<li>输入<br>– <strong>input</strong> (seq_len, batch, input_size)<br>– <strong>h_0</strong> (num_layers <em> num_directions, batch, hidden_size)<br>– <strong>c_0</strong> (num_layers </em> num_directions, batch, hidden_size)</li>
<li><p>输出<br>– <strong>output</strong> (seq_len, batch, num_directions <em> hidden_size)<br>– <strong>h_n</strong> (num_layers </em> num_directions, batch, hidden_size)<br>– <strong>c_n</strong> (num_layers * num_directions, batch, hidden_size)</p>
<p>所以forward中的x，<code>x, _ = self.rnn(x)</code>传递的参数是对应输入<strong>input</strong> (seq_len, batch, input_size)这个tensor，而不是对应的参数列表。同样<code>_</code>所代表的参数也就是<strong>h_n</strong> 和<strong>c_n</strong>。</p>
</li>
</ul>
<h3 id="迭代"><a href="#迭代" class="headerlink" title="迭代"></a>迭代</h3><p>迭代过程进行了10000次迭代：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line"><span class="comment"># 新版本中可以不使用Variable了    </span></span><br><span class="line"><span class="comment">#     var_x = Variable(train_x).cuda() </span></span><br><span class="line"><span class="comment">#     var_y = Variable(train_y).cuda()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#将tensor放在GPU上面进行运算</span></span><br><span class="line">    var_x = train_x.cuda()</span><br><span class="line">    var_y = train_y.cuda()</span><br><span class="line"> </span><br><span class="line">    out = net(var_x)</span><br><span class="line">    loss = criterion(out, var_y)</span><br><span class="line"> </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> (e+<span class="number">1</span>)%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">&#x27;Epoch: &#123;&#125;, Loss:&#123;:.5f&#125;&#x27;</span>.<span class="built_in">format</span>(e+<span class="number">1</span>, loss.item()))</span><br><span class="line"><span class="comment">#存储训练好的模型参数        </span></span><br><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;example_wp_log.net_params.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="测试过程"><a href="#测试过程" class="headerlink" title="测试过程"></a>测试过程</h3><p>在测试的时候我发现源码中并没有用到之前划分的30%的测试集来单独进行测试，而是直接把原来的完整数据给丢进去来训练的，这儿有点没搞懂。因为按理来说需要单独使用测试集进行测试来评判模型的性能的，所以我单独把测试的数据集给提出来，使用单独的测试集进行了测试。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.load_state_dict(torch.load(<span class="string">&#x27;example_wp_log.net_params.pkl&#x27;</span>)) </span><br><span class="line">var_data = torch.from_numpy(test_X).cuda()<span class="comment">#net在GPU上面，所以输入的测试集合也要转入到GPU上面</span></span><br><span class="line">pred_test = net(var_data) <span class="comment"># 测试集的预测结果</span></span><br><span class="line">pred_test = pred_test.cpu().view(-<span class="number">1</span>).data.numpy()<span class="comment">#先转移到cpu上才能转换为numpy</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#乘以原来归一化的刻度放缩回到原来的值域 </span></span><br><span class="line">origin_test_Y = test_Y*scalar</span><br><span class="line">origin_pred_test = pred_test*scalar</span><br><span class="line"></span><br><span class="line"><span class="comment">#画图</span></span><br><span class="line">plt.plot(origin_pred_test, <span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;prediction&#x27;</span>)</span><br><span class="line">plt.plot(origin_test_Y, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;real&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算MSE</span></span><br><span class="line"><span class="comment">#loss = criterion(out, var_y)？</span></span><br><span class="line">true_data = origin_test_Y</span><br><span class="line">true_data = np.array(true_data)</span><br><span class="line">true_data = np.squeeze(true_data)  <span class="comment"># 从二维变成一维</span></span><br><span class="line">MSE = true_data - origin_pred_test</span><br><span class="line">MSE = MSE*MSE</span><br><span class="line">MSE_loss = <span class="built_in">sum</span>(MSE)/<span class="built_in">len</span>(MSE)</span><br><span class="line">print(MSE_loss)</span><br></pre></td></tr></table></figure>
<p>计算出来的MSE为<code>0.195649022176008</code>, 画出来的曲线图为：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1576911405/blog/deep-learning/LSTM_time_series.png" alt=""></p>
<h3 id="GPU加速"><a href="#GPU加速" class="headerlink" title="GPU加速"></a>GPU加速</h3><p><code>use_gpu = torch.cuda.is_available()  # 判断是否有GPU加速</code></p>
<p>CUDA 加速需要设置的为：</p>
<ol>
<li>迭代的过程中输入的tensor放到GPU上  var_x = train_x.cuda()</li>
<li>模型转移到GPU net.cuda()</li>
<li>损失函数转移到GPU criterion = nn.MSELoss().cuda()</li>
</ol>
<h2 id="Prophet"><a href="#Prophet" class="headerlink" title="Prophet"></a>Prophet</h2><p>Prophet是facebook开源的一个时间序列预测工具,使用了时间序列分解与机器学习拟合的方法。详细介绍参考<a href="https://zhuanlan.zhihu.com/p/52330017">张戎</a>的介绍。</p>
<h3 id="Prophet的安装"><a href="#Prophet的安装" class="headerlink" title="Prophet的安装"></a>Prophet的安装</h3><p>在安装Prophet的时候并没有想官网介绍的那么简单，首先需要先安装Pystan,但是直接<code>pip install pystan</code>会报编译器内部错误，使用<code>conda install -c conda-forge pystan</code>之后问题解决，然后再使用<code>pip install fbprophet</code> 进行安装。</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>实验的例子就是官网的例子</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> fbprophet <span class="keyword">import</span> Prophet</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;example_wp_log_peyton_manning.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Prophet使用</span></span><br><span class="line">m = Prophet()</span><br><span class="line">m.fit(df)</span><br><span class="line"><span class="comment">#需要预测时间段为整个365天，也就是下一年的整个天数</span></span><br><span class="line">future = m.make_future_dataframe(periods=<span class="number">365</span>)</span><br><span class="line"><span class="comment">#开始预测</span></span><br><span class="line">forecast = m.predict(future)</span><br><span class="line"><span class="comment">#预测的结果保存在yhat_upper列中</span></span><br><span class="line">forecast[[<span class="string">&#x27;ds&#x27;</span>, <span class="string">&#x27;yhat&#x27;</span>, <span class="string">&#x27;yhat_lower&#x27;</span>, <span class="string">&#x27;yhat_upper&#x27;</span>]].tail()</span><br><span class="line"><span class="comment">#画图</span></span><br><span class="line">plt.plot(fb_pre, <span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;prediction&#x27;</span>)</span><br><span class="line">plt.plot(origin_test_Y, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;real&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment">#计算MSE</span></span><br><span class="line">fb_pre = np.array(forecast[<span class="string">&#x27;yhat&#x27;</span>].iloc[<span class="number">2034</span>:<span class="number">2905</span>])<span class="comment">#2034到2905是前面30%的测试集所对应的数据范围</span></span><br><span class="line">MSE = true_data - fb_pre</span><br><span class="line">MSE = MSE*MSE</span><br><span class="line">MSE_loss = <span class="built_in">sum</span>(MSE)/<span class="built_in">len</span>(MSE)</span><br><span class="line">print(MSE_loss)</span><br></pre></td></tr></table></figure>
<p>计算出来的MSE为：<code>0.25229994660830146</code>,画出来的图像为：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1576911405/blog/deep-learning/Prophet_time_series.png" alt=""></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>MSE</th>
</tr>
</thead>
<tbody>
<tr>
<td>LSTM</td>
<td>0.195649022176008</td>
</tr>
<tr>
<td>Prophet</td>
<td>0.25229994660830146</td>
</tr>
</tbody>
</table>
</div>
<p>可以看到使用LSTM的预测结果要比Prophet的结果好，但是也有可能是我还没有去调整Prophet的参数导致Prophet的性能差一些的。同时Prophet可以预测整整一年的时间，这个比起使用LSTM要厉害很多，实验中的LSTM使用的是单步预测的方法，也就是只能根据前段时刻的数据来预测下一个时刻的数据，如果要做到像Prophet那样预测未来一段时刻的数据，需要使用多步预测的方法，我查了下涉及到seq2seq，貌似比较复杂，还没有做实验。</p>
<p>自己是小白，实验可能存在相关问题与不足之处，欢迎反馈。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>Pytorch中的<a href="https://zhuanlan.zhihu.com/p/41261640">LSTM参数</a></p>
<p><a href="https://facebook.github.io/prophet/">Prophet官网</a></p>
<p><a href="https://www.okcode.net/article/43571">Prophet安装问题</a> </p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>时间序列</tag>
        <tag>LSTM</tag>
        <tag>Pytorch</tag>
        <tag>Prophet</tag>
      </tags>
  </entry>
  <entry>
    <title>NSQ学习笔记1——简介</title>
    <url>/2023-02-05-NSQ%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B.html</url>
    <content><![CDATA[<p><a href="https://nsq.io/">NSQ</a>是一个使用Go开发的分布式消息队列，对于学习Go语言和消息队列而言都是一个非常不错的开源项目，因此记录下学习NSQ的一些笔记。</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>官网上NSQ宣传的特点包括</p>
<ol>
<li>分布式——去中心化，无单点故障，推模型来降低消息延迟</li>
<li>可扩展——可水平扩展</li>
<li>运维友好——容易配置和部署，提供预编译和Docker镜像版本</li>
<li>易集成——支持主流语言的客户端，以及HTTP接口发送消息<br>实际简单体验下来，的确部署和启动都比较快速方便，不需要提前做大量的配置就可以简单启动demo。</li>
</ol>
<p>作为消息队列，NSQ的消息语义保证有：</p>
<ol>
<li>消息默认非持久化。消息保存在内存中，超过存储水位后会临时存到磁盘中。</li>
<li>消息传递最少一次。这也意味着使用者自己去处理消息重传后导致的幂等问题。</li>
<li>接受到的消息是无序的。主要是因为各个节点之前没有共享数据造成的，官方建议使用者自己使用延迟窗口来进行排序。</li>
<li>消费者最终一定会发现所有生产者。</li>
</ol>
<h1 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h1><h2 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h2><p>与Kafka的Topic设计做一个简单的对比，方便更好的学习。<br>NSQ中和Kafka一样使用Topic来组织消息也主要是发布定于模式，但使用推模型发送消息，而Kafka是基于拉模型的。因此实时性上NSQ会更好一些。<br>NSQ中Topic也会实际的存储消息，并且与Channels中的消息是单独队列来存储。而Kafka中的Topic主要是逻辑概念，实际的数据存储是由分区来存储消息的。<br>NSQ中每个Topic有一到多个的Channel来存储消息，每个Channel都会复制Topic中的消息。而Kafka中同一个Topic下的不同分区的消息是不同的。<br>NSQ中每个Channel 也会由多个消费者相连接。</p>
<p>下图展示了消息的轨迹。可以看到消息先是存储在topic中，然后广播到其对应的每个Channel中，最后被每个Channel中的消费者竞争消费。<br><img src="https://https://res.cloudinary.com/bravey/image/upload/v1678520435/blog/nsq/topic_channels.gif" alt=""></p>
<h2 id="节点发现"><a href="#节点发现" class="headerlink" title="节点发现"></a>节点发现</h2><p><code>nsqd</code>是消息处理节点，每个 <code>nsqd</code>节点都可以发送Topic相关的消息。但在分布式部署的模式下，发送消息之前需要先与 <code>nsqlookupd</code>建立TCP长连接周期性心跳更新状态，这样其他 <code>nsqd</code>节点才可以发现当前节点以及当前节点上的Topic对应的消息。</p>
<p>此外 <code>nsqlookupd</code>节点也作为中间件来实现生产者和消费者的解耦，生产者与消费者通过 <code>nsqlookupd</code>来发现彼此。<br><img src="https://res.cloudinary.com/bravey/image/upload/v1678520435/blog/nsq/lookupd1.jpg" alt=""><br>官方建议 <code>nsqlookupd</code>最少部署两个来提供服务的可用性。<br>消费者通过向 <code>nsqlookupd</code>发送HTTP请求来获得可用的nsqd节点和节点上的Topic，Channels中的消息。<br><img src="https://res.cloudinary.com/bravey/image/upload/v1678520435/blog/nsq/lookupd2.jpg" alt=""></p>
<h2 id="单点故障消除"><a href="#单点故障消除" class="headerlink" title="单点故障消除"></a>单点故障消除</h2><p>因为每个消费者与所有提供相应Topic的 <code>nsqd</code>节点连接，所以不会出现单点故障。某个 <code>nsqd</code>节点出现故障也不会影响服务。（这里自己有个疑问，消息是会在所有节点上保存副本吗？不然单点故障了消息岂不就丢失了？）<br><img src="https://res.cloudinary.com/bravey/image/upload/v1678520462/blog/nsq/nospof.jpg" alt=""></p>
<h2 id="消息内存存储"><a href="#消息内存存储" class="headerlink" title="消息内存存储"></a>消息内存存储</h2><p>消息默认使用内存存储，只有当超过水位的时候，会将消息存储到磁盘中。<br>这里有一个深度的概念就是指队列中消息的积压深度。当深度超过阈值的时候，消息就转存到磁盘中。</p>
<p>当将深度设置的很低的时候，比如1或者0，那么每个消息都会落盘，可以通过这种方式确保预期外重启的时候的消息不会丢失。<br><img src="https://res.cloudinary.com/bravey/image/upload/v1678520434/blog/nsq/disk.jpg" alt=""></p>
<h2 id="效率"><a href="#效率" class="headerlink" title="效率"></a>效率</h2><p>为了大幅提升性能和消息吞吐率，NSQ设计的数据协议使用推模型来发送消息。<br><img src="https://res.cloudinary.com/bravey/image/upload/v1678520435/blog/nsq/rdy.jpg" alt=""><br>消费者在与 <code>nsqd</code>建立连接并订阅Topic后，通过将自己设置为RDY状态来接受消息，同时设置对应RDY状态的值来接受指定的数量的消息。比如图中RDY 2则只会收到两条消息。通过这个方式来灵活的调节消费者的消费能力。这也提供了指定客户端进行调度的方式。</p>
<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p><code>nsqd</code>向 <code>nsqlookupd</code>注册服务，而消费者通过 <code>nsqlookupd</code>来发现服务。之后消费者和所有 <code>nsqd</code>建立连接。<br><img src="https://https://res.cloudinary.com/bravey/image/upload/v1678520435/blog/nsq/overview.jpg" alt=""></p>
<h1 id="DEMO"><a href="#DEMO" class="headerlink" title="DEMO"></a>DEMO</h1><p>启动demo的方式参见官方的<a href="https://nsq.io/overview/quick_start.html">教程</a> 不需要额外设置相应的配置就能快速启动了，同时使用 <code>nsqadmin</code>来提供前端管理服务。本文不赘述。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>使用《数据密集型应用系统设计》一书中对 发布/订阅模式的流处理系统的分类方法对NSQ进行总结，可以比较好的了解NSQ的设计逻辑。书中的方法为提出如下两个问题进行分类。</p>
<ol>
<li>如果生产者发送消息的速度比消费者快所能处理的快，会发生什么？<ol>
<li>一般而言由三种选择：系统丢弃消息；将消息缓存在队列中；使用流量控制。NSQ的方式是消息缓存在队列中，超过队列阈值后写入磁盘。相比而言Kafka是直接写入磁盘。</li>
</ol>
</li>
<li>如果节点崩溃或者暂时离线，是否会有消息丢失？<ol>
<li>这一点如果NSQ将转存磁盘的阈值设置为0，那么所有消息都会落盘，因此不存在消息丢失。但是对于内存中的消息是如何保证不丢的，这个还需要后续进一步深入了解。</li>
</ol>
</li>
</ol>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://nsq.io/overview/design.html">https://nsq.io/overview/design.html</a><br><a href="https://docs.google.com/presentation/d/1e9yIm-0aNba_H1gX_u7D1Qe5VWeU26FCjg7EEzoUztE/edit#slide=id.g3c47333ca6_0_0">https://docs.google.com/presentation/d/1e9yIm-0aNba_H1gX_u7D1Qe5VWeU26FCjg7EEzoUztE/edit#slide=id.g3c47333ca6_0_0</a><br><a href="https://blog.lpflpf.cn/passages/nsqd-study-1/">https://blog.lpflpf.cn/passages/nsqd-study-1/</a></p>
]]></content>
      <categories>
        <category>NSQ</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title>C专家编程读书笔记</title>
    <url>/2021-03-18-C%E4%B8%93%E5%AE%B6%E7%BC%96%E7%A8%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<h1 id="第一章-C-穿越时空的迷雾"><a href="#第一章-C-穿越时空的迷雾" class="headerlink" title="第一章 C 穿越时空的迷雾"></a>第一章 C 穿越时空的迷雾</h1><ol>
<li><p>每次编写新函数时，都应该使用<strong>函数原型</strong>。（可以放在头文件中）</p>
</li>
<li><p><strong>const</strong>关键字限定的变量，表示该变量只读。最有用的地方是用来限定函数的形参，防止函数内部修改实参指针所指的数据。const并不表示该变量是常量。</p>
</li>
<li><p>尽量不使用unsigned 无符号类型，只在使用位段和二进制掩码时使用无符号类型。</p>
</li>
<li><p>进行比较的时候，使操作数均为有符号数或者无符号数。</p>
<p>下面的代码int 在比较的时候会升级为unsigned int， 从-1转换后得到的结果是非常大的正整数，从而if语句不会进入。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">solution</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> <span class="built_in">array</span>[] = &#123; <span class="number">23</span>, <span class="number">34</span>,<span class="number">12</span>,<span class="number">17</span>,<span class="number">204</span>,<span class="number">99</span>,<span class="number">16</span>&#125;;</span><br><span class="line">        <span class="keyword">int</span> d = <span class="number">-1</span>, x;</span><br><span class="line">        <span class="keyword">if</span> (d &lt;= (<span class="keyword">int</span>) (<span class="keyword">sizeof</span>(<span class="built_in">array</span>) / <span class="keyword">sizeof</span>(<span class="built_in">array</span>[<span class="number">0</span>])) - <span class="number">2</span>)&#123; <span class="comment">// 不加int 不会进入if语句</span></span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; d &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            x = <span class="built_in">array</span>[d+<span class="number">1</span>];            </span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; x &lt;&lt; <span class="built_in">endl</span>;<span class="built_in">cout</span> &lt;&lt; d &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; d &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="第二章-不是BUG，而是语言特性"><a href="#第二章-不是BUG，而是语言特性" class="headerlink" title="第二章 不是BUG，而是语言特性"></a>第二章 不是BUG，而是语言特性</h1></li>
<li><p>switch 语句默认是fall through,即case语句不加break的话，会依次执行下去。break跳出的是这个switch语句</p>
</li>
<li><p>在ANSI C标准中相邻的字符串常量会被自动合并，如果字符串数组在初始化的时候漏掉一个逗号，则会使两个元素自动合并而不报错。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">stringMerge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">char</span> *resource[] = &#123;</span><br><span class="line">		<span class="string">&quot;monitor&quot;</span>,</span><br><span class="line">		<span class="string">&quot;disk&quot;</span>,</span><br><span class="line">		<span class="string">&quot;mouse&quot;</span></span><br><span class="line">		<span class="string">&quot;keybord&quot;</span>,</span><br><span class="line">		<span class="string">&quot;pen&quot;</span>,</span><br><span class="line">	&#125;;</span><br><span class="line">	<span class="built_in">cout</span> &lt;&lt; resource[<span class="number">2</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出为mousekeybord，这样使数组元素少了一个，造成越界错误。</p>
</li>
<li><p><code>static</code>修饰的变量只初始化一次，可以用来使同一段代码，第一次执行（对static变量初始化）和后面执行（不进行初始化）结果不同。</p>
</li>
<li><p>C中的<strong>函数</strong>默认是全局可见的，相当于默认加了<code>extern</code> 关键字，限制对这个函数的访问需要使用<code>static</code>。一个符号要么是全局可见要么是其他都看不见。</p>
</li>
<li><p><code>static</code>关键字：</p>
<ol>
<li>在函数内部，表示该变量值在各个调用期间保持延续性，不会被销毁</li>
<li>修饰函数，表示函数只对本文件可见</li>
</ol>
</li>
<li><p><code>extern</code> 关键字</p>
<ol>
<li>修饰函数，表示函数全局可见。冗余操作</li>
<li>修饰变量，表示在其他地方可见。（跨文件使用全局变量）</li>
</ol>
</li>
<li><p>表达式中包含布尔操作，算数运算，位操作等混合计算，<strong>适当的地方加上括号</strong>，有些优先顺序可能与自己想的不一样，所以使用括号来明确表达。</p>
</li>
<li><p>结合性</p>
<ol>
<li>赋值运算符为右结合性，从右向左执行。<code>a=b=c</code> c先赋值给b，b再赋值给a</li>
<li>位操作符&amp;,|位左结合性，从左向右执行。</li>
</ol>
</li>
<li><p>使用<code>fgets()</code> 替代<code>gets()</code>，后者不会检查读入的限制，会有漏洞，前者只接受有限数量的字符。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title>C语言传递指针参数的陷阱</title>
    <url>/2020-12-12-C%E8%AF%AD%E8%A8%80%E4%BC%A0%E9%80%92%E6%8C%87%E9%92%88%E5%8F%82%E6%95%B0%E7%9A%84%E9%99%B7%E9%98%B1.html</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>师兄发来一段代码问代码是否有误。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Malloc1</span><span class="params">(<span class="keyword">char</span> *p, <span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">	p = (<span class="keyword">char</span>*)<span class="built_in">malloc</span>(num * <span class="keyword">sizeof</span>(<span class="keyword">char</span>));<span class="comment">// 传进来的地址值没有用到，没有使用解引用。</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MyTest1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">char</span> *s = <span class="literal">NULL</span>;</span><br><span class="line">	Malloc1(s, <span class="number">10</span>);</span><br><span class="line">	<span class="built_in">strcpy</span>(s, <span class="string">&quot;hello&quot;</span>);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;%s\n&quot;</span>, s);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="二级指针"><a href="#二级指针" class="headerlink" title="二级指针"></a>二级指针</h3><p>指针是一个变量，它的值是一个地址，地址指向为一个对应类型的变量。</p>
<p>赋值符号<code>=</code>, 左边是一个地址，右边是赋给该地址的值。变量是一个地址的别名。</p>
<h4 id="amp-符号"><a href="#amp-符号" class="headerlink" title="&amp;符号"></a>&amp;符号</h4><p><code>int &amp;a =b</code> a是b的引用，是b的别名,此时<code>&amp;</code>用来表示引用</p>
<p>单独使用时，取地址符<code>&amp;</code>,用来获得一个变量的地址。<code>&amp;a</code>为变量<code>a</code>的地址</p>
<h4 id="符号"><a href="#符号" class="headerlink" title="*符号"></a>*符号</h4><p><code>int *a = &amp;b</code> a是指向b的指针，此时<code>*</code>表示指针变量</p>
<p>解引用符<code>*</code>，用来得到指针指向的内存地址的值。<code>*a</code>为a这个指针指向的对象的值。</p>
<p>如下程序：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">printAddr</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> a = <span class="number">2</span>;</span><br><span class="line">	<span class="keyword">int</span> *pa = &amp;a;</span><br><span class="line">	<span class="keyword">int</span> **ppa = &amp;pa;</span><br><span class="line"></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;the address of a is:%p\n&quot;</span>, &amp;a);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;the address of a is:%p, which is also the value of pointer pa\n&quot;</span>, pa);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;the dereferenced value of pointer pa is:%d\n&quot;</span>, *pa);</span><br><span class="line"></span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;the address of pointer pa is:%p\n&quot;</span>, &amp;pa);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;the address of pointer pa is:%p, which is also the value of pointer ppa\n&quot;</span>, ppa);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;the first dereferenced value of pointer ppa is:%p, which is also the address of a\n&quot;</span>, *ppa);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;the second dereferenced value of pointer ppa is:%d, which is also the value of a\n&quot;</span>, **ppa);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应输出为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">the address of a is:000000000062FDE4</span><br><span class="line">the address of a is:000000000062FDE4, which is also the value of pointer pa</span><br><span class="line">the dereferenced value of pointer pa is:2</span><br><span class="line">the address of pointer pa is:000000000062FDD8</span><br><span class="line">the address of pointer pa is:000000000062FDD8, which is also the value of pointer ppa</span><br><span class="line">the first dereferenced value of pointer ppa is:000000000062FDE4, which is also the address of a</span><br><span class="line">the second dereferenced value of pointer ppa is:2, which is also the value of a</span><br></pre></td></tr></table></figure>
<p>最后建议每一次指针都加一个p,每解引用一次，也就是加一个<code>*</code>，对应划掉一个p，也就得到对该指针解引用得到的变量值。这样只是方便快速推导，因为实际的理解比较绕。</p>
<h3 id="传参的几种方式"><a href="#传参的几种方式" class="headerlink" title="传参的几种方式"></a>传参的几种方式</h3><p>函数被调用的时候，用实参的值来初始化形参。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void func1(int a)&#123;&#x2F;&#x2F;a为形参</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func1(b);&#x2F;&#x2F;b为实参</span><br></pre></td></tr></table></figure>
<p>在调用的时候可以理解为有个<code>int a = b</code>的过程。</p>
<h4 id="按值传递"><a href="#按值传递" class="headerlink" title="按值传递"></a>按值传递</h4><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func1</span><span class="params">(<span class="keyword">int</span> a)</span></span>&#123;<span class="comment">//a为形参</span></span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func1(b);<span class="comment">//b为实参</span></span><br></pre></td></tr></table></figure>
<p>使用实参的值来进行初始化，形参在被调用函数中值得变化不会影响实参。</p>
<h5 id="按指针传递"><a href="#按指针传递" class="headerlink" title="按指针传递"></a>按指针传递</h5><p>指针形参也是一种<strong>值的传递</strong>，只是因为传递的是一个对象的地址，因此可以通过解引用符<code>*</code>来对该对象直接进行修改，所以在被调用函数中可以直接根据地址来进行操作，从而使得该地址的变量被修改。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func1</span><span class="params">(<span class="keyword">int</span> *a)</span></span>&#123;<span class="comment">//a为形参</span></span><br><span class="line">	...</span><br><span class="line">	*a = <span class="number">4</span>;<span class="comment">//使用解引用符号，修改函数外面的值。</span></span><br><span class="line">    a = <span class="number">0</span>;<span class="comment">//将a指针保存的地址值从b的地址变成了0，实际参数没有任何改变。</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> b = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> *c = &amp;b;</span><br><span class="line">func1(b);<span class="comment">//b为实参</span></span><br></pre></td></tr></table></figure>
<p>指针形参，如果要修改外面的实参指向的变量值，就必须使用<code>*</code>来进行解引用。</p>
<h4 id="按引用传递"><a href="#按引用传递" class="headerlink" title="按引用传递"></a>按引用传递</h4><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func1</span><span class="params">(<span class="keyword">int</span> &amp;a)</span></span>&#123;<span class="comment">//a为形参</span></span><br><span class="line">	...</span><br><span class="line">	a = <span class="number">4</span><span class="comment">//对形参的修改会使得实参变化</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> b = <span class="number">0</span>;</span><br><span class="line">func1(b);<span class="comment">//b为实参</span></span><br></pre></td></tr></table></figure>
<p>使用引用传参，可以避免值的拷贝。指针形参和引用传递，都能够修改实参的值是因为都是根据地址来进行操作的。</p>
<h2 id="错误的原因"><a href="#错误的原因" class="headerlink" title="错误的原因"></a>错误的原因</h2><p>所以经过上面的复习，错误的原因就是：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Malloc1</span><span class="params">(<span class="keyword">char</span> *p, <span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">	p = (<span class="keyword">char</span>*)<span class="built_in">malloc</span>(num * <span class="keyword">sizeof</span>(<span class="keyword">char</span>));<span class="comment">// 传进来的地址值没有用到，没有使用解引用。</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>整个函数的作用只是在形参初始的时候把s指向的地址也就是NULL,即0这个地址赋值给了p指针。之后又把p的值从0,变为了<code>malloc</code>函数分配的一个地址。在这个函数中只有p这个形参的指向地址发生了改变，实参s指针并未发生改变。</p>
<p>所以经过这个函数的调用后，s指针存放的地址依然是0，导致后面的<code>strcpy(s, &quot;hello&quot;);</code>是对空指针进行操作，所以报错。</p>
<h2 id="正确的写法"><a href="#正确的写法" class="headerlink" title="正确的写法"></a>正确的写法</h2><p>首先明确这段代码的意图是想让s指针这个变量的值在被调用函数中发生变化，因为需要修改实参变量的值，所以需要传入实参的地址，指向指针的地址也就是一个二级指针。所以形参应该是一个二级指针，在被调用函数中使用<code>*</code>来对地址进行解引用。</p>
<figure class="highlight cc"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Malloc</span><span class="params">(<span class="keyword">char</span> **p, <span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">	*p = (<span class="keyword">char</span>*)<span class="built_in">malloc</span>(num * <span class="keyword">sizeof</span>(<span class="keyword">char</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MyTest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">char</span> *s = <span class="literal">NULL</span>;</span><br><span class="line">	Malloc(&amp;s, <span class="number">10</span>);</span><br><span class="line">	<span class="built_in">strcpy</span>(s, <span class="string">&quot;hello&quot;</span>);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;%s\n&quot;</span>, s);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>不是语法的错误,最开始以为没有错误，是因为错误地以为指针作为形参就修改了外层实参的值。但是实际上不一定，比如不加解引用符号<code>*</code>则不会修改。而就算加了解引用符号，修改的也是外层实参指向的变量的值。所以一级指针无法完成修改外层指针这个变量本身的值的目的，需要使用二级指针。</li>
<li>修改实参的值，需要传入实参的地址。修改指针的值，需要传入指针变量的地址。</li>
</ol>
]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>指针</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title>Measuring and Benchmarking Power Consumption and Energy Efficiency</title>
    <url>/2020-07-14-Measuring-and-Benchmarking-Power-Consumption-and-Energy-Efficiency.html</url>
    <content><![CDATA[<h2 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h2><p>ICPE 18</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>功率，能效，性能，基准，测量，负载水平，SPEC</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ol>
<li>显存能耗测量方法不考虑多个负载级别和工作负载组合。</li>
<li>介绍了PTDaemon 能耗测量工具和Chauffeur 能耗评测框架</li>
<li>SPEC SERT 包含的工作负载，并介绍行业标准的计算效率基准</li>
</ol>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><ol>
<li>如何精确测量能耗？</li>
<li>数据中心的冗余机制（灾难备份）导致的额外负载需要被考虑</li>
<li>SPEC 能耗方法学多重负载级别下进行能耗效率测评</li>
<li>功率测量是在SPEC PTDaemon中实现的，它与功率分析仪和温度传感器进行通信。工作负载分派，结果收集和测试执行由Chauffeur框架处理。</li>
</ol>
<hr>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="遵守的原则"><a href="#遵守的原则" class="headerlink" title="遵守的原则"></a>遵守的原则</h3><ol>
<li>可重现</li>
<li>公平</li>
<li>可验证</li>
<li>可使用</li>
</ol>
<h3 id="评测电力要求"><a href="#评测电力要求" class="headerlink" title="评测电力要求"></a>评测电力要求</h3><ol>
<li>均方根电力测量</li>
<li>每秒通过通讯接口将测量值记录到外部设备上</li>
<li>不确定性少于1%</li>
<li>定期校准至国家标准</li>
<li>设备配置控制和程序界面记录</li>
<li>能够处理安培数峰值（波峰因数），以在恶劣的功率条件下实现正确的读数</li>
</ol>
<p>考虑环境温度</p>
<h3 id="PTDaemon"><a href="#PTDaemon" class="headerlink" title="PTDaemon"></a>PTDaemon</h3><p>基于TCP-IP的公共接口集成到基准线束中，对评测软件隐藏不同硬件接口的协议和行为</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1594956254/blog/paper/批注_2020-07-15_095058.jpg" alt=""></p>
<p>通过PTDaemon与传感器进行交互（TCP/IP），设备类型由守护程序初始调用时在命令行上本地传递的参数指定。</p>
<p>SUT和PTDaemon之间的通信协议独立于特定的功率测量设备类型,可以独立于要支持的测量设备来开发基准。</p>
<p>PTDaemon实现：</p>
<ul>
<li>主进程：控制初试化网络命令界面</li>
<li>单独线程：管理功耗仪与温度传感器</li>
</ul>
<p>为了支持不同的功率分析仪，每个受支持的设备都需要在PTDaemon中拥有自己的模块。周期性的更新各个模块以增加设备支持。可以根据手册来自己提交设备支持。</p>
<h3 id="能耗效率方法学"><a href="#能耗效率方法学" class="headerlink" title="能耗效率方法学"></a>能耗效率方法学</h3><p><strong>校准步</strong> ：被测机器上的目标负载最大事务率？记为100%。负载的多层次测量。</p>
<p>负载层次定义为最大应用<strong>程序吞吐量的百分比</strong>，而不是目标CPU利用率。CPU利用率在不同核心和系统上差异较大。使用吞吐量百分比更加精确</p>
<h4 id="设备安装"><a href="#设备安装" class="headerlink" title="设备安装"></a>设备安装</h4><p>两个物理系统：</p>
<ul>
<li>控制系统：运行线束，报告器，并与外部测量设备连接。</li>
<li>被测机：运行负载</li>
</ul>
<p>通过网络进行同步，数据收集</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1594956254/blog/paper/批注_2020-07-15_105031.jpg" alt=""></p>
<p>每个逻辑CPU生成一个客户端，事务负载在客户端上单独运行。通过运行多个客户端实现并行性。</p>
<p>至少一个功耗仪与温度传感器。温度传感器确保实验环境一致。</p>
<h3 id="负载和工作量"><a href="#负载和工作量" class="headerlink" title="负载和工作量"></a>负载和工作量</h3><p>多个worklet构成一个workload，一个worklet是小规模的单元（事务）</p>
<h3 id="阶段和间隔"><a href="#阶段和间隔" class="headerlink" title="阶段和间隔"></a>阶段和间隔</h3><p>worklet的三个阶段：</p>
<ul>
<li>warmup：不记录测量值，避免瞬时影响</li>
<li>校准：得到负载的最大层次</li>
<li><p>测量：实际测量</p>
<p>每个阶段一个或多个间隔，用于阶段配置工作执行。</p>
</li>
</ul>
<p>每个间隔包含前测量期与后测量期pre-measurement and a post-measurement period.，事务被在目标层次被执行，但不记录测量值。15s</p>
<p>测量阶段在两个时期之间，120s</p>
<p>切换负载，10s的缓冲期避免影响。间隔顺序执行，渐进测量顺序（多层次性能负载的测量 ）</p>
<p>事务吞吐率：校准阶段结果和目标负载层次的百分比决定</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1594956254/blog/paper/批注_2020-07-15_111230.jpg" alt=" "></p>
<p>使用每个间隔的能耗与吞吐率的平均值计算间隔的能耗效率</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1594956254/blog/paper/批注_2020-07-15_111713.jpg" alt=""></p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="Chauffeur"><a href="#Chauffeur" class="headerlink" title="Chauffeur"></a>Chauffeur</h3><p>框架支持在多种负载下测量能耗。</p>
<p>SPEC PTDaemon与功率分析仪和温度传感器连接，Chauffeur对PTDaemon进行必要的调用，以便在适当的时间间隔内收集数据。</p>
<p>多种用途下运行的能力。与ssh2008相同原则。</p>
<p><strong>可伸缩性</strong> ：多进程与多线程，在各种服务器上提供伸缩性。支持多节点运行</p>
<p><strong>易用</strong> ： 自动配置，自动验证结果</p>
<p><strong>便携性</strong> Java实现，跨系统</p>
<p><strong>灵活性</strong> 灵活的改变运行时行为，（收集不同数据格式）通过配置文件，XML，HTML，TXT，CSV格式进行报告</p>
<h3 id="Chauffeur-WDK-测量任意负载"><a href="#Chauffeur-WDK-测量任意负载" class="headerlink" title="Chauffeur WDK 测量任意负载"></a>Chauffeur WDK 测量任意负载</h3><p>代码编写逻辑</p>
<p>使用框架简化了测试逻辑，只用专注测试逻辑。</p>
<p>自动对负载进行多层次测试</p>
<p>开发一个worklet两个组件：（代码逻辑）</p>
<ul>
<li>Transaction事务 ：worklet测试的业务逻辑 两个方法<ul>
<li>产生输入（事务的随机数据）</li>
<li>处理（接受输入然后处理并得到结果）</li>
</ul>
</li>
<li>User用户：获取状态信息</li>
</ul>
<p>支持协同worklet，多个worklet同时工作。IO写入模拟</p>
<h3 id="SERT-和负载"><a href="#SERT-和负载" class="headerlink" title="SERT 和负载"></a>SERT 和负载</h3><p>最基础的配置要求：一个功耗仪，温度传感器，SUT，Controller</p>
<p>使用Chauffeur框架。控制安装在Controller上的软件，处理能耗记录的后勤工作。</p>
<p>SUT从Chauffeur实例（Director）接收指令来执行负载集合。负载集合由Worklet组成，Worklets是实际的代码，旨在强调特定的一个或多个特定系统资源，例如CPU，内存或存储IO。</p>
<p>每个功率分析仪和温度传感器均与SPEC <strong>PTDaemon的专用实例进行交互</strong>，该SPEC PTDaemon实例在执行Worklet时收集其读数。</p>
<p><strong>报告器</strong>收集整理数据产出HTML，XML等格式的输出</p>
<p>评测的挑战：复杂的配置。（格式错误等等）</p>
<p><strong>系统配置发现</strong></p>
<p>SERT通过自动硬件发现过程和易于使用的GUI工作流程来解决这些问题，该流程可帮助用户生成高质量的准确报告。GUI减轻了测试配置，执行和报告编辑的负担，因此用户可以<strong>专注于获得结果</strong>。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1594956254/blog/paper/批注_2020-07-15_152831.jpg" alt=""></p>
<p><strong>定义和执行</strong></p>
<p>SERT由worklets套件组成，每个worklet从具体放方面测试被测机。LUworklet：CPU密集型的矩阵分解。序列IOworklet执行序列化IO操作。</p>
<p>不同方面的<strong>负载</strong>：CPU，内存和存储IO。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1594956254/blog/paper/批注_2020-07-15_153316.jpg" alt=""></p>
<p>Worklet连续执行，运行在自己的JVM或进程中避免干扰。每个JVM固定到特定处理器，以避免人为限制扩展。</p>
<p>多个JVM来运行单个worklet避免软件瓶颈限制伸缩性，主要是测量硬件的能耗不是软件的堆栈。</p>
<p><strong>Worklet</strong></p>
<p>12个</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1594956254/blog/paper/批注_2020-07-15_155322.jpg" alt=""></p>
<p><strong>CPU</strong></p>
<p>数据压缩，加密/解密，复数算法，矩阵分解，浮点数组处理，排序算法，字符串处理</p>
<p><strong>存储IO</strong></p>
<p>读与写的事务</p>
<p><strong>内存</strong></p>
<p>使用预计算和缓存的数据查找来进行XML文档的操作和验证，以及对数据转换的四个主要类别进行具有读/写操作的数组操作；</p>
<p><strong>主动怠速</strong></p>
<p>稳定阶段，空闲阶段</p>
<p>没有网络IO的worklet,由配置修改器处理</p>
<h3 id="SPECpower-ssj2008"><a href="#SPECpower-ssj2008" class="headerlink" title="SPECpower_ssj2008"></a>SPECpower_ssj2008</h3><p>略过</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>刻画了SPEC Power的方法学</p>
<p>SPEC PTDaemon可实现功耗的精确测量，而Chauffeur框架可实现工作负荷的调度，布置，执行和结果收集。<br>可以在ChauffeurWDK中使用PTDaemon和Chauffeur，以实现和测试研发工作负载的能源效率。<br>另一方面，SPEC SERT已经提供了大量可用于研究和服务器评级的工作负载。</p>
<hr>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ol>
<li><p>PTDaemon Chauffeur，SERT，ssj2008之间的关系？<a href="http://trickmore.blogspot.com/search/label/SERT">参考</a> <a href="https://zhuanlan.zhihu.com/p/45518506">参考2</a></p>
<ol>
<li>ssj是基础，SERT通过PTDaemon</li>
</ol>
</li>
<li><p>支持比较多的功耗仪，定期更新（优点），配置性高</p>
</li>
<li><p>transaction?怎么翻译？ 事务一个工作流程，或模块</p>
<ol>
<li>一次数据压缩就是一个transaction</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>能耗</tag>
        <tag>SPEC SERT</tag>
      </tags>
  </entry>
  <entry>
    <title>Extremely Large Minibatch SGD:Training ResNet-50 on ImageNet in 15 Minutes</title>
    <url>/2020-06-02-Extremely-Large-Minibatch-SGDTraining-ResNet-50-on-ImageNet-in-15-Minutes.html</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文通过将minibatch size提高到32k实现了在ImageNET上用15分钟训练完ResNet50。为了保证精读使用了RMSprop warm-up,batch normalization，以及slow-start learning rate schedule.</p>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>训练深度神经网络在计算上是昂贵的。更高的可扩展性（更大的数据集和更复杂的模型）和更高的生产率（更短的培训时间以及更快的试验和错误）要求通过<strong>分布式计算</strong>来加速。<br>本文证明，在不影响准确性经过精心设计的软件和硬件系统的情况下，使用large batch可以进行高度并行的训练。</p>
<hr>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>主要使用了《Accurate, large minibatch SGD: training ImageNet in 1 hour》文章的方法。</p>
<h3 id="RMSprop-Warm-up"><a href="#RMSprop-Warm-up" class="headerlink" title="RMSprop Warm-up"></a>RMSprop Warm-up</h3><p>本文发现主要的挑战是训练开始时的优化难度。为了解决这个问题，本文从RMSprop [7]开始训练，然后逐步过渡到SGD。</p>
<p>momentum SGD 和RMSprop 的结合</p>
<script type="math/tex; mode=display">
\begin{aligned}
m_{t} &=\mu_{2} m_{t-1}+\left(1-\mu_{2}\right) g_{t}^{2} \\
\Delta_{t} &=\mu_{1} \Delta_{t-1}-\left(\alpha_{\mathrm{SGD}}+\frac{\alpha_{\mathrm{RMSprop}}}{\sqrt{m_{t}}+\varepsilon}\right) g_{t}, \text { and } \\
\theta_{t} &=\theta_{t-1}+\eta \Delta_{t}
\end{aligned}</script><p>从RMSprop 开始，然后切换到SGD(需要平滑切换，突然切换有副作用)</p>
<p>对$\alpha_{SGD}$的调度，类似与ELU激活函数：</p>
<script type="math/tex; mode=display">
\alpha_{\mathrm{SGD}}=\left\{\begin{array}{ll}
\frac{1}{2} \exp \left(2\left(\mathrm{epoch}-\beta_{\mathrm{center}}\right) / \beta_{\mathrm{period}}\right) & \left(\mathrm{epoch}<\beta_{\mathrm{center}}\right) \\
\frac{1}{2}+2\left(\mathrm{epoch}-\beta_{\mathrm{center}}\right) / \beta_{\mathrm{period}} & \left(\mathrm{epoch}<\beta_{\mathrm{center}}+\frac{1}{2} \beta_{\mathrm{period}}\right) \\
1 & (\text { otherwise })
\end{array}\right.</script><p>$\alpha_{SGD}$先指数增长，之后到达$\beta_{center}$epoch后达到1/2,之后线性增长到$\beta_{center} + \frac{1}{2}\beta_{period}$ epoch, 超参数$\beta_{center}=10,\beta_{period}=5$</p>
<h3 id="Slow-Start-Learning-Rate-Schedule"><a href="#Slow-Start-Learning-Rate-Schedule" class="headerlink" title="Slow-Start Learning Rate Schedule"></a>Slow-Start Learning Rate Schedule</h3><p>为了进一步克服最初的优化困难，本文使用略微修改的学习率时间表，以延长初始阶段并降低初始学习率</p>
<p>初始学习率</p>
<script type="math/tex; mode=display">
\eta_{base} =0.1 \cdot \frac{b_{\text {botal }}}{256}=0.1 \cdot \frac{n b_{\text {local }}}{256}</script><p>$b_{local}$ 是一个worker的batch size。</p>
<p>SGD前40个epoch用$0.5\cdot\eta_{base}$,随后的30个epoch为$0.075\cdot\eta_{base}$,接下来的15个epoch $0.01\cdot\eta_{base}$,最后5个为$0.001\cdot\eta_{base}$</p>
<h3 id="Batch-Normalization-without-Moving-Averages"><a href="#Batch-Normalization-without-Moving-Averages" class="headerlink" title="Batch Normalization without Moving Averages"></a>Batch Normalization without Moving Averages</h3><p>随着batch size的增加，均值和方差的batch normalization会移动平均值导致实际均值和方差的不准确估计。为了解决这个问题，本文只考虑了最后的minibatch，而不是移动平均数，并且对这些统计数据使用全归约通信来获得验证之前所有Workers的平均数。</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><strong>软件</strong>：使用Chainer和ChanierMN Chainer是一个开源的深度学习框架，具有“按运行定义”方法。 ChainerMN是Chainer的附加软件包，可实现具有同步数据并行性的多节点分布式深度学习。作为底层的通信库，本文使用了NCCL 2.0.5版和Open MPI 1.10.2版。</p>
<p>使用半精度浮点数来减少通信开销，对模型准确的影响较小。</p>
<p><strong>硬件</strong>： 使用MN-1内部集群。包含128个节点。</p>
<p><strong>训练时间</strong></p>
<p>通信和迭代时间</p>
<p><img src="/2020-06-02-Extremely-Large-Minibatch-SGDTraining-ResNet-50-on-ImageNet-in-15-Minutes.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-02 175529.jpg" alt=""></p>
<p><img src="/2020-06-02-Extremely-Large-Minibatch-SGDTraining-ResNet-50-on-ImageNet-in-15-Minutes.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-02 175659.jpg" alt=""></p>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><hr>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ol>
<li>使用的软件不常用可以替换为Pytorch？</li>
<li>比较的软件框架，硬件都不一致，是否有说服力？有没有统一平台的benchmark？</li>
</ol>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>Large Batch</tag>
      </tags>
  </entry>
  <entry>
    <title>Convolutional Neural Networks for Sentence Classification</title>
    <url>/2020-06-02-Convolutional-Neural-Networks-for-Sentence-Classification.html</url>
    <content><![CDATA[<h2 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h2><p>2014 EMNLP</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文报告了一系列在卷积神经网络（CNN）上进行的一系列实验，这些卷积神经网络在针对句子级别分类任务的预训练词向量之上进行了训练。本文表明，几乎没有超参数调整和静态矢量的简单CNN在多个基准上均能获得出色的结果。通过微调学习特定于任务的向量可进一步提高性能。<br>本文另外建议对体系结构进行简单的修改，以允许使用特定于任务的向量和静态向量。<br>本文讨论的CNN模型在7个任务中的4个改进了现有技术，其中包括情感分析和问题分类。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><hr>
<h2 id="图表"><a href="#图表" class="headerlink" title="图表"></a>图表</h2><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在自然语言处理中，深度学习方法的许多工作都涉及通过神经语言模型学习<strong>单词向量</strong>表示并对学习的词向量进行合成以进行分类。Word Vector：本质上是特征提取器，可在其维度上编码单词的<strong>语义特征</strong>。在这种密集表示中，语义上接近的词在较低维向量空间中同样接近（以欧几里得或余弦距离）。</p>
<p>CNN模型在语义解析，搜索查询，句子建模以及其他的一些NLP任务中取得了出色的结果。</p>
<p>本文从无监督的神经语言模型获得的单词向量之上训练一个具有卷积层的简单CNN。单词向量为Mikolov等从1000亿单词的谷歌新闻中预训练得到。使<strong>单词向量保持静态</strong>，仅学习模型的其他参数。尽管对超参数的调整很少，但这个简单的模型在多个基准上均能获得出色的结果，这表明<strong>预训练的向量是“通用”特征提取器</strong>，可用于各种分类任务。通过微调学习特定于任务的向量可以进一步改进。最后，本文描述了对体系结构的简单修改，以通过具有多个通道来允许使用预训练向量和特定于任务的向量。</p>
<hr>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="/2020-06-02-Convolutional-Neural-Networks-for-Sentence-Classification.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-02 145754.jpg" alt="架构图"></p>
<p>采用多种不同尺寸的卷积核，对每个卷积核提取的特征图使用最大池化，最后使用FNN和softmax层进行分类。</p>
<p>双通道的word vector，一个保持不变，一个会在bp反向后微调。</p>
<h3 id="规则化"><a href="#规则化" class="headerlink" title="规则化"></a>规则化</h3><p>在卷积层使用了dropout，以及l2正则。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在不同的benchmarks上进行了测试。</p>
<p><img src="/2020-06-02-Convolutional-Neural-Networks-for-Sentence-Classification.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-02 151330.jpg" alt=""></p>
<p><strong>超参数和训练</strong></p>
<p>卷积的尺寸为3,4,5，dropout 概率0.5，l2正则系数3，batch size为50，通过grid search搜索得到的。</p>
<p><strong>预训练词向量</strong></p>
<p>使用Word2vec，300维的向量，用词带模型训练。未被表示的单词，随机初始化。</p>
<p><strong>模型变种</strong></p>
<p>CNN-rand：所有单词都随机初始化</p>
<p>CNN-static: 使用word2vec来初始化，并不再改变</p>
<p>CNN-non-static: 使用word2vec来初始化，在每个任务中改变</p>
<p>CNN-multichannel: 一个通道的改变，一个不改变。</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="/2020-06-02-Convolutional-Neural-Networks-for-Sentence-Classification.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-02 153818.jpg" alt=""></p>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
  </entry>
  <entry>
    <title>LARGE BATCH OPTIMIZATION FOR DEEP LEARNING TRAINING BERT IN 76 MINUTES</title>
    <url>/2020-06-01-LARGE-BATCH-OPTIMIZATION-FOR-DEEP-LEARNING-TRAINING-BERT-IN-76-MINUTES.html</url>
    <content><![CDATA[<h2 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h2><p>ICLR 2020</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Large Batch加速训练的方法LARS，在注意力机制模型比如BERT上表现不好，性能提升在各个任务中表现不一致。本文首先研究一种有原则的分层自适应策略，以使用Large mini-batches来加快深度神经网络的训练。使用这种策略，本文开发了一种称为<strong>LAMB</strong>的新的分层自适应大批量优化技术。本文提供LAMB以及LARS的<strong>收敛分析</strong>，表明算法可以在一般非凸设置下的收敛到固定点。</p>
<p>LAMB在各种任务（例如BERT和RESNET-50训练）中的表现出色，而<strong>超参数调整却非常少</strong>。在BERT训练中，本文的优化程序可以使用非常大的32868批量，而不会降低性能。通过batch size增加到TPUv3 Pod的内存限制，BERT训练时间可以从3天减少到仅76分钟。 LAMB实现公布在<a href="[https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py">网上</a> )</p>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>随着大规模数据集的出现，即使使用诸如梯度梯度下降（SGD）等计算有效的优化方法来训练大型深度神经网络也变得特别具有挑战性。即训练大型模型的时间变得非常长。</p>
<p>本文的<strong>目标</strong>是研究和开发优化技术，以加速训练大型深度神经网络，其中主要集中在基于SGD变体的方法上。SGD的可扩展性受到其固有顺序性的限制。由于存在这种局限性，在深度学习背景下改善SGD培训时间的传统方法主要采用<strong>分布式异步</strong>设置，但是，由于异步引入的隐式过时（stalenes）限制了方法的并行化，通常会导致性能下降。简单地增加batch size 虽然可以增加速度，但是会降低泛化性能并降低计算收益。</p>
<p>Large mini-batch上的同步SGD受益于SGD中使用的随机梯度变化的减少，从而可以在SGD中使用更大的学习率，根据batch size的具体大小<strong>线性调整学习率</strong>可以进一步加快训练。但是线性缩放调整学习率的<strong>缺点</strong>：（i）在初始阶段，学习率的线性缩放是有害的；因此，最初需要使用手动调整的缓慢增加学习速度的预热策略，并且（ii）学习速度的线性缩放在超过一定的batch size之后也是有害的。</p>
<p>最近提出了使用<strong>分层自适应学习率</strong>的SGD变体来解决此问题，比如非常出名的LARS算法。但该算法的性能在跨任务上不一致，比如BERT上表现糟糕。此外LARS的适应方法，还缺少很多理论分析。</p>
<p><strong>本文贡献</strong></p>
<ol>
<li>在LARS的启发下，本文研究了专门针对大批量学习的通用适应策略，并为该策略提供了直觉</li>
<li>基于适应策略，本文开发了一种新的优化算法（LAMB），以实现SGD中学习率的适应性。此外，本文为LARS和LAMB都提供了收敛分析，以在非凸设置中达到固定点。<br>本文重点介绍了将这些方法用于大批量设置的好处。</li>
<li>本文展示了LAMB在多个挑战性任务中的强大性能。使用LAMB，本文将训练BERT的批量大小扩展到32k以上，而不会降低性能。时间从3天减少到76分钟。本文的工作是将BERT训练时间减少到几个小时以内的第一项工作。</li>
<li>本文还展示了LAMB训练像RESNET这样的最新图像分类模型的效率。本文是第一个可以为RESNET-50达到SOTA的自适应求解器，因为像Adam这样的自适应求解器无法获得这些任务的SGD精度。</li>
</ol>
<hr>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3><p>完整的推导见论文，</p>
<p>本文首先将优化问题定义为：非凸随机优化问题</p>
<script type="math/tex; mode=display">
\min _{x \in \mathbb{R}^{d}} f(x):=\mathbb{E}_{s \sim \mathbb{P}}[\ell(x, s)]+\frac{\lambda}{2}\|x\|^{2}</script><p>SGD方法是解决上述问题最简单一阶算法：</p>
<script type="math/tex; mode=display">
x_{t+1}=x_{t}-\eta_{t} \frac{1}{\left|\mathcal{S}_{t}\right|} \sum_{s_{t} \in \mathcal{S}_{t}} \nabla \ell\left(x_{t}, s_{t}\right)+\lambda x_{t}</script><p>对于large batch b = T并使用适当的学习率，对于SGD的迭代，有：</p>
<script type="math/tex; mode=display">
\mathbb{E}\left[\left\|\nabla f\left(x_{a}\right)\right\|^{2}\right] \leq O\left(\frac{\left(f\left(x_{1}\right)-f\left(x^{*}\right)\right) L_{\infty}}{T}+\frac{\|\sigma\|^{2}}{T}\right)</script><p>在实践中很难调整SGD中的学习速率，尤其是在大批量设置中。此外，对$ L_{\infty}$的依赖（尺寸上最大的平滑度）可能会导致收敛速度显着降低。</p>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><h4 id="常规算法"><a href="#常规算法" class="headerlink" title="常规算法"></a>常规算法</h4><p>一个基础的优化算法(SGD,ADAM)的迭代策略是：</p>
<script type="math/tex; mode=display">
x_{t+1}=x_{t}+\eta_{t} u_{t}</script><p>$ u_{t}$是t时间步的更新量，文章建议对Large batch进行两项更改。</p>
<ol>
<li>更新量使用l2正则，即使用$u_t/|u_t|$,逐层完成的。</li>
<li>学习率使用函数$\phi\left(\left|x_{t}\right|\right)$ 进行调整。也是逐层完成。</li>
</ol>
<p>修改后的SGD更新策略：</p>
<script type="math/tex; mode=display">
x_{t+1}^{(i)}=x_{t}^{(i)}-\eta_{t} \frac{\phi\left(\left\|x_{t}^{(i)}\right\|\right)}{\left\|g_{t}^{(i)}\right\|} g_{t}^{(i)}</script><h4 id="LARS-算法"><a href="#LARS-算法" class="headerlink" title="LARS 算法"></a>LARS 算法</h4><p><img src="/2020-06-01-LARGE-BATCH-OPTIMIZATION-FOR-DEEP-LEARNING-TRAINING-BERT-IN-76-MINUTES.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-01 151610.jpg" alt=""></p>
<p>文章对LARS算法提供了收敛分析</p>
<p><img src="/2020-06-01-LARGE-BATCH-OPTIMIZATION-FOR-DEEP-LEARNING-TRAINING-BERT-IN-76-MINUTES.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-01 152422.jpg" alt=""></p>
<h4 id="LAMB-算法"><a href="#LAMB-算法" class="headerlink" title="LAMB 算法"></a>LAMB 算法</h4><p>伪代码：</p>
<p><img src="/2020-06-01-LARGE-BATCH-OPTIMIZATION-FOR-DEEP-LEARNING-TRAINING-BERT-IN-76-MINUTES.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-01 152156.jpg" alt=""></p>
<p>与LARS算法不同：LAMB的适应性有两个方面：（i）关于ADAM中使用的第二个时刻的平方根的每维归一化；以及（ii）由于分层适应性而获得的分层归一化。</p>
<p>收敛性证明：</p>
<p><img src="/2020-06-01-LARGE-BATCH-OPTIMIZATION-FOR-DEEP-LEARNING-TRAINING-BERT-IN-76-MINUTES.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-01 152507.jpg" alt=""></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>模型：BERT和RESNET-50</p>
<p>参数设置 $\beta_1 = 0.9,\beta_2=0.999$，只微调了学习率。多项式衰减的学习率：$\eta_{t}=\eta_{0} \times(1-t / T)$,在batch size增大的时候没有再次微调超参数。使用LR缩放规则的平方根来自动调整学习率和线性epoch的warm up。</p>
<p>实验平台：TPUv3 Pod。</p>
<p>使用了grid search 来调整学习方法ADAM，ADAGRAD，LARS等的超参数。</p>
<h3 id="BERT训练"><a href="#BERT训练" class="headerlink" title="BERT训练"></a>BERT训练</h3><p>数据集：与BERT原文的一致，主要关注SQuAD任务。使用F1分数作为指标，对比的模型是BERT 论文中的baseline。</p>
<p>训练过程：除了修改优化函数LAMB为，其余均一致。</p>
<p><img src="/2020-06-01-LARGE-BATCH-OPTIMIZATION-FOR-DEEP-LEARNING-TRAINING-BERT-IN-76-MINUTES.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-01 162220.jpg" alt=""></p>
<p>对LAMB使用了Mixed-Batch 训练</p>
<p>BERT训练的第一阶段：batch size限制到65536，因为进一步加大并没有更好的加速效果。第二阶段重新对学习率进行warm up。</p>
<p><strong>与ADAMW 和LARS的比较</strong></p>
<p><img src="/2020-06-01-LARGE-BATCH-OPTIMIZATION-FOR-DEEP-LEARNING-TRAINING-BERT-IN-76-MINUTES.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-01 163643.jpg" alt=""></p>
<h3 id="IMAGENET上的RESNET-50训练"><a href="#IMAGENET上的RESNET-50训练" class="headerlink" title="IMAGENET上的RESNET-50训练"></a>IMAGENET上的RESNET-50训练</h3><p>各种优化函数的对比</p>
<p><img src="/2020-06-01-LARGE-BATCH-OPTIMIZATION-FOR-DEEP-LEARNING-TRAINING-BERT-IN-76-MINUTES.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-01 171012.jpg" alt=""></p>
<h3 id="超参数调整"><a href="#超参数调整" class="headerlink" title="超参数调整"></a>超参数调整</h3><p>自动调整的学习率：</p>
<p><img src="/2020-06-01-LARGE-BATCH-OPTIMIZATION-FOR-DEEP-LEARNING-TRAINING-BERT-IN-76-MINUTES.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-01 171256.jpg" alt=""></p>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><hr>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ol>
<li>本文是第一项对BERT的优化工作，以往的工作主要针对ResNet等视觉模型，对于新推出的GPT-3有没有相关工作？</li>
</ol>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
  </entry>
  <entry>
    <title>DEEP MULTIMODAL LEARNING FOR EMOTION RECOGNITION IN SPOKEN LANGUAGE</title>
    <url>/2020-05-30-DEEP-MULTIMODAL-LEARNING-FOR-EMOTION-RECOGNITION-IN-SPOKEN-LANGUAGE.html</url>
    <content><![CDATA[<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>情感识别，口语，多模态学习</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>提出了一种新颖的深度多模态框架，用于基于句子级的口头语言来预测人类情感。本文的架构具有两个鲜明的特点。<strong>首先</strong>，它通过混合的深度多模态结构<strong>从文本和音频中提取高级特征</strong>，该结构考虑了文本的空间信息，音频的时间信息以及低层次的人工特征的高级关联。<br>其次，本文使用三层深度神经网络融合所有特征，以学习各种模态之间的相关性，并将特征提取和融合模块一起训练，从而实现整个结构的最佳全局微调。<br>在IEMOCAP数据集上评估了提出的框架。五个情感类别的加权准确率达到60.4％。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>混合的深度框架，包括卷积网络，CNN-LSTM和DNN来从文本和音频中提取空间和时间信息以及升学特征。</p>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>人类的言语表达了内容和态度。通过语音交流时，人们自然会同时吸收内容和情感，以理解说话者的实际意图。情感识别被定义为从人类中提取一组情感状态，对于在人机交互中自动检测人类的意义是必需的。语音情感识别是情感计算领域中的一种，它从<strong>语音中提取情感状态</strong>，并揭示口头语言下的态度。</p>
<p>和计算机视觉领域的情感识别相比，结合文本和音频形式的<strong>工作相对较少</strong>。为了检测话语中的情绪，人们经常同时考虑文字含义和韵律。因此使用<strong>多模态架构</strong>来考虑文本和音频输入是有必要的。</p>
<h3 id="挑战与相关工作"><a href="#挑战与相关工作" class="headerlink" title="挑战与相关工作"></a><strong>挑战与相关工作</strong></h3><p>情感识别<strong>挑战1</strong>是从语音数据中提取有效特征。可以通过OpenSmile软件提取数千个具有功能统计信息的低级声学描述符和派生词（LLD），提取出来的低层次特征很难代表高级关联，被认为不足以区分情感。可以使用CNN从Word embedding中提取高级文本特征以表示文本特征,但是没有考虑时序关联。</p>
<p><strong>挑战2</strong>多模态的融合，有两种融合策略：特征层融合以及决策层融合。特征层融合在决策之前合并单个特征表示，从而显着提高了性能。然而，直接将连接的特征馈送到分类器中或使用浅层融合模型，这些模型难以学习不同模态之间的复杂相互关系。由三个受限玻尔兹曼机器层组成的深度置信网络通过融合高级视听功能，比浅层融合模型具有更好的性能，但是这种方法分离了特征提取和特征融合的训练阶段。这种方法的最大问题是它不能保证参数的全局调整，即不能通过预测的损失来调整特征提取模块的参数。</p>
<h3 id="本文工作"><a href="#本文工作" class="headerlink" title="本文工作"></a>本文工作</h3><p>本文建立了一个混合的深度模型结构。使用了CNN来从单词和词类标签中提取文本特征，CNN-LSTM架构从Mel频谱系数（MFSC）能量图捕获声学的时空特征，而三层深度神经网络可从低级手工特征中学习高级声学关联。然后，本文使用三层深度神经网络<strong>将所有提取的特征连接</strong>起来，以学习跨模态的相互关系，并通过softmax分类器对情绪进行分类。<br>本文直接将特征提取模块和融合模型一起训练，以便最终损失可以适当地用于调整所有参数。<br>所提出的结构在IEMOCAP多模态数据集上针对五种情绪达到了60.4％的加权准确度。</p>
<hr>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>网络的的整体架构：</p>
<p><img src="/2020-05-30-DEEP-MULTIMODAL-LEARNING-FOR-EMOTION-RECOGNITION-IN-SPOKEN-LANGUAGE.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-05-30 162811.jpg" alt=""></p>
<p>框架由三个模块组成：数据预处理，特征提取和特征融合。</p>
<p>数据预处理模块处理输入的语音流，并输出相应的文本语句，词性标签，音频信号和提取的低级手工声学特征。</p>
<p>特征提取层从单词，词类标签，语言型号，低层次人工特征四个分支中各自提取特征。</p>
<p>特征融合模块：将输出特征串联起来作为一个联合特征表示，并通过一个深度神经网络学习相互关系。</p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>将输入语音流分为句子级文本和相应的音频片段，使用NLTK来提取每一句的词类标签POS，删除了标点符号，对语言信号同时使用OpenSmile提取低水平的音调和人声相关特征。基本频率，与音调/能量相关的特征，零交叉率（ZCR），抖动，微光，梅尔频率倒谱系数（MFCC），一些统计学特征：平坦度，偏度，四分位数，标准差，均方根,总共6382维的特征。</p>
<h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p><strong>Word embedding</strong>：使用Word2vec（从谷歌新闻预训练），zero padding填充，一个卷积层后一个最大池化层，使用多种尺寸的卷积核（2,3,4,5的尺寸），每种尺寸都有256个卷积核，最终的特征向量1024维。</p>
<p><strong>POS embedding</strong>：使用自己的POS标签数据进行训练。将POS编码为10维，然后卷积架构同Word embedding，最终也是1024维</p>
<p><strong>语言信号</strong>：</p>
<p><img src="/2020-05-30-DEEP-MULTIMODAL-LEARNING-FOR-EMOTION-RECOGNITION-IN-SPOKEN-LANGUAGE.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-05-31 101851.jpg" alt=""></p>
<p>首先提取MFSC系数，使用64个滤波器组来提取MFSC，并提取了增量系数和双增量系数。选择64作为上下文窗口大小，选择15帧作为移动窗口以分割整个MFSC图。4维度的MFSC图$n×64×64×3$ n是窗口数量。8层的ConvNet，来提取MFSC图中的<strong>空间关联</strong>，4个卷积层，4个最大池化层。卷积层的卷积核尺寸$3<em>3$，池化层的尺寸$2</em>2$,之后使用全连接层与Dense 层来将特征关联，最后在使用LSTM来提取<strong>时序关联</strong>，选择最后一层LSTM的隐藏层状态作为特征向量，1024维。</p>
<p><strong>韵律和声音质量的低级特征</strong></p>
<p>使用三层的神经网络来提取低级特征的关联，使用了Max-min normalization，输入层6382维，隐藏层2048和1024.</p>
<h3 id="特征融合"><a href="#特征融合" class="headerlink" title="特征融合"></a>特征融合</h3><p>3层神经网络，一个softmax层，进行融合并分类。2个隐藏层维度2048和1024.使用线性SVM来替换Softmax没有明显的提升。</p>
<h3 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h3><p>直接将特征提取和融合模块一起训练。使用ReLU激活函数，以及Dropout来防止过拟合。lr初始为0.01，Adam优化函数，分类交叉熵损失函数。</p>
<p>另一个问题是<strong>内部协变量偏移</strong>，内部协变量偏移定义为由于训练期间网络参数的变化而导致的网络激活分布的变化，在每层中使用batch normalization来更好地学习分布，提高训练效率。</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>在Interactive Emotional Dyadic Motion Capture Database (IEMOCAP)实验，只考虑音频和文本数据。3个注释者对句子情感打标签，包括快乐，悲伤，中立，愤怒，惊讶，激动，沮丧，厌恶，恐惧和其他。仅使用带有至少两个同意的情感标签的句子。最终合并为<strong>4个</strong>：1213 Hap, 1032 Sad (sad), 1084 Ang (anger), 774 Neu (neutral), and 1136 Fru (frustration). 5折交叉验证。</p>
<p>文章各种方法单独使用的精确度比较：</p>
<p><img src="/2020-05-30-DEEP-MULTIMODAL-LEARNING-FOR-EMOTION-RECOGNITION-IN-SPOKEN-LANGUAGE.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-05-31 105359.jpg" alt=""></p>
<p>本文方法与其他人的工作比较：</p>
<p><img src="/2020-05-30-DEEP-MULTIMODAL-LEARNING-FOR-EMOTION-RECOGNITION-IN-SPOKEN-LANGUAGE.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-05-31 105551.jpg" alt=""></p>
<hr>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ol>
<li>特征那么多是否需要特征降维。</li>
<li>Dense Layer 和FNN 的区别？</li>
<li>只有3个注释者来打标签，标签是否一定合理？</li>
</ol>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>情感识别</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Architectures for Named Entity Recognition</title>
    <url>/2020-05-28-Neural-Architectures-for-Named-Entity-Recognition.html</url>
    <content><![CDATA[<h2 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h2><h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>最先进的命名实体识别系统在很大程度上<strong>依赖手工制作的功能和特定领域的知识</strong>，以便从可用的受监督的小型培训资料库中有效学习。<br>本文介绍了两种新的神经体系结构-一种基于双向LSTM和条件随机场，另一种采用受移位减少解析器启发的基于过渡的方法构造和标记片段。<br>本文的模型依赖于两个有关单词的<strong>信息资源</strong>：从<strong>监督语料库</strong>学习的基于字符的单词表示形式和从<strong>无注释语料库</strong>学习的无监督单词表示形式。<br>文章模型无需使用任何特定于语言的知识或资源（例如，地名词典），就可以使用四种语言在NER中获得当前最优的性能。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>性能最优，即使与使用了额外资源的模型相比。</p>
<p>本文模型的一个关键方面是，它们可以通过简单的CRF体系结构或使用基于过渡的算法来显式构造和标记输入块来对输出标签依赖性进行建模。<br><strong>单词表示形式</strong>对于成功也至关重要。既使用<strong>预训练</strong>的单词表示形式，又使用“<strong>基于字符</strong>的”表示形式来捕获形态学和正字法信息。<br>为了防止学习器过于依赖一个表示形式类，使用了Dropout。</p>
<hr>
<h2 id="图表"><a href="#图表" class="headerlink" title="图表"></a>图表</h2><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>命名实体识别（NER）是一个具有<strong>挑战性</strong>的学习问题。<br>一方面，在大多数语言和领域中，只有很<strong>少量的监督训练数据</strong>可用。<br>另一方面，对于可以作为命名实体的单词种类<strong>几乎没有限制</strong>，因此很难从这种小的数据样本中进行概括。<br><strong>当前方法</strong>：使用精心构造的字法特征和特定于语言的知识资源（例如地名词典），被广泛用于解决此任务。<br><strong>问题</strong>在新的语言和新的领域中开发特定于语言的资源和功能的成本很高，使得这种方法的<strong>泛化性很差</strong></p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>在本文中，本文介绍了针对NER的神经体系结构，除了少量的有监督的训练数据和未标记的语料库之外，它不使用语言特定的资源或功能。<br>本文的模型旨在捕获<strong>两种直觉</strong>。<br><strong>首先</strong>，由于名称通常由多个标记组成，因此对每个标记的<strong>标记决策</strong>进行<strong>联合推理</strong>非常重要。文章比较了两个模型，（i）双向LSTM，其上有一个序列条件随机层LSTM-CRF，以及（ii）一种新模型，该模型使用受转换启发的算法来构造和标记输入语句的块，基于堆栈的LSTM （S-LSTM）表示的状态来进行解析。</p>
<p><strong>其次</strong>，“是一个命名实体”的令牌级别的证据既包括<strong>拼写证据</strong>（被标记为名字的单词看起来像什么？）又包括<strong>分布证据 </strong>（被标记的单词倾向于在语料库中出现在哪里？）。</p>
<p>为了捕获正字法敏感性，使用<strong>基于字符的单词表示模型</strong>来捕<strong>获分布敏感性</strong>，将这些<strong>表示法与分布表示法结合起来</strong></p>
<p>本文的单词表示法将这两种方式结合在一起，并采用Dropout训练来鼓励模型学习信任两种证据来源</p>
<hr>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="LSTM-CRF模型"><a href="#LSTM-CRF模型" class="headerlink" title="LSTM-CRF模型"></a>LSTM-CRF模型</h3><p><strong>LSTM实现：</strong></p>
<p>四个状态门的计算：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{i}_{t}&= \sigma\left(\mathbf{W}_{x i} \mathbf{x}_{t}+\mathbf{W}_{h i} \mathbf{h}_{t-1}+\mathbf{W}_{c i} \mathbf{c}_{t-1}+\mathbf{b}_{i}\right) \\
\mathbf{c}_{t}&=\left(1-\mathbf{i}_{t}\right) \odot \mathbf{c}_{t-1}+\\
& \mathbf{i}_{t} | \odot \tanh \left(\mathbf{W}_{x c} \mathbf{x}_{t}+\mathbf{W}_{h c} \mathbf{h}_{t-1}+\mathbf{b}_{c}\right) \\
\mathbf{o}_{t}&=\sigma\left(\mathbf{W}_{x o} \mathbf{x}_{t}+\mathbf{W}_{h o} \mathbf{h}_{t-1}+\mathbf{W}_{c o} \mathbf{c}_{t}+\mathbf{b}_{o}\right) \\
\mathbf{h}_{t}&=\mathbf{o}_{t} \odot \tanh \left(\mathbf{c}_{t}\right)
\end{aligned}</script><p><strong>CRF标签模型</strong></p>
<p>直接将双向LSTM模型的输出$h_t$ 作为特征来进行tagging决策，在POS 标注上取得了成功，当输出标签之间存在<strong>很强的依赖性</strong>时，其独立的分类决策会受到限制，比如NER任务。因为特征标签的可解释序列的“语法”施加了几个严格的约束（例如，I-PER不能遵循B-LOC；这将无法使用<strong>独立性假设</strong>进行建模。</p>
<p>使用CRF来替代独立建模，对于输入的句子序列：</p>
<script type="math/tex; mode=display">
\mathbf{X}=\left(\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{n}\right)</script><p> P是双向LSTM网络输出的得分矩阵。 P的大小为$n*k$，其中k为不同标签的数量，$P_{i,j}$对应于句子中第i个单词的第j个标签的分数。<br>对于预测的序列：</p>
<script type="math/tex; mode=display">
\begin{equation}\mathbf{y}=\left(y_{1}, y_{2}, \ldots, y_{n}\right)\end{equation}</script><p>将这个序列的得分记为：</p>
<script type="math/tex; mode=display">
\begin{equation}s(\mathbf{X}, \mathbf{y})=\sum_{i=0}^{n} A_{y_{i}, y_{i+1}}+\sum_{i=1}^{n} P_{i, y_{i}}\end{equation}</script><p>其中A是转移得分的矩阵，$A_{i,j}$表示从标签i到标签j的转移得分。 $y_0$和$y_n$是句子的开始和结束标记，本文将其添加到可能的标记集中。因此，A是大小为$(k + 2)*(k+2)$的方阵。</p>
<p>对所有可能的标签序列用softmax来计算出对应的概率。</p>
<script type="math/tex; mode=display">
p(\mathbf{y} | \mathbf{X})=\frac{e^{s(\mathbf{X}, \mathbf{y})}}{\sum_{\widetilde{\mathbf{y}} \in \mathbf{Y}_{\mathbf{X}}} e^{s(\mathbf{X}, \overline)}}</script><p>在训练过程中最大化正确标签序列的对数概率：公式1</p>
<script type="math/tex; mode=display">
\begin{aligned}
\log (p(\mathbf{y} | \mathbf{X})) &=s(\mathbf{X}, \mathbf{y})-\log \left(\sum_{\tilde{\mathbf{y}} \in \mathbf{Y}_{\mathbf{X}}} e^{s(\mathbf{X}, \tilde{\mathbf{y}})}\right) \\
&=s(\mathbf{X}, \mathbf{y})-\underset{\widetilde{\mathbf{y}} \in \mathbf{Y}_{\mathbf{X}}}{\operatorname{logadd} s(\mathbf{X}, \widetilde{\mathbf{y}})}
\end{aligned}</script><p>其中$Y_{X}$代表句子X的所有可能的标记序列（包括那些不符合IOB格式的标记序列）。鼓励本文的网络产生有效的输出标签序列。在解码时，输出有最大得分的预测序列。公式2</p>
<script type="math/tex; mode=display">
\mathbf{y}^{*}=\underset{\tilde{\mathbf{y}} \in \mathbf{Y}_{\mathbf{X}}}{\operatorname{argmax}} s(\mathbf{X}, \widetilde{\mathbf{y}})</script><p>仅对输出之间的二元组交互进行建模，公式1和公式2可以用动态规划来计算。</p>
<p><strong>参数化和训练</strong></p>
<p>与每个标记$P_{i,j}$的每个标记决策相关联的分数定义为单词上下文嵌入之间的点积。</p>
<p>网络架构图：</p>
<p><img src="/2020-05-28-Neural-Architectures-for-Named-Entity-Recognition.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-05-28 172526.jpg" alt=""></p>
<p>圆圈表示观察到的变量，菱形是其父代的确定性函数，双圆圈是随机变量。</p>
<p>该模型的参数是2元组兼容性分数A的矩阵，以及产生矩阵P的参数，即双向LSTM的参数，线性特征权重和词嵌入。<br>将序列的词嵌入作为输入。</p>
<p><strong>标记方案</strong></p>
<p>命名实体识别的任务是为句子中的每个单词分配一个命名实体标签。单个命名实体可以跨越一个句子中的多个标记。句子通常以IOB格式（内部，外部，开头）表示，其中，如果令牌是命名实体的开头，则每个令牌都标记为B标签；如果令牌在命名实体中，而不是第一个令牌，则将其标记为I标签，在指定实体内，否则为O。</p>
<p>本文使用<strong>IOBES标记方案</strong>，这是通常用于命名实体识别的IOB的变体，它编码有关单例实体（S）的信息并显式标记命名实体（E）的末尾。</p>
<p>使用此方案，以高置信度将一个单词标记为I-label可以将后续单词的选择范围缩小到I-label或E-label，但是，IOB方案仅能够确定后续单词不能是内部单词另一个标签。<br>使用更具表现力的标记方案（如IOBES）可略微提高模型性能。本文没有观察到显着改进。</p>
<p>另外一个Stack LSTM在次忽略，因为不用这个模型。</p>
<h3 id="输入Word-Embedding"><a href="#输入Word-Embedding" class="headerlink" title="输入Word Embedding"></a>输入Word Embedding</h3><p>输入是单个单词的向量表示。从有限的NER训练数据中学习单词类型的独立表示形式是一个困难的问题：太多的参数无法可靠地估计。<br>由于许多语言都有正字法或形态学证据，即某物是一个名称（或不是名称），因此对单词的<strong>拼写敏感的表示形式</strong>是必要的。使用基于字符的模型，该模型根据单词组成的字符表示来构造单词表示.</p>
<p>第二个<strong>直觉</strong>是，名称可能会变化很大，它们会在大型语料库的常规上下文中出现。因此使用从对语序敏感的大型语料库预训练过的Word Embedding。</p>
<p>使用Dropout来训练，非常重要。</p>
<p><strong>基于字符的单词模型</strong></p>
<p>本文的创新点：学习了字符级别的特征，而不是手工设计有关单词的前缀和后缀信息。学习字符级嵌入具有学习特定于手头任务和领域的表示的优势。已经发现它们对于形态丰富的语言非常有用，并且可以处理词性标注和语言建模或依赖项解析之类的任务中的词汇不足问题。</p>
<p>两个Word Embedding组合的，基于字符双向的词嵌入，以满足双向LSTM的前向与后向。以及词表中预训练的Embedding？</p>
<p><img src="/2020-05-28-Neural-Architectures-for-Named-Entity-Recognition.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-05-28 203336.jpg" alt=""></p>
<p>正向和逆向各25维。</p>
<p>RNN/LSTM他们的代表偏向于他们的最新输入。前向LSTM的最终表示形式是单词后缀的准确表示形式，而向后LSTM的最终状态形式则是其前缀的更好表示形式。</p>
<p><strong>预训练的embedding</strong></p>
<p>使用skip-n-gram来训练的，在训练过程中微调。英语是100维的，其他是64维。</p>
<p><strong>Dropout训练</strong></p>
<p>在embedding层的最后一层进行Dropout，LSTM层之前加入。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>随机梯度下降方法来训练。 lr 0.01，渐变剪裁为5，Adam等并没有表现的比SGD更好。LSTM的维度是100维，只是单层，dropout概率：0.5</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>CoNLL-2002 和 CoNLL-2003 datasets 四种不同类型的命名实体：位置，人名，组织和其他实体。除了将英语NER数据集中的每个数字替换为零之外，没有执行任何数据集预处理。</p>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>使用LSTM-CRF模型能够在荷兰语，德语和西班牙语获得当前最优性能，在英语上获得接近最优的性能，没有任何人工设计的功能或地名词典。尽管基于过渡的算法的性能不如LSTM-CRF模型，但同样也超过了几种语言之前发布的最佳结果。</p>
<p><img src="/2020-05-28-Neural-Architectures-for-Named-Entity-Recognition.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-05-28 213646.jpg" alt=""></p>
<p><img src="/2020-05-28-Neural-Architectures-for-Named-Entity-Recognition.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-05-28 214010.jpg" alt=""></p>
<p>文章还分别对CRF，字符级别模型，预训练等各个成分的提升效果进行了实验：</p>
<p>对词嵌入进行预训练可以使本文在F1中的整体表现得到最大的改善，即+7.31的提升。</p>
<p><img src="/2020-05-28-Neural-Architectures-for-Named-Entity-Recognition.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-05-28 215158.jpg" alt=""></p>
<hr>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ol>
<li>文章结果没有使用中文的实验，可以自己考虑在中文上的结果。</li>
<li>文章提到了字符级别的学习非常有用，使用到中文上的话，效果应该也不错。</li>
</ol>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>NLP</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>面试面经</title>
    <url>/2020-05-16-%E9%9D%A2%E8%AF%95%E9%9D%A2%E7%BB%8F.html</url>
    <content><![CDATA[<ol>
<li><p>核函数的种类有哪些，它们各自的应用场景是什么 </p>
<ol>
<li>线性核、多项式核、高斯核。 特征维数高选择线性核 样本数量可观、特征少选择高斯核（非线性核） 样本数量非常多选择线性核（避免造成庞大的计算量） </li>
</ol>
</li>
<li><p>L1和L2正则化有什么区别？ </p>
<ol>
<li>L1是模型各个参数的绝对值之和,L2为各个参数平方和的开方值。L1更趋向于产生少量的特征,其它特征为0,最优的参数值很大概率出现在坐标轴上,从而导致产生稀疏的权重矩阵,而L2会选择更多的矩阵,但是这些矩阵趋向于0。 </li>
</ol>
</li>
<li><p><a href="https://www.nowcoder.com/discuss/406936?type=post&amp;order=time&amp;pos=&amp;page=1&amp;channel=666&amp;source_id=search_post&amp;subType=2">https://www.nowcoder.com/discuss/406936?type=post&amp;order=time&amp;pos=&amp;page=1&amp;channel=666&amp;source_id=search_post&amp;subType=2</a></p>
</li>
<li><p>2.浏览器输入地址后都发生了什么</p>
</li>
<li><p>TCP解除时的步骤（4次挥手），TCP和UDP的区别</p>
</li>
<li><p>100亿个数怎么求中位数</p>
<p>数组求top k</p>
<p>进程与线程</p>
<p>tcp四次挥手介绍一下 为什么会有第二次、第三次、第四次？</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/93515595">https://zhuanlan.zhihu.com/p/93515595</a></p>
</li>
<li><p><a href="https://blog.csdn.net/Butterfly_resting/article/details/89668661">https://blog.csdn.net/Butterfly_resting/article/details/89668661</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/91539644">https://zhuanlan.zhihu.com/p/91539644</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title>使用LSTM+Pytorch对电影评论进行情感分类</title>
    <url>/2020-05-12-%E4%BD%BF%E7%94%A8LSTM+Pytorch%E5%AF%B9%E7%94%B5%E5%BD%B1%E8%AF%84%E8%AE%BA%E8%BF%9B%E8%A1%8C%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB.html</url>
    <content><![CDATA[<p>项目的<a href="https://github.com/BraveY/AI-with-code/tree/master/Sentiment-classification">github地址</a> 包括notebook和python文件以及训练、验证、测试数据，预训练权重较大，上传到了百度网盘链接：<a href="https://pan.baidu.com/s/1mLcPTgb2m5HPgkT3XcGVCg">https://pan.baidu.com/s/1mLcPTgb2m5HPgkT3XcGVCg</a> 提取码：n41n</p>
<h2 id="包的导入"><a href="#包的导入" class="headerlink" title="包的导入"></a>包的导入</h2><p>与以前的相比，主要增加了简繁转换的包zhconv，变长序列处理的pad_sequence, pack_padded_sequence, pad_packed_sequence等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> re <span class="comment">#split使用</span></span><br><span class="line"><span class="keyword">import</span> gensim <span class="comment"># word2vec预训练加载 </span></span><br><span class="line"><span class="keyword">import</span> jieba <span class="comment">#分词</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter </span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> zhconv <span class="keyword">import</span> convert <span class="comment">#简繁转换</span></span><br><span class="line"><span class="comment"># 变长序列的处理</span></span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence,pack_padded_sequence,pad_packed_sequence</span><br><span class="line"><span class="comment"># from tqdm import tqdm</span></span><br></pre></td></tr></table></figure>
<h2 id="参数配置"><a href="#参数配置" class="headerlink" title="参数配置"></a>参数配置</h2><p>这里为了使用预训练的中文维基词向量，必须将embedding层的维度设置为50维以和预训练权重匹配，其他的参数如dropout 概率，层数等都可以自定以</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DictObj</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="comment"># 私有变量是map</span></span><br><span class="line">    <span class="comment"># 设置变量的时候 初始化设置map</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, mp</span>):</span></span><br><span class="line">        self.<span class="built_in">map</span> = mp</span><br><span class="line">        <span class="comment"># print(mp)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set 可以省略 如果直接初始化设置，而不在程序中修改配置的话</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span>(<span class="params">self, name, value</span>):</span></span><br><span class="line">        <span class="keyword">if</span> name == <span class="string">&#x27;map&#x27;</span>:<span class="comment"># 初始化的设置 走默认的方法</span></span><br><span class="line">            <span class="comment"># print(&quot;init set attr&quot;, name ,&quot;value:&quot;, value)</span></span><br><span class="line">            <span class="built_in">object</span>.__setattr__(self, name, value)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="comment"># print(&#x27;set attr called &#x27;, name, value)</span></span><br><span class="line">        self.<span class="built_in">map</span>[name] = value</span><br><span class="line"><span class="comment"># 之所以自己新建一个类就是为了能够实现直接调用名字的功能。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span>(<span class="params">self, name</span>):</span></span><br><span class="line">        <span class="comment"># print(&#x27;get attr called &#x27;, name)</span></span><br><span class="line">        <span class="keyword">return</span>  self.<span class="built_in">map</span>[name]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Config = DictObj(&#123;</span><br><span class="line">    <span class="string">&#x27;train_path&#x27;</span> : <span class="string">&quot;D:/AIdata/Sentiment-classification/Dataset/train.txt&quot;</span>,</span><br><span class="line">    <span class="string">&#x27;test_path&#x27;</span> : <span class="string">&quot;D:/AIdata/Sentiment-classification/Dataset/test.txt&quot;</span>,</span><br><span class="line">    <span class="string">&#x27;validation_path&#x27;</span> : <span class="string">&quot;D:/AIdata/Sentiment-classification/Dataset/validation.txt&quot;</span>,</span><br><span class="line">    <span class="string">&#x27;pred_word2vec_path&#x27;</span>:<span class="string">&#x27;D:/AIdata/Sentiment-classification/Dataset/wiki_word2vec_50.bin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;tensorboard_path&#x27;</span>:<span class="string">&#x27;./tensorboard&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;model_save_path&#x27;</span>:<span class="string">&#x27;./modelDict/model.pth&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;embedding_dim&#x27;</span>:<span class="number">50</span>,</span><br><span class="line">    <span class="string">&#x27;hidden_dim&#x27;</span>:<span class="number">100</span>,</span><br><span class="line">    <span class="string">&#x27;lr&#x27;</span>:<span class="number">0.001</span>,</span><br><span class="line">    <span class="string">&#x27;LSTM_layers&#x27;</span>:<span class="number">3</span>,</span><br><span class="line">    <span class="string">&#x27;drop_prob&#x27;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;seed&#x27;</span>:<span class="number">0</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="数据集构建"><a href="#数据集构建" class="headerlink" title="数据集构建"></a>数据集构建</h2><h3 id="词汇表建立"><a href="#词汇表建立" class="headerlink" title="词汇表建立"></a>词汇表建立</h3><p>首先建立训练数据的词汇表，实现汉字转索引。构建词汇表的逻辑：首先读取训练集的数据，然后使用zhconv包统一转换成简体，<br>因为数据集本身就已经是分词后的数据了，只需要对应的读入这些词汇然后去重，之后根据去重的list构建两个word2ix 和ix2word即可。</p>
<p>这里思路比较简单，但是有个坑，导致我调了一天的bug。就是每次set操作后对应的顺序是不同的，因为我没有将词汇表保存下来，想的是每次程序运行的时候再来重新构建，因此每次重新set之后得到的词汇表也是不一致的，导致同样的语言文本经过不同的词汇表转换后，每次都得到不同的输入，<br>因此导致训练好的模型每次重新加载kernel之后得到的测试集准确率都不一样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 简繁转换 并构建词汇表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_word_dict</span>(<span class="params">train_path</span>):</span></span><br><span class="line">    words = []</span><br><span class="line">    max_len = <span class="number">0</span></span><br><span class="line">    total_len = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(train_path,<span class="string">&#x27;r&#x27;</span>,encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span>  lines:</span><br><span class="line">            line = convert(line, <span class="string">&#x27;zh-cn&#x27;</span>) <span class="comment">#转换成大陆简体</span></span><br><span class="line">            line_words = re.split(<span class="string">r&#x27;[\s]&#x27;</span>, line)[<span class="number">1</span>:-<span class="number">1</span>] <span class="comment"># 按照空字符\t\n 空格来切分</span></span><br><span class="line">            max_len = <span class="built_in">max</span>(max_len, <span class="built_in">len</span>(line_words))</span><br><span class="line">            total_len += <span class="built_in">len</span>(line_words)</span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> line_words:</span><br><span class="line">                words.append(w)</span><br><span class="line">    words = <span class="built_in">list</span>(<span class="built_in">set</span>(words))<span class="comment">#最终去重</span></span><br><span class="line">    words = <span class="built_in">sorted</span>(words) <span class="comment"># 一定要排序不然每次读取后生成此表都不一致，主要是set后顺序不同</span></span><br><span class="line">    <span class="comment">#用unknown来表示不在训练语料中的词汇</span></span><br><span class="line">    word2ix = &#123;w:i+<span class="number">1</span> <span class="keyword">for</span> i,w <span class="keyword">in</span> <span class="built_in">enumerate</span>(words)&#125; <span class="comment"># 第0是unknown的 所以i+1</span></span><br><span class="line">    ix2word = &#123;i+<span class="number">1</span>:w <span class="keyword">for</span> i,w <span class="keyword">in</span> <span class="built_in">enumerate</span>(words)&#125;</span><br><span class="line">    word2ix[<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] = <span class="number">0</span></span><br><span class="line">    ix2word[<span class="number">0</span>] = <span class="string">&#x27;&lt;unk&gt;&#x27;</span></span><br><span class="line">    avg_len = total_len / <span class="built_in">len</span>(lines)</span><br><span class="line">    <span class="keyword">return</span> word2ix, ix2word, max_len,  avg_len</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word2ix, ix2word, max_len, avg_len = build_word_dict(Config.train_path)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(max_len, avg_len)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">679 44.67896789678968</span><br></pre></td></tr></table></figure>
<h3 id="数据变长处理"><a href="#数据变长处理" class="headerlink" title="数据变长处理"></a>数据变长处理</h3><p>输入样本的中，词汇的长度不一致，最大的长度有679个词，平均而言只有44个词，所以如果只是单纯的填0来进行维度统一的话，大量的0填充会让模型产生误差，<br>参考<a href="https://zhuanlan.zhihu.com/p/34418001">忆臻文章</a>中的图片：<img src="https://pic3.zhimg.com/80/v2-b0aab81f3e671fad36761203c4b5df2a_720w.jpg" alt=""> </p>
<p>为了处理这种情况需要将序列长度不一致的样本，根据长度排序后进行按照批次来分别填充，详细介绍参考<a href="https://zhuanlan.zhihu.com/p/59772104">尹相楠的文章</a> 和<a href="https://zhuanlan.zhihu.com/p/70822702">腾仔的文章</a>, 在这不赘述。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mycollate_fn</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="comment"># 这里的data是getittem返回的（input，label）的二元组，总共有batch_size个</span></span><br><span class="line">    data.sort(key=<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x[<span class="number">0</span>]), reverse=<span class="literal">True</span>)  <span class="comment"># 根据input来排序</span></span><br><span class="line">    data_length = [<span class="built_in">len</span>(sq[<span class="number">0</span>]) <span class="keyword">for</span> sq <span class="keyword">in</span> data]</span><br><span class="line">    input_data = []</span><br><span class="line">    label_data = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">        input_data.append(i[<span class="number">0</span>])</span><br><span class="line">        label_data.append(i[<span class="number">1</span>])</span><br><span class="line">    input_data = pad_sequence(input_data, batch_first=<span class="literal">True</span>, padding_value=<span class="number">0</span>)</span><br><span class="line">    label_data = torch.tensor(label_data)</span><br><span class="line">    <span class="keyword">return</span> input_data, label_data, data_length</span><br></pre></td></tr></table></figure>
<p>数据集的类里面主要是获取数据和标签，稍微需要注意的是考虑到测试集和验证集中一些不会在训练语料库中出现的词汇，需要将这些词汇置为0，来避免索引错误</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CommentDataSet</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data_path, word2ix, ix2word</span>):</span></span><br><span class="line">        self.data_path = data_path</span><br><span class="line">        self.word2ix = word2ix</span><br><span class="line">        self.ix2word = ix2word</span><br><span class="line">        self.data, self.label = self.get_data_label()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx: <span class="built_in">int</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.data[idx], self.label[idx]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_data_label</span>(<span class="params">self</span>):</span></span><br><span class="line">        data = []</span><br><span class="line">        label = []</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(self.data_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            lines = f.readlines()</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    label.append(torch.tensor(<span class="built_in">int</span>(line[<span class="number">0</span>]), dtype=torch.int64))</span><br><span class="line">                <span class="keyword">except</span> BaseException:  <span class="comment"># 遇到首个字符不是标签的就跳过比如空行，并打印</span></span><br><span class="line">                    print(<span class="string">&#x27;not expected line:&#x27;</span> + line)</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                line = convert(line, <span class="string">&#x27;zh-cn&#x27;</span>)  <span class="comment"># 转换成大陆简体</span></span><br><span class="line">                line_words = re.split(<span class="string">r&#x27;[\s]&#x27;</span>, line)[<span class="number">1</span>:-<span class="number">1</span>]  <span class="comment"># 按照空字符\t\n 空格来切分</span></span><br><span class="line">                words_to_idx = []</span><br><span class="line">                <span class="keyword">for</span> w <span class="keyword">in</span> line_words:</span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        index = self.word2ix[w]</span><br><span class="line">                    <span class="keyword">except</span> BaseException:</span><br><span class="line">                        index = <span class="number">0</span>  <span class="comment"># 测试集，验证集中可能出现没有收录的词语，置为0</span></span><br><span class="line">                    <span class="comment">#                 words_to_idx = [self.word2ix[w] for w in line_words]</span></span><br><span class="line">                    words_to_idx.append(index)</span><br><span class="line">                data.append(torch.tensor(words_to_idx, dtype=torch.int64))</span><br><span class="line">        <span class="keyword">return</span> data, label</span><br></pre></td></tr></table></figure>
<h2 id="训练集，验证集，测试集，加载"><a href="#训练集，验证集，测试集，加载" class="headerlink" title="训练集，验证集，测试集，加载"></a>训练集，验证集，测试集，加载</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = CommentDataSet(Config.train_path, word2ix, ix2word)</span><br><span class="line">train_loader = DataLoader(train_data, batch_size=<span class="number">16</span>, shuffle=<span class="literal">True</span>,</span><br><span class="line">                         num_workers=<span class="number">0</span>, collate_fn=mycollate_fn,)</span><br><span class="line"></span><br><span class="line">validation_data = CommentDataSet(Config.validation_path, word2ix, ix2word)</span><br><span class="line">validation_loader = DataLoader(validation_data, batch_size=<span class="number">16</span>, shuffle=<span class="literal">True</span>,</span><br><span class="line">                         num_workers=<span class="number">0</span>, collate_fn=mycollate_fn,)</span><br><span class="line"></span><br><span class="line">test_data = CommentDataSet(Config.test_path, word2ix, ix2word)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=<span class="number">16</span>, shuffle=<span class="literal">False</span>,</span><br><span class="line">                         num_workers=<span class="number">0</span>, collate_fn=mycollate_fn,)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">not expected line:</span><br><span class="line"></span><br><span class="line">not expected line:</span><br></pre></td></tr></table></figure>
<p>​    </p>
<h2 id="预训练权重加载"><a href="#预训练权重加载" class="headerlink" title="预训练权重加载"></a>预训练权重加载</h2><p>这里需要将预训练的中文word2vec的权重初始到pytorch embedding层，主要的逻辑思路首先使用gensim包来加载权重，然后根据前面建立的词汇表，初始一个vocab_size*embedding_dim的0矩阵weight，之后对每个词汇查询是否在预训练的word2vec中有权重，如果有的话就将这个权重复制到weight中，最后使用weight来初始embedding层就可以了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># word2vec加载</span></span><br><span class="line">word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(Config.pred_word2vec_path, binary=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#50维的向量</span></span><br><span class="line">word2vec_model.__dict__[<span class="string">&#x27;vectors&#x27;</span>].shape</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(426677, 50)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pre_weight</span>(<span class="params">vocab_size</span>):</span></span><br><span class="line">    weight = torch.zeros(vocab_size,Config.embedding_dim)</span><br><span class="line">    <span class="comment">#初始权重</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word2vec_model.index2word)):<span class="comment">#预训练中没有word2ix，所以只能用索引来遍历</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            index = word2ix[word2vec_model.index2word[i]]<span class="comment">#得到预训练中的词汇的新索引</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        weight[index, :] = torch.from_numpy(word2vec_model.get_vector(</span><br><span class="line">            ix2word[word2ix[word2vec_model.index2word[i]]]))<span class="comment">#得到对应的词向量</span></span><br><span class="line">    <span class="keyword">return</span> weight</span><br></pre></td></tr></table></figure>
<h2 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h2><p>模型的构建与前面的<a href="https://zhuanlan.zhihu.com/p/138270447">LSTM自动写诗</a>大体一致,即embedding后LSTM层然后3层全连接，激活函数选择了tanh。不同的点在于，这里的输出只保留时间步的最后一步，用来当作预测结果。也就是最后一个全连接层的输出取最后一个时间步的输出。以及为了防止过拟合而采用了Dropout。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentimentModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embedding_dim, hidden_dim,pre_weight</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SentimentModel, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embeddings = nn.Embedding.from_pretrained(pre_weight)</span><br><span class="line">        <span class="comment"># requires_grad指定是否在训练过程中对词向量的权重进行微调</span></span><br><span class="line">        self.embeddings.weight.requires_grad = <span class="literal">True</span></span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=Config.LSTM_layers,</span><br><span class="line">                            batch_first=<span class="literal">True</span>, dropout=Config.drop_prob, bidirectional=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(Config.drop_prob)</span><br><span class="line">        self.fc1 = nn.Linear(self.hidden_dim,<span class="number">256</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">256</span>,<span class="number">32</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#         self.linear = nn.Linear(self.hidden_dim, vocab_size)# 输出的大小是词表的维度，</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, batch_seq_len, hidden=<span class="literal">None</span></span>):</span></span><br><span class="line">        embeds = self.embeddings(<span class="built_in">input</span>)  <span class="comment"># [batch, seq_len] =&gt; [batch, seq_len, embed_dim]</span></span><br><span class="line">        embeds = pack_padded_sequence(embeds,batch_seq_len, batch_first=<span class="literal">True</span>)</span><br><span class="line">        batch_size, seq_len = <span class="built_in">input</span>.size()</span><br><span class="line">        <span class="keyword">if</span> hidden <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            h_0 = <span class="built_in">input</span>.data.new(Config.LSTM_layers*<span class="number">1</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).<span class="built_in">float</span>()</span><br><span class="line">            c_0 = <span class="built_in">input</span>.data.new(Config.LSTM_layers*<span class="number">1</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).<span class="built_in">float</span>()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            h_0, c_0 = hidden</span><br><span class="line">        output, hidden = self.lstm(embeds, (h_0, c_0))<span class="comment">#hidden 是h,和c 这两个隐状态</span></span><br><span class="line">        output,_ = pad_packed_sequence(output,batch_first=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        output = self.dropout(torch.tanh(self.fc1(output)))</span><br><span class="line">        output = torch.tanh(self.fc2(output))</span><br><span class="line">        output = self.fc3(output)</span><br><span class="line">        last_outputs = self.get_last_output(output, batch_seq_len)</span><br><span class="line"><span class="comment">#         output = output.reshape(batch_size * seq_len, -1)</span></span><br><span class="line">        <span class="keyword">return</span> last_outputs,hidden</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_last_output</span>(<span class="params">self,output,batch_seq_len</span>):</span></span><br><span class="line">        last_outputs = torch.zeros((output.shape[<span class="number">0</span>],output.shape[<span class="number">2</span>]))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(batch_seq_len)):</span><br><span class="line">            last_outputs[i] =  output[i][batch_seq_len[i]-<span class="number">1</span>]<span class="comment">#index 是长度 -1</span></span><br><span class="line">        last_outputs = last_outputs.to(output.device)</span><br><span class="line">        <span class="keyword">return</span> last_outputs</span><br></pre></td></tr></table></figure>
<h2 id="准确率指标"><a href="#准确率指标" class="headerlink" title="准确率指标"></a>准确率指标</h2><p>分别有两个指标一个是topk的AverageMeter，另一个是使用混淆矩阵。混淆矩阵的实现的时候先转为(pred, label)的二元对，然后相应的填充到表中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AvgrageMeter</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.reset()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.avg = <span class="number">0</span></span><br><span class="line">        self.<span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">        self.cnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, val, n=<span class="number">1</span></span>):</span></span><br><span class="line">        self.<span class="built_in">sum</span> += val * n</span><br><span class="line">        self.cnt += n</span><br><span class="line">        self.avg = self.<span class="built_in">sum</span> / self.cnt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#混淆矩阵指标</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConfuseMeter</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.reset()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 标签的分类：0 pos 1 neg </span></span><br><span class="line">        self.confuse_mat = torch.zeros(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">        self.tp = self.confuse_mat[<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">        self.fp = self.confuse_mat[<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">        self.tn = self.confuse_mat[<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">        self.fn = self.confuse_mat[<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">        self.acc = <span class="number">0</span></span><br><span class="line">        self.pre = <span class="number">0</span></span><br><span class="line">        self.rec = <span class="number">0</span></span><br><span class="line">        self.F1 = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, output, label</span>):</span></span><br><span class="line">        pred = output.argmax(dim = <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> l, p <span class="keyword">in</span> <span class="built_in">zip</span>(label.view(-<span class="number">1</span>),pred.view(-<span class="number">1</span>)):</span><br><span class="line">            self.confuse_mat[p.long(), l.long()] += <span class="number">1</span> <span class="comment"># 对应的格子加1</span></span><br><span class="line">        self.tp = self.confuse_mat[<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line">        self.fp = self.confuse_mat[<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">        self.tn = self.confuse_mat[<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">        self.fn = self.confuse_mat[<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">        self.acc = (self.tp+self.tn) / self.confuse_mat.<span class="built_in">sum</span>()</span><br><span class="line">        self.pre = self.tp / (self.tp + self.fp)</span><br><span class="line">        self.rec = self.tp / (self.tp + self.fn)</span><br><span class="line">        self.F1 = <span class="number">2</span> * self.pre*self.rec / (self.pre + self.rec)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## topk的准确率计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">output, label, topk=(<span class="params"><span class="number">1</span>,</span>)</span>):</span></span><br><span class="line">    maxk = <span class="built_in">max</span>(topk) </span><br><span class="line">    batch_size = label.size(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取前K的索引</span></span><br><span class="line">    _, pred = output.topk(maxk, <span class="number">1</span>, <span class="literal">True</span>, <span class="literal">True</span>) <span class="comment">#使用topk来获得前k个的索引</span></span><br><span class="line">    pred = pred.t() <span class="comment"># 进行转置</span></span><br><span class="line">    <span class="comment"># eq按照对应元素进行比较 view(1,-1) 自动转换到行为1,的形状， expand_as(pred) 扩展到pred的shape</span></span><br><span class="line">    <span class="comment"># expand_as 执行按行复制来扩展，要保证列相等</span></span><br><span class="line">    correct = pred.eq(label.view(<span class="number">1</span>, -<span class="number">1</span>).expand_as(pred)) <span class="comment"># 与正确标签序列形成的矩阵相比，生成True/False矩阵</span></span><br><span class="line"><span class="comment">#     print(correct)</span></span><br><span class="line"></span><br><span class="line">    rtn = []</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> topk:</span><br><span class="line">        correct_k = correct[:k].view(-<span class="number">1</span>).<span class="built_in">float</span>().<span class="built_in">sum</span>(<span class="number">0</span>) <span class="comment"># 前k行的数据 然后平整到1维度，来计算true的总个数</span></span><br><span class="line">        rtn.append(correct_k.mul_(<span class="number">100.0</span> / batch_size)) <span class="comment"># mul_() ternsor 的乘法  正确的数目/总的数目 乘以100 变成百分比</span></span><br><span class="line">    <span class="keyword">return</span> rtn</span><br></pre></td></tr></table></figure>
<h2 id="训练函数"><a href="#训练函数" class="headerlink" title="训练函数"></a>训练函数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#一个epoch的训练逻辑</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch,epochs, train_loader, device, model, criterion, optimizer,scheduler,tensorboard_path</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    top1 = AvgrageMeter()</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):  <span class="comment"># 0是下标起始位置默认为0</span></span><br><span class="line">        inputs, labels, batch_seq_len = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device), data[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># 初始为0，清除上个batch的梯度信息</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs,hidden = model(inputs,batch_seq_len)</span><br><span class="line"></span><br><span class="line">        loss = criterion(outputs,labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        _,pred = outputs.topk(<span class="number">1</span>)</span><br><span class="line">        prec1, prec2= accuracy(outputs, labels, topk=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">        n = inputs.size(<span class="number">0</span>)</span><br><span class="line">        top1.update(prec1.item(), n)</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        postfix = &#123;<span class="string">&#x27;train_loss&#x27;</span>: <span class="string">&#x27;%.6f&#x27;</span> % (train_loss / (i + <span class="number">1</span>)), <span class="string">&#x27;train_acc&#x27;</span>: <span class="string">&#x27;%.6f&#x27;</span> % top1.avg&#125;</span><br><span class="line">        train_loader.set_postfix(log=postfix)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ternsorboard 曲线绘制</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(tensorboard_path) == <span class="literal">False</span>: </span><br><span class="line">            os.mkdir(tensorboard_path)    </span><br><span class="line">        writer = SummaryWriter(tensorboard_path)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;Train/Loss&#x27;</span>, loss.item(), epoch)</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;Train/Accuracy&#x27;</span>, top1.avg, epoch)</span><br><span class="line">        writer.flush()</span><br><span class="line">    scheduler.step()</span><br><span class="line"></span><br><span class="line"><span class="comment">#     print(&#x27;Finished Training&#x27;)</span></span><br></pre></td></tr></table></figure>
<h2 id="验证函数"><a href="#验证函数" class="headerlink" title="验证函数"></a>验证函数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validate</span>(<span class="params">epoch,validate_loader, device, model, criterion, tensorboard_path</span>):</span></span><br><span class="line">    val_acc = <span class="number">0.0</span></span><br><span class="line">    model = model.to(device)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 进行评测的时候网络不更新梯度</span></span><br><span class="line">        val_top1 = AvgrageMeter()</span><br><span class="line">        validate_loader = tqdm(validate_loader)</span><br><span class="line">        validate_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(validate_loader, <span class="number">0</span>):  <span class="comment"># 0是下标起始位置默认为0</span></span><br><span class="line">            inputs, labels, batch_seq_len = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device), data[<span class="number">2</span>]</span><br><span class="line">            <span class="comment">#         inputs,labels = data[0],data[1]</span></span><br><span class="line">            outputs,_ = model(inputs, batch_seq_len)</span><br><span class="line">            loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">            prec1, prec2 = accuracy(outputs, labels, topk=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">            n = inputs.size(<span class="number">0</span>)</span><br><span class="line">            val_top1.update(prec1.item(), n)</span><br><span class="line">            validate_loss += loss.item()</span><br><span class="line">            postfix = &#123;<span class="string">&#x27;validate_loss&#x27;</span>: <span class="string">&#x27;%.6f&#x27;</span> % (validate_loss / (i + <span class="number">1</span>)), <span class="string">&#x27;validate_acc&#x27;</span>: <span class="string">&#x27;%.6f&#x27;</span> % val_top1.avg&#125;</span><br><span class="line">            validate_loader.set_postfix(log=postfix)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># ternsorboard 曲线绘制</span></span><br><span class="line">            <span class="keyword">if</span> os.path.exists(tensorboard_path) == <span class="literal">False</span>: </span><br><span class="line">                os.mkdir(tensorboard_path)    </span><br><span class="line">            writer = SummaryWriter(tensorboard_path)</span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;Validate/Loss&#x27;</span>, loss.item(), epoch)</span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;Validate/Accuracy&#x27;</span>, val_top1.avg, epoch)</span><br><span class="line">            writer.flush()</span><br><span class="line">        val_acc = val_top1.avg</span><br><span class="line">    <span class="keyword">return</span> val_acc</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">validate_loader, device, model, criterion</span>):</span></span><br><span class="line">    val_acc = <span class="number">0.0</span></span><br><span class="line">    model = model.to(device)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    confuse_meter = ConfuseMeter()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 进行评测的时候网络不更新梯度</span></span><br><span class="line">        val_top1 = AvgrageMeter()</span><br><span class="line">        validate_loader = tqdm(validate_loader)</span><br><span class="line">        validate_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(validate_loader, <span class="number">0</span>):  <span class="comment"># 0是下标起始位置默认为0</span></span><br><span class="line">            inputs, labels, batch_seq_len = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device), data[<span class="number">2</span>]</span><br><span class="line">            <span class="comment">#         inputs,labels = data[0],data[1]</span></span><br><span class="line">            outputs,_ = model(inputs, batch_seq_len)</span><br><span class="line"><span class="comment">#             loss = criterion(outputs, labels)</span></span><br><span class="line"></span><br><span class="line">            prec1, prec2 = accuracy(outputs, labels, topk=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">            n = inputs.size(<span class="number">0</span>)</span><br><span class="line">            val_top1.update(prec1.item(), n)</span><br><span class="line">            confuse_meter.update(outputs, labels)</span><br><span class="line"><span class="comment">#             validate_loss += loss.item()</span></span><br><span class="line">            postfix = &#123; <span class="string">&#x27;test_acc&#x27;</span>: <span class="string">&#x27;%.6f&#x27;</span> % val_top1.avg,</span><br><span class="line">                      <span class="string">&#x27;confuse_acc&#x27;</span>: <span class="string">&#x27;%.6f&#x27;</span> % confuse_meter.acc&#125;</span><br><span class="line">            validate_loader.set_postfix(log=postfix)</span><br><span class="line">        val_acc = val_top1.avg</span><br><span class="line">    <span class="keyword">return</span> confuse_meter</span><br></pre></td></tr></table></figure>
<h2 id="随机数种子设置"><a href="#随机数种子设置" class="headerlink" title="随机数种子设置"></a>随机数种子设置</h2><p>随机种子的设置需要在模型初始之前，这样才能保证模型每次初始化的时候得到的是一样的权重，从而保证能够复现每次训练结果<br><code>torch.backends.cudnn.benchmark = True</code> 参考 <a href="https://zhuanlan.zhihu.com/p/73711222">https://zhuanlan.zhihu.com/p/73711222</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_seed</span>(<span class="params">seed</span>):</span></span><br><span class="line">    <span class="comment"># seed</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed_all(seed)  <span class="comment">#并行gpu</span></span><br><span class="line">        torch.backends.cudnn.deterministic = <span class="literal">True</span>  <span class="comment">#cpu/gpu结果一致</span></span><br><span class="line"><span class="comment">#         torch.backends.cudnn.benchmark = True   #训练集变化不大时使训练加速   </span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">set_seed(Config.seed)</span><br></pre></td></tr></table></figure>
<h2 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = SentimentModel(embedding_dim=Config.embedding_dim,</span><br><span class="line">                      hidden_dim=Config.hidden_dim,</span><br><span class="line">                      pre_weight=pre_weight(<span class="built_in">len</span>(word2ix)))</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">epochs = <span class="number">3</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=Config.lr)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = <span class="number">10</span>,gamma=<span class="number">0.1</span>)<span class="comment">#学习率调整</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SentimentModel(</span><br><span class="line">  (embeddings): Embedding(51406, 50)</span><br><span class="line">  (lstm): LSTM(50, 100, num_layers&#x3D;3, batch_first&#x3D;True, dropout&#x3D;0.5)</span><br><span class="line">  (dropout): Dropout(p&#x3D;0.5, inplace&#x3D;False)</span><br><span class="line">  (fc1): Linear(in_features&#x3D;100, out_features&#x3D;256, bias&#x3D;True)</span><br><span class="line">  (fc2): Linear(in_features&#x3D;256, out_features&#x3D;32, bias&#x3D;True)</span><br><span class="line">  (fc3): Linear(in_features&#x3D;32, out_features&#x3D;2, bias&#x3D;True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="迭代训练"><a href="#迭代训练" class="headerlink" title="迭代训练"></a>迭代训练</h2><p>在每个epoch中同时收集验证集准确率，防止过拟合</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#因为使用tensorboard画图会产生很多日志文件，这里进行清空操作</span></span><br><span class="line"><span class="keyword">import</span> shutil  </span><br><span class="line"><span class="keyword">if</span> os.path.exists(Config.tensorboard_path):</span><br><span class="line">    shutil.rmtree(Config.tensorboard_path)  </span><br><span class="line">    os.mkdir(Config.tensorboard_path)</span><br></pre></td></tr></table></figure>
<p>训练时win10+Pytorch1.3会出现随机的bug，RuntimeError: cuda runtime error (719) : unspecified launch failure at C:/w/1/s/tmp_conda_3.6_081743/conda/conda-bld/pytorch_1572941935551/work/aten/src\THC/generic/THCTensorMath.cu:26 <a href="https://github.com/pytorch/pytorch/issues/27837">官方还没有解决</a> 需要重启kernel/或者系统              </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    train_loader = tqdm(train_loader)</span><br><span class="line">    train_loader.set_description(<span class="string">&#x27;[%s%04d/%04d %s%f]&#x27;</span> % (<span class="string">&#x27;Epoch:&#x27;</span>, epoch + <span class="number">1</span>, epochs, <span class="string">&#x27;lr:&#x27;</span>, scheduler.get_lr()[<span class="number">0</span>]))</span><br><span class="line">    train(epoch, epochs, train_loader, device, model, criterion, optimizer,scheduler, Config.tensorboard_path)</span><br><span class="line">    validate(epoch, validation_loader,device,model,criterion,Config.tensorboard_path)</span><br></pre></td></tr></table></figure>
<p>训练3个epoch后训练集准确率： ‘train_acc’: ‘92.499250’ ,验证集准确率：’validate_acc’: ‘82.376976’ </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#模型保存</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(Config.model_save_path) == <span class="literal">False</span>: </span><br><span class="line">    os.mkdir(<span class="string">&#x27;./modelDict/&#x27;</span>)   </span><br><span class="line">torch.save(model.state_dict(), Config.model_save_path)</span><br></pre></td></tr></table></figure>
<h2 id="测试集相关指标"><a href="#测试集相关指标" class="headerlink" title="测试集相关指标"></a>测试集相关指标</h2><p>包括精确率，召回率，F1Score以及混淆矩阵，测试集准确率达到85%,精确率88%，召回率80.7%，F1分数：0.84</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_test = SentimentModel(embedding_dim=Config.embedding_dim,</span><br><span class="line">                      hidden_dim=Config.hidden_dim,</span><br><span class="line">                      pre_weight=pre_weight(<span class="built_in">len</span>(word2ix)))</span><br><span class="line">optimizer_test = optim.Adam(model_test.parameters(), lr=Config.lr)</span><br><span class="line">scheduler_test = torch.optim.lr_scheduler.StepLR(optimizer, step_size = <span class="number">10</span>,gamma=<span class="number">0.1</span>)<span class="comment">#学习率调整</span></span><br><span class="line">criterion_test = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_test.load_state_dict(torch.load(Config.model_save_path),strict=<span class="literal">True</span>)  <span class="comment"># 模型加载</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;All keys matched successfully&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">confuse_meter = ConfuseMeter()</span><br><span class="line">confuse_meter = test(test_loader,device,model_test,criterion_test)    </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">&#x27;prec:%.6f  recall:%.6f  F1:%.6f&#x27;</span>%(confuse_meter.pre,confuse_meter.rec, confuse_meter.F1))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">prec:0.880240  recall:0.807692  F1:0.842407</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#混淆矩阵</span></span><br><span class="line">confuse_meter.confuse_mat</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[147.,  20.],</span><br><span class="line">        [ 35., 167.]])</span><br></pre></td></tr></table></figure>
<h2 id="模型使用"><a href="#模型使用" class="headerlink" title="模型使用"></a>模型使用</h2><p>使用模型来对自己收集的豆瓣上面对《龙岭迷窟》的评论进行分类预测。第一条是好评，第二条是差评，使用自己的模型能够正确对两条评论进行分类。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">comment_str, model, device</span>):</span></span><br><span class="line">    model = model.to(device)</span><br><span class="line">    seg_list = jieba.lcut(comment_str,cut_all=<span class="literal">False</span>)</span><br><span class="line">    words_to_idx = []</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> seg_list:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            index = word2ix[w] </span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            index = <span class="number">0</span> <span class="comment">#可能出现没有收录的词语，置为0 </span></span><br><span class="line">        words_to_idx.append(index)</span><br><span class="line">    inputs = torch.tensor(words_to_idx).to(device)</span><br><span class="line">    inputs = inputs.reshape(<span class="number">1</span>,<span class="built_in">len</span>(inputs))</span><br><span class="line">    outputs,_ = model(inputs, [<span class="built_in">len</span>(inputs),])</span><br><span class="line">    pred = outputs.argmax(<span class="number">1</span>).item()</span><br><span class="line">    <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">comment_str1 = <span class="string">&quot;这一部导演、监制、男一都和《怒晴湘西》都是原班人马，这次是黄土高原上《龙岭密窟》的探险故事，有蝙蝠群、巨型蜘蛛这些让人瑟瑟发抖的元素，紧张刺激的剧情挺期待的。潘老师演技一如既往地稳。本来对姜超的印象也还在李大嘴这个喜剧角色里，居然没让人失望，还挺贴合王胖子这个角色。&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (predict(comment_str1,model,device)):</span><br><span class="line">    print(<span class="string">&quot;Negative&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&quot;Positive&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Positive</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">comment_str2 = <span class="string">&quot;年代感太差，剧情非常的拖沓，还是冗余情节的拖沓。特效五毛，实在是太烂。潘粤明对这剧也太不上心了，胖得都能演王胖子了，好歹也锻炼一下。烂剧！&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (predict(comment_str2,model,device)):</span><br><span class="line">    print(<span class="string">&quot;Negative&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&quot;Positive&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Negative</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/nlpuser/article/details/83627709">https://blog.csdn.net/nlpuser/article/details/83627709</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/70822702">https://zhuanlan.zhihu.com/p/70822702</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/59772104">https://zhuanlan.zhihu.com/p/59772104</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title>范闲写诗器之用LSTM+Pytorch实现自动写诗</title>
    <url>/2020-05-05-%E8%8C%83%E9%97%B2%E5%86%99%E8%AF%97%E5%99%A8%E4%B9%8B%E7%94%A8LSTM+Pytorch%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E5%86%99%E8%AF%97.html</url>
    <content><![CDATA[<p>LSTM网络经常用于序列预测，因此在NLP领域很常用，本文将利用LSTM网络来搭建一个简单的自动写诗的demo，做这个的时候突然想起庆余年中范闲作诗的片段，所以就把它取名为范闲写诗器，用来供范闲参考哈哈哈。文章的源码放在<a href="https://github.com/BraveY/AI-with-code/tree/master/Automatic-poem-writing">github</a>上，求个赞！！！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter </span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># from tqdm import tqdm</span></span><br></pre></td></tr></table></figure>
<h2 id="相关配置"><a href="#相关配置" class="headerlink" title="相关配置"></a>相关配置</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DictObj</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="comment"># 私有变量是map</span></span><br><span class="line">    <span class="comment"># 设置变量的时候 初始化设置map</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, mp</span>):</span></span><br><span class="line">        self.<span class="built_in">map</span> = mp</span><br><span class="line">        <span class="comment"># print(mp)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set 可以省略 如果直接初始化设置</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span>(<span class="params">self, name, value</span>):</span></span><br><span class="line">        <span class="keyword">if</span> name == <span class="string">&#x27;map&#x27;</span>:<span class="comment"># 初始化的设置 走默认的方法</span></span><br><span class="line">            <span class="comment"># print(&quot;init set attr&quot;, name ,&quot;value:&quot;, value)</span></span><br><span class="line">            <span class="built_in">object</span>.__setattr__(self, name, value)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="comment"># print(&#x27;set attr called &#x27;, name, value)</span></span><br><span class="line">        self.<span class="built_in">map</span>[name] = value</span><br><span class="line"><span class="comment"># 之所以自己新建一个类就是为了能够实现直接调用名字的功能。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span>(<span class="params">self, name</span>):</span></span><br><span class="line">        <span class="comment"># print(&#x27;get attr called &#x27;, name)</span></span><br><span class="line">        <span class="keyword">return</span>  self.<span class="built_in">map</span>[name]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Config = DictObj(&#123;</span><br><span class="line">    <span class="string">&#x27;poem_path&#x27;</span> : <span class="string">&quot;./tang.npz&quot;</span>,</span><br><span class="line">    <span class="string">&#x27;tensorboard_path&#x27;</span>:<span class="string">&#x27;./tensorboard&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;model_save_path&#x27;</span>:<span class="string">&#x27;./modelDict/poem.pth&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;embedding_dim&#x27;</span>:<span class="number">100</span>,</span><br><span class="line">    <span class="string">&#x27;hidden_dim&#x27;</span>:<span class="number">1024</span>,</span><br><span class="line">    <span class="string">&#x27;lr&#x27;</span>:<span class="number">0.001</span>,</span><br><span class="line">    <span class="string">&#x27;LSTM_layers&#x27;</span>:<span class="number">3</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="唐诗数据查看"><a href="#唐诗数据查看" class="headerlink" title="唐诗数据查看"></a>唐诗数据查看</h2><p>唐诗数据文件分为三部分，data部分是唐诗数据的总共包含57580首唐诗数据，其中每一首都被格式化成125个字符，唐诗开始用’<START\>‘标志，结束用’<EOP\>‘标志,空余的用’<space\>‘标志， ix2word和word2ix是汉字的字典索引。因此可以不用自己去构建这个字典了。</space\></EOP\></START\></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">view_data</span>(<span class="params">poem_path</span>):</span></span><br><span class="line">    datas = np.load(poem_path)</span><br><span class="line">    data = datas[<span class="string">&#x27;data&#x27;</span>]</span><br><span class="line">    ix2word = datas[<span class="string">&#x27;ix2word&#x27;</span>].item()</span><br><span class="line">    word2ix = datas[<span class="string">&#x27;word2ix&#x27;</span>].item()</span><br><span class="line">    word_data = np.zeros((<span class="number">1</span>,data.shape[<span class="number">1</span>]),dtype=np.<span class="built_in">str</span>) <span class="comment"># 这样初始化后值会保留第一一个字符，所以输出中&#x27;&lt;START&gt;&#x27; 变成了&#x27;&lt;&#x27;</span></span><br><span class="line">    row = np.random.randint(data.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(data.shape[<span class="number">1</span>]):</span><br><span class="line">        word_data[<span class="number">0</span>,col] = ix2word[data[row,col]]</span><br><span class="line">    print(data.shape) <span class="comment">#(57580, 125)</span></span><br><span class="line">    print(word_data)<span class="comment">#随机查看</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">view_data(Config.poem_path)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(57580, 125)</span><br><span class="line">[[&#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39;</span><br><span class="line">  &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39;</span><br><span class="line">  &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39;</span><br><span class="line">  &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39;</span><br><span class="line">  &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;&lt;&#39; &#39;庭&#39; &#39;树&#39; &#39;晓&#39; &#39;禽&#39; &#39;动&#39; &#39;，&#39; &#39;郡&#39; &#39;楼&#39; &#39;残&#39; &#39;点&#39; &#39;声&#39; &#39;。&#39; &#39;灯&#39; &#39;挑&#39;</span><br><span class="line">  &#39;红&#39; &#39;烬&#39; &#39;落&#39; &#39;，&#39; &#39;酒&#39; &#39;煖&#39; &#39;白&#39; &#39;光&#39; &#39;生&#39; &#39;。&#39; &#39;髪&#39; &#39;少&#39; &#39;嫌&#39; &#39;梳&#39; &#39;利&#39; &#39;，&#39; &#39;颜&#39; &#39;衰&#39;</span><br><span class="line">  &#39;恨&#39; &#39;镜&#39; &#39;明&#39; &#39;。&#39; &#39;独&#39; &#39;吟&#39; &#39;谁&#39; &#39;应&#39; &#39;和&#39; &#39;，&#39; &#39;须&#39; &#39;寄&#39; &#39;洛&#39; &#39;阳&#39; &#39;城&#39; &#39;。&#39; &#39;&lt;&#39;]]</span><br></pre></td></tr></table></figure>
<p>可以看到125个字符中，大部分都是空格数据，我统计了下总的数据中将近57%的数据都是空格。如果不去除空格数据的话，模型的虽然最开始训练的时候就有60多的准确率，但是这些准确率是因为预测空格来造成的，所以需要将空格数据给去掉。</p>
<h2 id="构造数据集"><a href="#构造数据集" class="headerlink" title="构造数据集"></a>构造数据集</h2><p>这一步的主要工作是将原始的唐诗数据集中的空格给过滤掉，然后根据序列的长度seq_len重新划分无空格的数据集,并得到每个序列的标签。我最开始的时候比较困惑数据的标签应该是什么？纠结的点在于最开始以为上一句诗的标签是不是，下一句诗。比如“床前明月光，”的标签是不是“疑是地上霜”。后面阅读了些资料才搞懂正确的标签应该是这个汉字的下一个汉字，“床前明月光，”对应的标签是“前明月光，疑”。也就是基于字符级的语言模型。</p>
<p><a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch/tree/master/docs/chapter06_RNN">Dive-into-DL-Pytorch</a> 中的例子：</p>
<p><img src="https://pic3.zhimg.com/80/v2-d49f883912455f5c689190325d9e8b6e_720w.jpg" alt=""><br>搞懂了标签是什么后，这一步的代码逻辑就好理解了，先从路径文件中得到原始数据poem_data，然后将poem_data中的空格数据给过滤掉并平整到一维得到no_space_data，之后就根据索引去得到对应迭代次数的数据和标签了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PoemDataSet</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,poem_path,seq_len</span>):</span></span><br><span class="line">        self.seq_len = seq_len</span><br><span class="line">        self.poem_path = poem_path</span><br><span class="line">        self.poem_data, self.ix2word, self.word2ix = self.get_raw_data()</span><br><span class="line">        self.no_space_data = self.filter_space()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx:<span class="built_in">int</span></span>):</span></span><br><span class="line">        txt = self.no_space_data[idx*self.seq_len : (idx+<span class="number">1</span>)*self.seq_len]</span><br><span class="line">        label = self.no_space_data[idx*self.seq_len + <span class="number">1</span> : (idx+<span class="number">1</span>)*self.seq_len + <span class="number">1</span>] <span class="comment"># 将窗口向后移动一个字符就是标签</span></span><br><span class="line">        txt = torch.from_numpy(np.array(txt)).long()</span><br><span class="line">        label = torch.from_numpy(np.array(label)).long()</span><br><span class="line">        <span class="keyword">return</span> txt,label</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">int</span>(<span class="built_in">len</span>(self.no_space_data) / self.seq_len)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">filter_space</span>(<span class="params">self</span>):</span> <span class="comment"># 将空格的数据给过滤掉，并将原始数据平整到一维</span></span><br><span class="line">        t_data = torch.from_numpy(self.poem_data).view(-<span class="number">1</span>)</span><br><span class="line">        flat_data = t_data.numpy()</span><br><span class="line">        no_space_data = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> flat_data:</span><br><span class="line">            <span class="keyword">if</span> (i != <span class="number">8292</span> ):</span><br><span class="line">                no_space_data.append(i)</span><br><span class="line">        <span class="keyword">return</span> no_space_data</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_raw_data</span>(<span class="params">self</span>):</span></span><br><span class="line"><span class="comment">#         datas = np.load(self.poem_path,allow_pickle=True)  #numpy 1.16.2  以上引入了allow_pickle</span></span><br><span class="line">        datas = np.load(self.poem_path)</span><br><span class="line">        data = datas[<span class="string">&#x27;data&#x27;</span>]</span><br><span class="line">        ix2word = datas[<span class="string">&#x27;ix2word&#x27;</span>].item()</span><br><span class="line">        word2ix = datas[<span class="string">&#x27;word2ix&#x27;</span>].item()</span><br><span class="line">        <span class="keyword">return</span> data, ix2word, word2ix</span><br></pre></td></tr></table></figure>
<p>seq_len 这里我选择的是48，因为考虑到唐诗主要是五言绝句和七言绝句，各自加上一个标点符号也就是6和8，选择一个公约数48，这样刚好凑够8句无言或者6句七言，比较符合唐诗的偶数句对。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poem_ds = PoemDataSet(Config.poem_path, <span class="number">48</span>)</span><br><span class="line">ix2word = poem_ds.ix2word</span><br><span class="line">word2ix = poem_ds.word2ix</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poem_ds[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(tensor([8291, 6731, 4770, 1787, 8118, 7577, 7066, 4817,  648, 7121, 1542, 6483,</span><br><span class="line">         7435, 7686, 2889, 1671, 5862, 1949, 7066, 2596, 4785, 3629, 1379, 2703,</span><br><span class="line">         7435, 6064, 6041, 4666, 4038, 4881, 7066, 4747, 1534,   70, 3788, 3823,</span><br><span class="line">         7435, 4907, 5567,  201, 2834, 1519, 7066,  782,  782, 2063, 2031,  846]),</span><br><span class="line"> tensor([6731, 4770, 1787, 8118, 7577, 7066, 4817,  648, 7121, 1542, 6483, 7435,</span><br><span class="line">         7686, 2889, 1671, 5862, 1949, 7066, 2596, 4785, 3629, 1379, 2703, 7435,</span><br><span class="line">         6064, 6041, 4666, 4038, 4881, 7066, 4747, 1534,   70, 3788, 3823, 7435,</span><br><span class="line">         4907, 5567,  201, 2834, 1519, 7066,  782,  782, 2063, 2031,  846, 7435]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poem_loader =  DataLoader(poem_ds,</span><br><span class="line">                     batch_size=<span class="number">16</span>,</span><br><span class="line">                     shuffle=<span class="literal">True</span>,</span><br><span class="line">                     num_workers=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="模型构造"><a href="#模型构造" class="headerlink" title="模型构造"></a>模型构造</h2><p>模型使用embedding+LSTM来进行构造，embedding的理解参考 ，<a href="https://www.zybuluo.com/Dounm/note/591752">Word2Vec</a>和<a href="https://zhuanlan.zhihu.com/p/53194407">王喆的文章</a>。使用embedding层后将汉字转化为embedding向量，与简单使用One-hot编码可以更好地表示汉字的语义，同时减少特征维度。</p>
<p>向量化后使用LSTM网络来进行训练，LSTM的参数理解可能比较费劲，参考两张图进行理解：<br><a href="https://www.zhihu.com/question/41949741/answer/318771336">LSTM神经网络输入输出究竟是怎样的？ - Scofield的回答 - 知乎</a>中的<br><img src="https://pic2.zhimg.com/80/v2-b45f69904d546edde41d9539e4c5548c_720w.jpg" alt=""><br>这张图中就能比较好理解input_size=embedding_dim,hidden_dim这两个个参数了。输入的X的维度就是图中绿色节点的数目了，在这里是embedding_dim这个参数就是经过向量化后的每个汉字，hidden_dim就是图中黄色的节点个数了，只不过因为LSTM有h和c两个隐藏状态，所以hidden_dim同时设置了h和c两个隐藏层状态的维度，也就是图中的黄色节点需要乘以2，变成两个的。num_layers的理解参考<a href="https://zhuanlan.zhihu.com/p/79064602">LSTM细节分析理解（pytorch版） - ymmy的文章 - 知乎</a>的图片就是纵向的深度。<br><img src="https://pic4.zhimg.com/80/v2-ebf8cd2faa564d9d80a958dcf25e6b3b_720w.jpg" alt=""><br>LSTM的深度选择了3层，需要注意的是并不是越深越好，自己的初步实验显示加深到8层的效果并不比3层的效果好，因此选择了3层。</p>
<p>经过LSTM输出的维度也是hidden_dim，使用3层全连接来进一步处理。这里全连接层的激活函数选用的是tanh激活函数，因为初步的对比实验显示，tanh的效果比relu好一些。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import torch.nn.functional as F</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPoetryModel_tanh</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, hidden_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyPoetryModel_tanh, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)<span class="comment">#vocab_size:就是ix2word这个字典的长度。</span></span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=Config.LSTM_layers,</span><br><span class="line">                            batch_first=<span class="literal">True</span>,dropout=<span class="number">0</span>, bidirectional=<span class="literal">False</span>)</span><br><span class="line">        self.fc1 = nn.Linear(self.hidden_dim,<span class="number">2048</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">2048</span>,<span class="number">4096</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">4096</span>,vocab_size)</span><br><span class="line"><span class="comment">#         self.linear = nn.Linear(self.hidden_dim, vocab_size)# 输出的大小是词表的维度，</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden=<span class="literal">None</span></span>):</span></span><br><span class="line">        embeds = self.embeddings(<span class="built_in">input</span>)  <span class="comment"># [batch, seq_len] =&gt; [batch, seq_len, embed_dim]</span></span><br><span class="line">        batch_size, seq_len = <span class="built_in">input</span>.size()</span><br><span class="line">        <span class="keyword">if</span> hidden <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            h_0 = <span class="built_in">input</span>.data.new(Config.LSTM_layers*<span class="number">1</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).<span class="built_in">float</span>()</span><br><span class="line">            c_0 = <span class="built_in">input</span>.data.new(Config.LSTM_layers*<span class="number">1</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).<span class="built_in">float</span>()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            h_0, c_0 = hidden</span><br><span class="line">        output, hidden = self.lstm(embeds, (h_0, c_0))<span class="comment">#hidden 是h,和c 这两个隐状态</span></span><br><span class="line">        output = torch.tanh(self.fc1(output))</span><br><span class="line">        output = torch.tanh(self.fc2(output))</span><br><span class="line">        output = self.fc3(output)</span><br><span class="line">        output = output.reshape(batch_size * seq_len, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output,hidden</span><br></pre></td></tr></table></figure>
<h2 id="训练日志的设置"><a href="#训练日志的设置" class="headerlink" title="训练日志的设置"></a>训练日志的设置</h2><p>注释参考我之前的文章<a href="https://zhuanlan.zhihu.com/p/136421422">Kaggle猫狗识别Pytorch详细搭建过程 - BraveY的文章 - 知乎</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AvgrageMeter</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.reset()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.avg = <span class="number">0</span></span><br><span class="line">        self.<span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">        self.cnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, val, n=<span class="number">1</span></span>):</span></span><br><span class="line">        self.<span class="built_in">sum</span> += val * n</span><br><span class="line">        self.cnt += n</span><br><span class="line">        self.avg = self.<span class="built_in">sum</span> / self.cnt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## topk的准确率计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">output, label, topk=(<span class="params"><span class="number">1</span>,</span>)</span>):</span></span><br><span class="line">    maxk = <span class="built_in">max</span>(topk) </span><br><span class="line">    batch_size = label.size(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取前K的索引</span></span><br><span class="line">    _, pred = output.topk(maxk, <span class="number">1</span>, <span class="literal">True</span>, <span class="literal">True</span>) <span class="comment">#使用topk来获得前k个的索引</span></span><br><span class="line">    pred = pred.t() <span class="comment"># 进行转置</span></span><br><span class="line">    <span class="comment"># eq按照对应元素进行比较 view(1,-1) 自动转换到行为1,的形状， expand_as(pred) 扩展到pred的shape</span></span><br><span class="line">    <span class="comment"># expand_as 执行按行复制来扩展，要保证列相等</span></span><br><span class="line">    correct = pred.eq(label.view(<span class="number">1</span>, -<span class="number">1</span>).expand_as(pred)) <span class="comment"># 与正确标签序列形成的矩阵相比，生成True/False矩阵</span></span><br><span class="line"><span class="comment">#     print(correct)</span></span><br><span class="line"></span><br><span class="line">    rtn = []</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> topk:</span><br><span class="line">        correct_k = correct[:k].view(-<span class="number">1</span>).<span class="built_in">float</span>().<span class="built_in">sum</span>(<span class="number">0</span>) <span class="comment"># 前k行的数据 然后平整到1维度，来计算true的总个数</span></span><br><span class="line">        rtn.append(correct_k.mul_(<span class="number">100.0</span> / batch_size)) <span class="comment"># mul_() ternsor 的乘法  正确的数目/总的数目 乘以100 变成百分比</span></span><br><span class="line">    <span class="keyword">return</span> rtn</span><br></pre></td></tr></table></figure>
<h2 id="迭代训练函数"><a href="#迭代训练函数" class="headerlink" title="迭代训练函数"></a>迭代训练函数</h2><p>训练中使用tensorboard来绘制曲线，终端输入<code>tensorboard --logdir=/path_to_log_dir/ --port 6006</code> 可查看</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params"> epochs, train_loader, device, model, criterion, optimizer,scheduler,tensorboard_path</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    top1 = AvgrageMeter()</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        train_loss = <span class="number">0.0</span></span><br><span class="line">        train_loader = tqdm(train_loader)</span><br><span class="line">        train_loader.set_description(<span class="string">&#x27;[%s%04d/%04d %s%f]&#x27;</span> % (<span class="string">&#x27;Epoch:&#x27;</span>, epoch + <span class="number">1</span>, epochs, <span class="string">&#x27;lr:&#x27;</span>, scheduler.get_lr()[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):  <span class="comment"># 0是下标起始位置默认为0</span></span><br><span class="line">            inputs, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line"><span class="comment">#             print(&#x27; &#x27;.join(ix2word[inputs.view(-1)[k] for k in inputs.view(-1).shape.item()]))</span></span><br><span class="line">            labels = labels.view(-<span class="number">1</span>) <span class="comment"># 因为outputs经过平整，所以labels也要平整来对齐</span></span><br><span class="line">            <span class="comment"># 初始为0，清除上个batch的梯度信息</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            outputs,hidden = model(inputs)</span><br><span class="line">            loss = criterion(outputs,labels)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            _,pred = outputs.topk(<span class="number">1</span>)</span><br><span class="line"><span class="comment">#             print(get_word(pred))</span></span><br><span class="line"><span class="comment">#             print(get_word(labels))</span></span><br><span class="line">            prec1, prec2= accuracy(outputs, labels, topk=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">            n = inputs.size(<span class="number">0</span>)</span><br><span class="line">            top1.update(prec1.item(), n)</span><br><span class="line">            train_loss += loss.item()</span><br><span class="line">            postfix = &#123;<span class="string">&#x27;train_loss&#x27;</span>: <span class="string">&#x27;%.6f&#x27;</span> % (train_loss / (i + <span class="number">1</span>)), <span class="string">&#x27;train_acc&#x27;</span>: <span class="string">&#x27;%.6f&#x27;</span> % top1.avg&#125;</span><br><span class="line">            train_loader.set_postfix(log=postfix)</span><br><span class="line">            </span><br><span class="line"><span class="comment">#             break</span></span><br><span class="line">            <span class="comment"># ternsorboard 曲线绘制</span></span><br><span class="line">            <span class="keyword">if</span> os.path.exists(Config.tensorboard_path) == <span class="literal">False</span>: </span><br><span class="line">                os.mkdir(Config.tensorboard_path)    </span><br><span class="line">            writer = SummaryWriter(tensorboard_path)</span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;Train/Loss&#x27;</span>, loss.item(), epoch)</span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;Train/Accuracy&#x27;</span>, top1.avg, epoch)</span><br><span class="line">            writer.flush()</span><br><span class="line">        scheduler.step()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h2><p>初始化模型，并选择损失函数和优化函数。需要注意的是自己在训练过程中发现会发生loss上升的情况，这是因为到后面lr学习率过大导致的，解决的办法是是使用学习率动态调整。这里选择了步长调整的方法，每过10个epoch，学习率调整为原来的0.1。pytorch还有许多其他调整方法，参考这篇<a href="https://www.jianshu.com/p/26a7dbc15246">文章</a></p>
<p>初始学习率设置为0.001，稍微大点变成0.01最开始的训练效果都比较差，loss直接上百，不下降。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 上述参数的配置网络训练显存消耗为2395M，超过显存的话，重新调整下网络配置</span></span><br><span class="line">model = MyPoetryModel_tanh(<span class="built_in">len</span>(word2ix),</span><br><span class="line">                  embedding_dim=Config.embedding_dim,</span><br><span class="line">                  hidden_dim=Config.hidden_dim)</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">epochs = <span class="number">30</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=Config.lr)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = <span class="number">10</span>,gamma=<span class="number">0.1</span>)<span class="comment">#学习率调整</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MyPoetryModel_tanh(</span><br><span class="line">  (embeddings): Embedding(8293, 100)</span><br><span class="line">  (lstm): LSTM(100, 1024, num_layers&#x3D;3, batch_first&#x3D;True)</span><br><span class="line">  (fc1): Linear(in_features&#x3D;1024, out_features&#x3D;2048, bias&#x3D;True)</span><br><span class="line">  (fc2): Linear(in_features&#x3D;2048, out_features&#x3D;4096, bias&#x3D;True)</span><br><span class="line">  (fc3): Linear(in_features&#x3D;4096, out_features&#x3D;8293, bias&#x3D;True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># model.load_state_dict(torch.load(Config.model_save_path))  # 模型加载、</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;All keys matched successfully&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#因为使用tensorboard画图会产生很多日志文件，这里进行清空操作</span></span><br><span class="line"><span class="keyword">import</span> shutil  </span><br><span class="line"><span class="keyword">if</span> os.path.exists(Config.tensorboard_path):</span><br><span class="line">    shutil.rmtree(Config.tensorboard_path)  </span><br><span class="line">    os.mkdir(Config.tensorboard_path)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train(epochs, poem_loader, device, model, criterion, optimizer,scheduler, Config.tensorboard_path)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#模型保存</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(Config.model_save_path) == <span class="literal">False</span>: </span><br><span class="line">    os.mkdir(Config.model_save_path)   </span><br><span class="line">torch.save(model.state_dict(), Config.model_save_path)</span><br></pre></td></tr></table></figure>
<h2 id="模型使用"><a href="#模型使用" class="headerlink" title="模型使用"></a>模型使用</h2><p>使用训练好的模型来进行自动写诗创作，模型训练了30多个epoch，’train_loss’: ‘0.452125’, ‘train_acc’: ‘91.745990’</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.load_state_dict(torch.load(Config.model_save_path))  <span class="comment"># 模型加载</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;All keys matched successfully&gt;</span><br></pre></td></tr></table></figure>
<p>生成的逻辑是输入一个汉字之后，给出对应的预测输出，如果这个输出所在的范围在给定的句子中，就摒弃这个输出，并用给定句子的下一个字做为输入，<br>直到输出的汉字超过给定的句子范围，用预测的输出句子作为下一个输入。<br>因为每次模型输出的还包括h和c两个隐藏状态，所以前面的输入都会更新隐藏状态，来影响当前的输出。也就是hidden这个tensor是一直在模型中传递，只要没有结束</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">model, start_words, ix2word, word2ix,device</span>):</span></span><br><span class="line">    results = <span class="built_in">list</span>(start_words)</span><br><span class="line">    start_words_len = <span class="built_in">len</span>(start_words)</span><br><span class="line">    <span class="comment"># 第一个词语是&lt;START&gt;</span></span><br><span class="line">    <span class="built_in">input</span> = torch.Tensor([word2ix[<span class="string">&#x27;&lt;START&gt;&#x27;</span>]]).view(<span class="number">1</span>, <span class="number">1</span>).long()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#最开始的隐状态初始为0矩阵</span></span><br><span class="line">    hidden = torch.zeros((<span class="number">2</span>, Config.LSTM_layers*<span class="number">1</span>,<span class="number">1</span>,Config.hidden_dim),dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">    <span class="built_in">input</span> = <span class="built_in">input</span>.to(device)</span><br><span class="line">    hidden = hidden.to(device)</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">48</span>):<span class="comment">#诗的长度</span></span><br><span class="line">                output, hidden = model(<span class="built_in">input</span>, hidden)</span><br><span class="line">                <span class="comment"># 如果在给定的句首中，input为句首中的下一个字</span></span><br><span class="line">                <span class="keyword">if</span> i &lt; start_words_len:</span><br><span class="line">                    w = results[i]</span><br><span class="line">                    <span class="built_in">input</span> = <span class="built_in">input</span>.data.new([word2ix[w]]).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">               <span class="comment"># 否则将output作为下一个input进行</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    top_index = output.data[<span class="number">0</span>].topk(<span class="number">1</span>)[<span class="number">1</span>][<span class="number">0</span>].item()<span class="comment">#输出的预测的字</span></span><br><span class="line">                    w = ix2word[top_index]</span><br><span class="line">                    results.append(w)</span><br><span class="line">                    <span class="built_in">input</span> = <span class="built_in">input</span>.data.new([top_index]).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> w == <span class="string">&#x27;&lt;EOP&gt;&#x27;</span>: <span class="comment"># 输出了结束标志就退出</span></span><br><span class="line">                    <span class="keyword">del</span> results[-<span class="number">1</span>]</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = generate(model,<span class="string">&#x27;雨&#x27;</span>, ix2word,word2ix,device)</span><br><span class="line">print(<span class="string">&#x27; &#x27;</span>.join(i <span class="keyword">for</span> i <span class="keyword">in</span> results))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">雨 余 芳 草 净 沙 尘 ， 水 绿 滩 平 一 带 春 。 唯 有 啼 鹃 似 留 客 ， 桃 花 深 处 更 无 人 。</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = generate(model,<span class="string">&#x27;湖光秋月两相得&#x27;</span>, ix2word,word2ix,device)</span><br><span class="line">print(<span class="string">&#x27; &#x27;</span>.join(i <span class="keyword">for</span> i <span class="keyword">in</span> results))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">湖 光 秋 月 两 相 得 ， 楚 调 抖 纹 难 自 干 。 唱 至 公 来 尊 意 敬 ， 为 君 急 唱 曲 江 清 。</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = generate(model,<span class="string">&#x27;人生得意须尽欢，&#x27;</span>, ix2word,word2ix,device)</span><br><span class="line">print(<span class="string">&#x27; &#x27;</span>.join(i <span class="keyword">for</span> i <span class="keyword">in</span> results))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">人 生 得 意 须 尽 欢 ， 吾 见 古 人 未 能 休 。 空 令 月 镜 终 坐 我 ， 梦 去 十 年 前 几 回 。 谁 谓 一 朝 天 不 极 ， 重 阳 堪 发 白 髭 肥 。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = generate(model,<span class="string">&#x27;万里悲秋常作客，&#x27;</span>, ix2word,word2ix,device)</span><br><span class="line">print(<span class="string">&#x27; &#x27;</span>.join(i <span class="keyword">for</span> i <span class="keyword">in</span> results))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">万 里 悲 秋 常 作 客 ， 伤 人 他 日 识 文 诚 。 经 时 偏 忆 诸 公 处 ， 一 叶 黄 花 未 有 情 。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = generate(model,<span class="string">&#x27;风急天高猿啸哀，渚清沙白鸟飞回。&#x27;</span>, ix2word,word2ix,device)</span><br><span class="line">print(<span class="string">&#x27; &#x27;</span>.join(i <span class="keyword">for</span> i <span class="keyword">in</span> results))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">风 急 天 高 猿 啸 哀 ， 渚 清 沙 白 鸟 飞 回 。 孤 吟 一 片 秋 云 起 ， 漏 起 傍 天 白 雨 来 。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = generate(model,<span class="string">&#x27;千山鸟飞绝，万径人踪灭。&#x27;</span>, ix2word,word2ix,device)</span><br><span class="line">print(<span class="string">&#x27; &#x27;</span>.join(i <span class="keyword">for</span> i <span class="keyword">in</span> results))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">千 山 鸟 飞 绝 ， 万 径 人 踪 灭 。 日 暮 沙 外 亭 ， 自 思 林 下 客 。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch/tree/master/docs/chapter06_RNN">Dive-into-DL-Pytorch</a> RNN章节</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Extreme Weather A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events</title>
    <url>/2020-04-27-Extreme-Weather-A-large-scale-climate-dataset-for-semi-supervised-detection,-localization,-and-understanding.html</url>
    <content><![CDATA[<h2 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h2><p>2017NIPS</p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>当有大量标记数据可用时，完全监督的卷积神经网络（CNN）可以为分类众所周知的极端天气事件提供可接受的准确性。但是，许多不同类型的空间局部气候模式引起人们的兴趣，包括飓风，温带气旋，天气前沿和阻塞事件等。<br>这些模式的<strong>现有标记数据可能以各种方式不完整</strong>，例如仅覆盖某些年份或地理区域并具有假阴性。<br>因此，这种类型的气候数据带来了许多有趣的机器学习挑战。<br>本文提出了一种多通道时空CNN体系结构，用于<strong>半监督边界框预测和探索性数据分析</strong>。<br>本文证明了本文的方法能够利用时间信息和未标记的数据来改善极端天气事件的定位。<br>此外，本文探索了模型学习到的表示形式，以便更好地理解这一重要数据。<br>本文<strong>提供了一个极端天气数据集</strong>，以鼓励该领域的机器学习研究，并帮助促进进一步的工作，以了解和缓解气候变化的影响。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><hr>
<h2 id="图表"><a href="#图表" class="headerlink" title="图表"></a>图表</h2><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在气候数据中查找极端天气事件的任务类似于检测视频中的物体和活动的任务-深度学习技术的流行应用。</p>
<p>一个重要的区别是，就气候数据而言，“视频”具有16个或更多的“渠道”信息（例如水蒸气，压力和温度），而常规视频只有3个（RGB）。<br>此外，气候模拟与自然图像的统计数据不同。</p>
<p>实现了3D（高度，宽度，时间）卷积编码器解码器，并在瓶颈处应用了新颖的单遍边界框回归损失。这是深度自动编码体系结构首次用于边界框回归。</p>
<p>本文的主要贡献是（1）基线Bounding Box损失公式；<br>（2）本文的架构是从针对极端天气事件的工程启发式技术迈向半监督学习型功能的第一步；<br>（3）ExtremeWeather数据集，本文将其分为三个基准测试组：一个用于模型探索的小型模型，一个用于中等模型的模型，以及一个包含整整27年气候模拟输出的模型。</p>
<hr>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>气候科学界使用三种全球数据集：观测数据（卫星，网格气象站）；以及再分析数据（通过将不同的观测产品同化为气候模型获得）和模拟数据。</p>
<p>本文使用的是模拟数据，尽管此数据集包含了过去的信息，但在此数据集上进行深度学习的性能仍然可以告知这些方法在未来模拟中的有效性。</p>
<p>使用CAM5（Community Atmospheric Model v5）模拟，当空间分辨率为25km时，模型输出的快照图像尺寸为768*1152，16个通道（地表温度，地表压力，降水等），时间分辨率为3小时，每一天给出8张快照。</p>
<p>1979年到2005年的模拟，总共78840张16通道768*1152尺寸的图片。</p>
<h3 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h3><p>使用TECA标记的热带低压（TD）热带气旋（TC），热带气旋（ETC）和大气河流（AR）。</p>
<p>使用TECA 标记可能有错误，本身一些类别的定义就有争议。</p>
<h3 id="划分"><a href="#划分" class="headerlink" title="划分"></a>划分</h3><p>训练集测试集的划分</p>
<p><img src="/2020-04-27-Extreme-Weather-A-large-scale-climate-dataset-for-semi-supervised-detection,-localization,-and-understanding.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-03 154658.jpg" alt=""></p>
<p>小规模数据集中的标签分布：</p>
<p><img src="/2020-04-27-Extreme-Weather-A-large-scale-climate-dataset-for-semi-supervised-detection,-localization,-and-understanding.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-03 154744.jpg" alt=""></p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><strong>baseline模型：</strong></p>
<p>encoder-decoder架构，使用CNN编码，解码器也是CNN使用绑定权重和反卷积层，leaky RELU激活函数</p>
<p>采用半监督的方法，自动编码器的代码（瓶颈）层用作损失层的输入，从而预测：</p>
<ul>
<li>Bbox 的位置和尺寸</li>
<li>Bbox的类别</li>
<li>Bbox的置信度</li>
</ul>
<p><img src="/2020-04-27-Extreme-Weather-A-large-scale-climate-dataset-for-semi-supervised-detection,-localization,-and-understanding.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-04-27 193119.jpg" alt=""></p>
<p>损失函数：有监督的边框回归损失+无监督的重构损失</p>
<script type="math/tex; mode=display">
L=L_{s u p}+\lambda L_{r e c}</script><p>重构损失：输入与重构的均方差，M是图片中的总的像素数</p>
<script type="math/tex; mode=display">
L_{r e c}=\frac{1}{M}\left\|X-X^{*}\right\|_{2}^{2}</script><p>将$798<em>1152$的图片用$64</em>64$的锚点框划分为$12*18$的网格，对每个预测框计算3种分数（1）预测框与锚框的大小和位置有多少不同；</p>
<p>（2）感兴趣的对象在预测框中的置信度（“对象”）；</p>
<p>（3）该对象的类别概率分布。</p>
<p>每一个部分使用$3<em>3$的卷积核，相当于$192</em>192$像素尺寸的感受野。</p>
<p>边界框回归损失：与YOLO的定义相似</p>
<script type="math/tex; mode=display">
L_{s u p}=\frac{1}{N}\left(L_{b o x}+L_{c o n f}+L_{c l s}\right)</script><p>N是时间步，$L_{box}$的损失：</p>
<script type="math/tex; mode=display">
L_{b o x}=\alpha \sum_{i} \mathbb{1}_{i}^{o b j} R\left(u_{i}-u_{i}^{*}\right)+\beta \sum_{i} \mathbb{1}_{i}^{o b j} R\left(v_{i}-v_{i}^{*}\right)</script><p>u和v是类似SSD的box坐标转移。R是平滑L1损失</p>
<p>置信度损失：</p>
<script type="math/tex; mode=display">
L_{c o n f}=\sum_{i} \mathbb{1}_{i}^{o b j}\left[-\log \left(p(o b j)_{i}\right)\right]+\gamma * \sum_{i} \mathbb{1}_{i}^{n o o b j}\left[-\log \left(p\left(\overline{o b j}_{i}\right)\right)\right]</script><p>交叉熵损失</p>
<script type="math/tex; mode=display">
L_{c l s}=\sum_{i} \mathbb{1}_{i}^{o b j} \sum_{c \in \text {classes}}-p^{*}(c) \log (p(c))</script><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>使用2维的卷积编码器提取的特征图：</p>
<p><img src="/2020-04-27-Extreme-Weather-A-large-scale-climate-dataset-for-semi-supervised-detection,-localization,-and-understanding.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-03 161939.jpg" alt=""></p>
<p>使用2维和3维卷积，以及监督与半监督模型的实验结果。</p>
<p><img src="/2020-04-27-Extreme-Weather-A-large-scale-climate-dataset-for-semi-supervised-detection,-localization,-and-understanding.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-03 162312.jpg" alt=""></p>
<p>使用t-SNE可视化监督与半监督学习到的特征</p>
<p><img src="/2020-04-27-Extreme-Weather-A-large-scale-climate-dataset-for-semi-supervised-detection,-localization,-and-understanding.htm/Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-06-03 162952.jpg" alt=""></p>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><hr>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ol>
<li>主要贡献数据集的提供。</li>
<li>TECA已经可以标记了，模型的作用呢？</li>
<li>数据集的创新点：1. 对人类更有意义 2.挑战更高 ，多维度。多通道就可以了？</li>
<li>卷积编码器？</li>
<li>模型输入16维的数据，然后再图像上标记出极端天气的边框，半监督体现在哪儿？</li>
</ol>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>半监督</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ return一个对象的执行细节</title>
    <url>/2020-04-19-C++-return%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1%E7%9A%84%E6%89%A7%E8%A1%8C%E7%BB%86%E8%8A%82.html</url>
    <content><![CDATA[<h1 id="C-return一个对象的执行细节"><a href="#C-return一个对象的执行细节" class="headerlink" title="C++ return一个对象的执行细节"></a>C++ return一个对象的执行细节</h1><p>同学发过来一道考试题，要求解释输出。好久没有用过这些知识了，所以顺便复习下。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1587302660/blog/cpp_test.jpg" alt=""></p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>为了搞清楚里面知识，所以把代码增加了地址的输出来进行实验：</p>
<figure class="highlight cc"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;limits.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">	Solution(): id_(<span class="number">0</span>) &#123;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Default Constructor:id=&quot;</span></span><br><span class="line">		     &lt;&lt; id_ &lt;&lt; <span class="string">&quot;	addr=:&quot;</span> &lt;&lt; <span class="keyword">this</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	Solution(<span class="keyword">int</span> id): id_(id) &#123;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Constructor with Paramter:id=&quot;</span></span><br><span class="line">		     &lt;&lt; id_ &lt;&lt; <span class="string">&quot;	addr=:&quot;</span> &lt;&lt; <span class="keyword">this</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	Solution(<span class="keyword">const</span> Solution&amp;course): id_(course.id_) &#123;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Copy Constructor :id=&quot;</span></span><br><span class="line">		     &lt;&lt; id_ &lt;&lt; <span class="string">&quot;	addr=:&quot;</span> &lt;&lt; <span class="keyword">this</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	~Solution() &#123;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Destructor:id=&quot;</span></span><br><span class="line">		     &lt;&lt; id_ &lt;&lt; <span class="string">&quot;	addr=:&quot;</span> &lt;&lt; <span class="keyword">this</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">int</span> <span class="title">GetId</span><span class="params">()</span> </span>&#123;<span class="keyword">return</span> id_;&#125;;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>:</span><br><span class="line">	<span class="keyword">int</span> id_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">Solution <span class="title">Max</span><span class="params">(Solution a, Solution b)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (a.GetId() &gt; b.GetId()) &#123;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;end &quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="keyword">return</span> a;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;end after return&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;end &quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">		<span class="keyword">return</span> b;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;end after return&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Q2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="function">Solution <span class="title">class1</span><span class="params">(<span class="number">1</span>)</span></span>;</span><br><span class="line">	<span class="comment">//Solution class2;</span></span><br><span class="line">	Solution class2 = class1;</span><br><span class="line">	<span class="comment">//class2 = class1;</span></span><br><span class="line">	Solution classX(5), classY(6);</span><br><span class="line">	Solution classZ;</span><br><span class="line">	<span class="comment">// classZ = class1;</span></span><br><span class="line">	<span class="comment">// Max(classX, classY);</span></span><br><span class="line">	classZ = Max(classX, classY);</span><br><span class="line">	<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;after max&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">	Solution *pClasses = <span class="keyword">new</span> Solution[<span class="number">3</span>];</span><br><span class="line">	<span class="keyword">delete</span> [] pClasses;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> <span class="keyword">const</span> *argv[])</span> </span>&#123;</span><br><span class="line">	<span class="comment">/* code */</span></span><br><span class="line">	Q2();</span><br><span class="line">	system(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出的结果是：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Constructor with Paramter:id=1  addr=:0x61fedc </span><br><span class="line">Copy Constructor :id=1  addr=:0x61fed8</span><br><span class="line">Constructor with Paramter:id=5  addr=:0x61fed4</span><br><span class="line">Constructor with Paramter:id=6  addr=:0x61fed0</span><br><span class="line">Default Constructor:id=0        addr=:0x61fecc</span><br><span class="line">Copy Constructor :id=6  addr=:0x61fee4</span><br><span class="line">Copy Constructor :id=5  addr=:0x61fee8</span><br><span class="line">end</span><br><span class="line">Copy Constructor :id=6  addr=:0x61fee0</span><br><span class="line">Destructor:id=6 addr=:0x61fee0</span><br><span class="line">Destructor:id=5 addr=:0x61fee8</span><br><span class="line">Destructor:id=6 addr=:0x61fee4</span><br><span class="line">after max</span><br><span class="line">Default Constructor:id=0        addr=:0xfc1d74</span><br><span class="line">Default Constructor:id=0        addr=:0xfc1d78</span><br><span class="line">Default Constructor:id=0        addr=:0xfc1d7c</span><br><span class="line">Destructor:id=0 addr=:0xfc1d7c</span><br><span class="line">Destructor:id=0 addr=:0xfc1d78</span><br><span class="line">Destructor:id=0 addr=:0xfc1d74</span><br><span class="line">Destructor:id=6 addr=:0x61fecc</span><br><span class="line">Destructor:id=6 addr=:0x61fed0</span><br><span class="line">Destructor:id=5 addr=:0x61fed4</span><br><span class="line">Destructor:id=1 addr=:0x61fed8</span><br><span class="line">Destructor:id=1 addr=:0x61fedc</span><br></pre></td></tr></table></figure>
<p>可以看到输出结果与题目中的结果是有差别的，题目中的篮圈析构顺序是566，实验结果是656</p>
<h2 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h2><p>在调用Max之前的输出结果很容易理解</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Constructor with Paramter:id=1  addr=:0x61fedc  # class1 的参数构造</span><br><span class="line">Copy Constructor :id=1  addr=:0x61fed8			#Solution class2 = class1的拷贝构造</span><br><span class="line">Constructor with Paramter:id=5  addr=:0x61fed4  #classX的参数构造</span><br><span class="line">Constructor with Paramter:id=6  addr=:0x61fed0  #classY的参数构造</span><br><span class="line">Default Constructor:id=0        addr=:0x61fecc	#classZ的默认构造</span><br></pre></td></tr></table></figure>
<p>进入Max函数后从右向左依次进行参数的压栈，所以首先是b的拷贝构造，然后是a的拷贝构造。</p>
<p>接下来是一个很<strong>重要的细节</strong>，在执行return的时候会自动执行一个拷贝构造来生成一个临时的对象temp用来返回。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Copy Constructor :id=6  addr=:0x61fee4 			#b的拷贝构造</span><br><span class="line">Copy Constructor :id=5  addr=:0x61fee8 			#a的拷贝构造</span><br><span class="line">end</span><br><span class="line">Copy Constructor :id=6  addr=:0x61fee0			#return 产生的临时对象temp的拷贝构造</span><br></pre></td></tr></table></figure>
<p>在Max函数中参数入栈的顺序是：先是b然后是a最后是temp。</p>
<p>需要注意的是，在执行完return之后并没有立即就执行了出栈的操作。需要在完成<code>classZ = Max(classX, classY);</code> 这个赋值语句后才会执行出栈操作对栈中的对象执行析构操作。析构的顺序就是出栈的顺序，先入后出，后入先出。所以首先是temp变量的析构，然后是b这个形参的析构，最后才是a这个形参的析构。</p>
<p>所以参考链接里面的顺序也是有误的，并不是首先析构形参。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">after max</span><br><span class="line">Default Constructor:id=0        addr=:0xfc1d74 # temp变量的析构</span><br><span class="line">Default Constructor:id=0        addr=:0xfc1d78 # b这个形参的析构</span><br><span class="line">Default Constructor:id=0        addr=:0xfc1d7c # a这个形参的析构</span><br></pre></td></tr></table></figure>
<p>之后因为使用new来创建的数据，其内存是分配在堆上面的并没有在Max的函数栈中，需要手动delete来销毁来防止内存泄漏。因此首先是3个0个构造和析构。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> new</span></span><br><span class="line">Default Constructor:id=0        addr=:0xfc1d74</span><br><span class="line">Default Constructor:id=0        addr=:0xfc1d78</span><br><span class="line">Default Constructor:id=0        addr=:0xfc1d7c</span><br><span class="line"><span class="meta">#</span><span class="bash">delete</span></span><br><span class="line">Destructor:id=0 addr=:0xfc1d7c</span><br><span class="line">Destructor:id=0 addr=:0xfc1d78</span><br><span class="line">Destructor:id=0 addr=:0xfc1d74</span><br></pre></td></tr></table></figure>
<p>最后是对Max这个函数中入栈的局部变量执行出栈与销毁的操作。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Destructor:id=6 addr=:0x61fecc 		#classZ的析构</span><br><span class="line">Destructor:id=6 addr=:0x61fed0		#classY的析构</span><br><span class="line">Destructor:id=5 addr=:0x61fed4		#classX的析构</span><br><span class="line">Destructor:id=1 addr=:0x61fed8		#class2的析构</span><br><span class="line">Destructor:id=1 addr=:0x61fedc		#class1的析构</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>主要是两个点比较迷惑</p>
<ol>
<li>Max函数return 的操作是通过自动调用拷贝构造一个临时对象来执行的返回的，临时对象的析构需要等到<code>classZ = Max(classX, classY);</code>语句执行完毕后</li>
<li>如果先构造了一个对象，然后再使用=来进行赋值是不会执行拷贝构造的，不是只要使用了=就会执行拷贝构造。所以<code>Solution class2 = class1;</code>这个语句没有先构造对象，因此执行了拷贝构造。而<code>classZ = Max(classX, classY);</code> 这个语句中，<code>classZ</code>事先用<code>Solution classZ;</code>构造过，所以不会再执行拷贝构造的操作。这个=只负责值之间的传递。</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/fruitz/article/details/41624017">https://blog.csdn.net/fruitz/article/details/41624017</a>  析构顺序有误</p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>构造函数</tag>
        <tag>析构函数</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux运维</title>
    <url>/2020-04-10-Linux%E8%BF%90%E7%BB%B4.html</url>
    <content><![CDATA[<h1 id="Linux运维"><a href="#Linux运维" class="headerlink" title="Linux运维"></a>Linux运维</h1><p>记录在使用Linux过程中遇到的一些问题的解决方式</p>
<p><strong>jupyter lab无权限创建文件</strong></p>
<p><code>chmod 777 file_path</code> 解决</p>
<p><strong>jupyter lab 服务器地址访问被拒绝</strong></p>
<p>启动时加上 —ip=0.0.0.0</p>
<p><code>jupyter lab --port=8889 --ip=0.0.0.0</code></p>
<p><strong>jupyter lab tqdm 进度条不显示</strong></p>
<p>出现</p>
<p><code>HBox(children=(FloatProgress……</code></p>
<p>解决：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 首先需要打开ipywidgets的插件</span></span><br><span class="line">jupyter nbextension enable --py widgetsnbextension</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> 然后还需要安装 Jupyter Lab的相关插件</span></span><br><span class="line">jupyter labextension install @jupyter-widgets/jupyterlab-manager</span><br></pre></td></tr></table></figure>
<p><strong>zip文件解压</strong></p>
<p><code>unzip file -d outputdir</code></p>
<p><strong>数据文件软链接</strong></p>
<p><code>ln -s real_path ./data</code> 从real_path 映射到data目录。</p>
<p><strong>Python本地导入模块</strong></p>
<p>注意导入的是模块的所在目录，对于包来说就是包这个文件夹所在的目录，而不是包中文件所在目录。</p>
<p><code>export PYTHONPATH=$PYTHONPATH:/model/path</code></p>
<p><strong>查看cuda版本</strong></p>
<p><code>cat /usr/local/cuda/version.txt</code></p>
<p><strong>tensorflow查看GPU是否可用</strong></p>
<p><code>print(&#39;GPU&#39;,tf.test.is_gpu_available())</code></p>
<p><strong>文件排序查看</strong></p>
<p>按大小查看</p>
<p>-s表示按文件大小排序，-l表示以长格式显示，即列出文件详细信息如上面的格式。 S是大写</p>
<p><code>ls -lSh</code>  降序查看</p>
<p><code>ls -lrSh</code> 升序查看</p>
<p>按时间查看， -t表示按时间排序，-r表示逆序</p>
<p><code>ls -lt</code> 降序查看</p>
<p><code>ls -lrt</code>升序查看</p>
<h2 id="查看文件夹大小"><a href="#查看文件夹大小" class="headerlink" title="查看文件夹大小"></a>查看文件夹大小</h2><p><code>du -h --max-depth=1</code></p>
<h2 id="vim"><a href="#vim" class="headerlink" title="vim"></a>vim</h2><p><strong>vim批量注释</strong></p>
<p><code>Ctrl + v</code> 列编辑，然后<code>Shift + i</code> 插入<code>#</code>之后连续两下Esc</p>
<p><strong>vim批量删除</strong></p>
<p><code>Ctrl + v</code> 列编辑，然后<code>d</code> 删除之后连续两下Esc</p>
<p><strong>vim批量粘贴</strong></p>
<p><code>v</code> 行视图，然后选中对应的行,<code>y</code> 复制，之后到需要粘贴的地方<code>p</code> 进行粘贴</p>
<p><strong>vim粘贴格式混乱</strong></p>
<p><code>:set paste</code>后<code>i</code>在粘贴</p>
<h2 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h2><h3 id="免密登录"><a href="#免密登录" class="headerlink" title="免密登录"></a>免密登录</h3><h4 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h4><ol>
<li>本机创建公钥和私钥文件 已有的话不用创建<code>ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsa -C &quot;xx-email&quot;</code></li>
<li>将本机的公钥拷贝到服务器上<ol>
<li><code>cat ~/.ssh/id_rsa.pub | ssh user@serverip &quot;cat - &gt;&gt; ~/.ssh/authorized_keys&quot;</code></li>
</ol>
</li>
<li>权限相关（chmod）<ol>
<li>.ssh目录的权限必须是700</li>
<li>.ssh/authorized_keys文件权限必须是600</li>
</ol>
</li>
</ol>
<p><a href="https://juejin.im/post/6844903734233792519">参考</a></p>
<h3 id="scp"><a href="#scp" class="headerlink" title="scp"></a>scp</h3><h3 id="复制文件"><a href="#复制文件" class="headerlink" title="复制文件"></a>复制文件</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scp -r local_folder remote_username@remote_ip:remote_folder </span><br></pre></td></tr></table></figure>
<h2 id="进程相关"><a href="#进程相关" class="headerlink" title="进程相关"></a>进程相关</h2><p>查看进程是否有子进程，或者是否开启了多个线程。ps 只能显示进程的pid，线程的pid无法获取，但是htop可以显示线程的pid。</p>
<p><code>cd /proc/pid/task</code>里面的目录数便是这个pid管理的子进程，子线程。</p>
<p><strong>查看进程之间的关系</strong></p>
<p><code>pstree -Aup</code> 显示所有进程之间的关系，也会显示生成的子线程。</p>
<h3 id="批量杀死"><a href="#批量杀死" class="headerlink" title="批量杀死"></a>批量杀死</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ps -ef | grep test | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9</span><br></pre></td></tr></table></figure>
<h2 id="shell脚本"><a href="#shell脚本" class="headerlink" title="shell脚本"></a>shell脚本</h2><h3 id="xargs"><a href="#xargs" class="headerlink" title="xargs"></a>xargs</h3><p>xargs 可以将管道或标准输入（stdin）数据转换成命令行参数，也能够从文件的输出中读取数据。</p>
<p>格式：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">somecommand |xargs -item  <span class="built_in">command</span></span><br></pre></td></tr></table></figure>
<h3 id="判断文件夹是否存在"><a href="#判断文件夹是否存在" class="headerlink" title="判断文件夹是否存在"></a>判断文件夹是否存在</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> [ ! -d <span class="string">&quot;/data/&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">  mkdir /data</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;文件夹已经存在&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<p>判断文件是否存在修改<code>-d</code> 为<code>-e</code></p>
<h2 id="用户相关"><a href="#用户相关" class="headerlink" title="用户相关"></a>用户相关</h2><h3 id="创建用户授予sudo"><a href="#创建用户授予sudo" class="headerlink" title="创建用户授予sudo"></a>创建用户授予sudo</h3><p>Ubuntu下</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo adduser user</span><br><span class="line">usermod -aG sudo username</span><br></pre></td></tr></table></figure>
<p>还可以修改<code>/etc/sudoers</code>但是需要root用户<code>wq!</code>来强制写入。</p>
<h3 id="查看登录用户的进程"><a href="#查看登录用户的进程" class="headerlink" title="查看登录用户的进程"></a>查看登录用户的进程</h3><p><code>w</code></p>
<h2 id="内存信息查看"><a href="#内存信息查看" class="headerlink" title="内存信息查看"></a>内存信息查看</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo dmidecode -t memory</span><br></pre></td></tr></table></figure>
<h2 id="内存通道数查看"><a href="#内存通道数查看" class="headerlink" title="内存通道数查看"></a>内存通道数查看</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dmidecode -t memory | grep Bank</span><br></pre></td></tr></table></figure>
<p><strong>dmidecode命令</strong>可以让你在Linux系统下获取有关硬件方面的信息<a href="https://man.linuxde.net/dmidecode#:~:text=dmidecode%E5%91%BD%E4%BB%A4%E5%8F%AF%E4%BB%A5%E8%AE%A9%E4%BD%A0,%E6%98%AF%E7%B3%BB%E7%BB%9F%E5%87%86%E7%A1%AE%E7%9A%84%E4%BF%A1%E6%81%AF%E3%80%82">参考</a></p>
<p>grep 加上参数 -A5表示多显示5行<code>grep -A5 Bank</code></p>
<h2 id="Linux发行版查看"><a href="#Linux发行版查看" class="headerlink" title="Linux发行版查看"></a>Linux发行版查看</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &#x2F;etc&#x2F;os-release</span><br></pre></td></tr></table></figure>
<h2 id="个性化"><a href="#个性化" class="headerlink" title="个性化"></a>个性化</h2><p>分别去github上找对应的配置。</p>
<p><a href="https://github.com/ohmyzsh/ohmyzsh">zsh</a></p>
<p><a href="https://github.com/gpakosz/.tmux">tmux</a></p>
<p><a href="https://github.com/amix/vimrc">vimrc</a></p>
<h3 id="切换zsh"><a href="#切换zsh" class="headerlink" title="切换zsh"></a>切换zsh</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">chsh -s /bin/zsh user</span><br></pre></td></tr></table></figure>
<p>在tmux 则是修改.tmux.conf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;github.com&#x2F;ohmyzsh&#x2F;ohmyzsh</span><br></pre></td></tr></table></figure>
<h2 id="程序链接"><a href="#程序链接" class="headerlink" title="程序链接"></a>程序链接</h2><p><code>ldd</code>来查看可执行文件依赖的动态链接库路径</p>
<p><a href="https://stackoverflow.com/questions/16710047/usr-bin-ld-cannot-find-lnameofthelibrary">usr/bin/ld: cannot find -l</a></p>
<h2 id="网络相关"><a href="#网络相关" class="headerlink" title="网络相关"></a>网络相关</h2><h3 id="端口查看"><a href="#端口查看" class="headerlink" title="端口查看"></a>端口查看</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">netstat -atn           # For tcp</span><br><span class="line">netstat -aun           # For udp</span><br><span class="line">netstat -atun          # For both</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title>天池Docker练习场实践</title>
    <url>/2020-03-24-%E5%A4%A9%E6%B1%A0Docker%E7%BB%83%E4%B9%A0%E5%9C%BA%E5%AE%9E%E8%B7%B5.html</url>
    <content><![CDATA[<h1 id="天池Docker练习场实践"><a href="#天池Docker练习场实践" class="headerlink" title="天池Docker练习场实践"></a>天池Docker练习场实践</h1><p>基本的操作框架详细参考<a href="https://tianchi.aliyun.com/competition/entrance/231759/tab/174">官方的手把手教程</a>即可，主要记录下自己实践过程中遇到的问题。</p>
<h2 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h2><p>参与者可<strong>分阶段</strong>提交容器镜像完成以下3个任务（分数依次占 30/30/40），根据评分系统的分数返回验证任务的完成情况。</p>
<ul>
<li>输出<code>Hello world</code></li>
<li>计算 <code>/tcdata/num_list.csv</code>中一列数字的总和。</li>
<li>在<code>/tcdata/num_list.csv</code>文件中寻找最大的10个数，从大到小生成一个ListList.</li>
</ul>
<p><code>num_list.csv</code>文件中只有一列不为负的整数，其中存在重复值，示例如下：</p>
<blockquote>
<p>102<br>6<br>11<br>11</p>
</blockquote>
<p>生成入口脚本<code>run.sh</code>，放置于镜像工作目录。运行后生成结果<code>result.json</code>放置于工作目录（与<code>run.sh</code>同目录），评分系统将根据<code>result.json</code>进行打分。json文件如下所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;  </span><br><span class="line">    &quot;Q1&quot;:&quot;Hello world&quot;, </span><br><span class="line">    &quot;Q2&quot;:sum值, </span><br><span class="line">    &quot;Q3&quot;:[top10_list] </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h2><h3 id="生成测试文件"><a href="#生成测试文件" class="headerlink" title="生成测试文件"></a>生成测试文件</h3><p>首先需要在自己本地的机子上面构造一个和官方格式一样的csv文件以方便自己调试。</p>
<p>因为是csv文件所以很自然的想到使用pandas来处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame <span class="keyword">as</span> df</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></table></figure>
<p>使用随机数来生成数据，然后转换为dataframe，再保存在csv文件中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_dict = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    data_dict.append(np.random.randint(<span class="number">0</span>,<span class="number">20</span>))</span><br><span class="line">data_df = df(data_dict)    </span><br></pre></td></tr></table></figure>
<p>因为csv文件只有一列，所以保存的时候需要关闭header和index</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_df.to_csv(<span class="string">&quot;num_list.csv&quot;</span>,header=<span class="literal">False</span>,index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="读取文件处理"><a href="#读取文件处理" class="headerlink" title="读取文件处理"></a>读取文件处理</h3><p>有了测试数据后就可以开始读入数据并进行处理了，读入的时候依然需要关闭header</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_df.to_csv(<span class="string">&quot;num_list.csv&quot;</span>,header=<span class="literal">False</span>,index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>任务的要求很简单直接放代码了，建立一个solution.py，内容如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_df.to_csv(<span class="string">&quot;/tcdata/num_list.csv&quot;</span>,header=<span class="literal">False</span>,index=<span class="literal">False</span>)</span><br><span class="line">read_list = <span class="built_in">list</span>(read_df.index)</span><br><span class="line">read_list = <span class="built_in">sorted</span>(read_list,reverse=<span class="literal">True</span>)</span><br><span class="line">result = &#123;</span><br><span class="line">    <span class="string">&quot;Q1&quot;</span>:<span class="string">&quot;Hello world&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Q2&quot;</span>:<span class="built_in">sum</span>(read_list),</span><br><span class="line">    <span class="string">&quot;Q3&quot;</span>:read_list[<span class="number">0</span>:<span class="number">10</span>]  </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span> (<span class="string">&quot;result.json&quot;</span>,<span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(result,f)</span><br></pre></td></tr></table></figure>
<h2 id="镜像构建"><a href="#镜像构建" class="headerlink" title="镜像构建"></a>镜像构建</h2><h3 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h3><p>在自己的工作目录下建立对应的Dockerfile，run.sh,和上面的solution.py</p>
<p>其中Dockerfile的内容和官方教程一致：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Base Images</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 从天池基础镜像构建</span></span></span><br><span class="line">FROM registry.cn-shanghai.aliyuncs.com/tcc-public/python:3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 把当前文件夹里的文件构建到镜像的根目录下</span></span></span><br><span class="line">ADD . /</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 指定默认工作目录为根目录（需要把run.sh和生成的结果文件都放在该文件夹下，提交后才能运行）</span></span></span><br><span class="line">WORKDIR /</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 镜像启动后统一执行 sh run.sh</span></span></span><br><span class="line">CMD [&quot;sh&quot;, &quot;run.sh&quot;]</span><br></pre></td></tr></table></figure>
<p>run.sh的内容为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python3 solution.py</span><br></pre></td></tr></table></figure>
<p>然后构建镜像</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker build -t registry.cn-shenzhen.aliyuncs.com/test_for_tianchi/test_for_tianchi_submit:1.0 .</span><br></pre></td></tr></table></figure>
<p>官方构建的镜像名字特别长，所以可以先使用个比较短的名字test，方便后面调试。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker build -t test:1.0 .</span><br></pre></td></tr></table></figure>
<p>从镜像构建一个新容器然后进入容器：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -it test:1.0 bash</span><br></pre></td></tr></table></figure>
<p>其中-it是交互模式，bash是需要执行的命令，表示进入容器的终端。</p>
<h3 id="依赖安装"><a href="#依赖安装" class="headerlink" title="依赖安装"></a>依赖安装</h3><p>在构建镜像的时候如果直接就按照官方的教程不调试的话直接提交会出现很多包未安装的问题，因为构建的镜像是个新的操作系统，从官方的python3基础镜像构建时，基础的镜像是没有安装很多常用的包的，比如上面需要使用到的pandas包。</p>
<p>在进入镜像后会发现，并没有安装vim，直接执行<code>apt-get install vim</code>也会报错找不到包，这时因为并没有更新源，所以需要先执行<code>apt-get update</code>来更新源，然后再安装vim。</p>
<p>之后使用pip安装pandas<code>pip install pandas</code></p>
<p>需要<strong>注意</strong>的是，在容器中安装完对应的包或者对容器的文件执行了修改后，不能直接退出，需要在另外一个终端使用<code>docker ps</code>列出当前容器的id，然后使用<code>docker commit container-id</code>来<strong>保存修改到镜像</strong>。如果没有保存而直接退出，下次使用镜像构建容器的时候依然需要重新安装对应的依赖。</p>
<h3 id="数据挂载"><a href="#数据挂载" class="headerlink" title="数据挂载"></a>数据挂载</h3><p>在本地生成了<code>num_list.csv</code>后，可以将对应的目录挂载到容器上，这样就可以在容器中进行完整的调试，挂载的命令为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -it -v &#x2F;local&#x2F;dir:&#x2F;tcdata image:version bash</span><br></pre></td></tr></table></figure>
<p>冒号前面的是本地的数据文件目录，后面的是容器中的目录。</p>
<h2 id="镜像推送"><a href="#镜像推送" class="headerlink" title="镜像推送"></a>镜像推送</h2><p>在完成本地容器调试后，就可以将镜像推送到仓库中了。</p>
<p>首先必须先进行登陆</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo docker login --username&#x3D;name registry.cn-shenzhen.aliyuncs.com</span><br></pre></td></tr></table></figure>
<p>然后如果前面的镜像名字是test等自定义的则需要重命名镜像</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo docker tag [ImageId] registry.cn-shenzhen.aliyuncs.com&#x2F;yky_docker&#x2F;image_name:[镜像版本号]</span><br></pre></td></tr></table></figure>
<p>最后执行推送命令就可以了：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo docker push registry.cn-shenzhen.aliyuncs.com&#x2F;yky_docker&#x2F;image_name:[镜像版本号]</span><br></pre></td></tr></table></figure>
<p>完成推送后到比赛界面提交对应的镜像即可。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://tianchi.aliyun.com/competition/entrance/231759/tab/174">官方手把手教程</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Pytorch框架的CNN网络实现手写数字（MNIST）识别</title>
    <url>/2020-03-13-%E4%BD%BF%E7%94%A8Pytorch%E6%A1%86%E6%9E%B6%E7%9A%84CNN%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%EF%BC%88MNIST%EF%BC%89%E8%AF%86%E5%88%AB.html</url>
    <content><![CDATA[<h1 id="使用Pytorch框架的CNN网络实现手写数字（MNIST）识别"><a href="#使用Pytorch框架的CNN网络实现手写数字（MNIST）识别" class="headerlink" title="使用Pytorch框架的CNN网络实现手写数字（MNIST）识别"></a>使用Pytorch框架的CNN网络实现手写数字（MNIST）识别</h1><p>本实践使用卷积神经网络（CNN）模型，用于预测手写数字图片。代码源文件在<a href="https://github.com/BraveY/AI-with-code/tree/master/MNIST-detection">github</a>上面</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/ba26eac845334208851e72c7a2dfef5e1eec566894ba430aba7492e72c49cacd" alt=""></p>
<p>首先导入必要的包</p>
<p>numpy—————&gt;python第三方库，用于进行科学计算</p>
<p>PIL——————&gt; Python Image Library,python第三方图像处理库</p>
<p>matplotlib——-&gt;python的绘图库 pyplot:matplotlib的绘图框架</p>
<p>os——————-&gt;提供了丰富的方法来处理文件和目录<br>torchvision——-&gt;提供很多数据集的下载，包括COCO，ImageNet，CIFCAR等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#导入需要的包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms,utils</span><br></pre></td></tr></table></figure>
<h1 id="Step1：准备数据。"><a href="#Step1：准备数据。" class="headerlink" title="Step1：准备数据。"></a><strong>Step1：准备数据。</strong></h1><p>(1)数据集介绍</p>
<p>MNIST数据集包含60000个训练集和10000测试数据集。分为图片和标签，图片是28*28的像素矩阵，标签为0~9共10个数字。</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/fc73217ae57f451a89badc801a903bb742e42eabd9434ecc8089efe19a66c076" alt=""></p>
<p>(2)data_train和data_test<br>root为数据集存放的路径，transform指定数据集导入的时候需要进行的变换，train设置为true表明导入的是训练集合，否则会测试集合。<br>Compose是把多种数据处理的方法集合在一起。使用transforms进行Tensor格式转换和Batch Normalization。</p>
<p>（3）打印看下数据是什么样的？数据已经经过transform的归一化处理</p>
<p>class torchvision.transforms.Normalize(mean, std)：<br>给定均值：(R,G,B) 方差：（R，G，B），将会把Tensor正则化。即：Normalized_image=(image-mean)/std.<br>MINIST是（1,28,28）不是RGB的三维，只有一维的灰度图数据，所以不是[0.5,0.5,0.5],而是[0.5]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform = transforms.Compose([transforms.ToTensor(),</span><br><span class="line">                               transforms.Normalize(mean=[<span class="number">0.5</span>],std=[<span class="number">0.5</span>])])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = datasets.MNIST(root = <span class="string">&quot;./data/&quot;</span>,</span><br><span class="line">                            transform=transform,</span><br><span class="line">                            train = <span class="literal">True</span>,</span><br><span class="line">                            download = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_data = datasets.MNIST(root=<span class="string">&quot;./data/&quot;</span>,</span><br><span class="line">                           transform = transform,</span><br><span class="line">                           train = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>数据下载慢的问题：<a href="https://blog.csdn.net/qq_31904559/article/details/96591484?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task">https://blog.csdn.net/qq_31904559/article/details/96591484?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(test_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">10000</span><br></pre></td></tr></table></figure>
<p>train_data 的个数：60000个训练样本</p>
<p>test_data 的个数：10000个训练样本</p>
<p>一个样本的格式为[data,label]，第一个存放数据，第二个存放标签</p>
<p><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/data/">https://pytorch-cn.readthedocs.io/zh/latest/package_references/data/</a><br>num_workers 表示用多少个子进程加载数据<br>shuffle 表示在装载过程中随机乱序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loader = torch.utils.data.DataLoader(train_data,batch_size=<span class="number">64</span>,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>,num_workers=<span class="number">2</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(test_data,batch_size=<span class="number">64</span>,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>,num_workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>设置batch_size=64后，加载器中的基本单为是一个batch的数据</p>
<p>所以train_loader 的长度是60000/64 = 938 个batch</p>
<p>test_loader 的长度是10000/64= 157个batch</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="built_in">len</span>(train_loader))</span><br><span class="line">print(<span class="built_in">len</span>(test_loader))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">938</span><br><span class="line">157</span><br></pre></td></tr></table></figure>
<p>加载到dataloader中后，一个dataloader是一个batch的数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># data_iter = iter(train_loader)</span></span><br><span class="line"><span class="comment"># print(next(data_iter))</span></span><br></pre></td></tr></table></figure>
<p>从二维数组生成一张图片</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">oneimg,label = train_data[<span class="number">0</span>]</span><br><span class="line">oneimg = oneimg.numpy().transpose(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>) </span><br><span class="line">std = [<span class="number">0.5</span>]</span><br><span class="line">mean = [<span class="number">0.5</span>]</span><br><span class="line">oneimg = oneimg * std + mean</span><br><span class="line">oneimg.resize(<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">plt.imshow(oneimg)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1584031660/blog/deep-learning/MNIST_17_0.png" alt="png"></p>
<p>从三维生成一张黑白图片</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">oneimg,label = train_data[<span class="number">0</span>]</span><br><span class="line">grid = utils.make_grid(oneimg)</span><br><span class="line">grid = grid.numpy().transpose(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>) </span><br><span class="line">std = [<span class="number">0.5</span>]</span><br><span class="line">mean = [<span class="number">0.5</span>]</span><br><span class="line">grid = grid * std + mean</span><br><span class="line">plt.imshow(grid)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1584031660/blog/deep-learning/MNIST_19_0.png" alt="png"></p>
<p>输出一个batch的图片和标签</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">images, lables = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_loader))</span><br><span class="line">img = utils.make_grid(images)</span><br><span class="line"><span class="comment"># transpose 转置函数(x=0,y=1,z=2),新的x是原来的y轴大小，新的y是原来的z轴大小，新的z是原来的x大小</span></span><br><span class="line"><span class="comment">#相当于把x=1这个一道最后面去。</span></span><br><span class="line">img = img.numpy().transpose(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>) </span><br><span class="line">std = [<span class="number">0.5</span>]</span><br><span class="line">mean = [<span class="number">0.5</span>]</span><br><span class="line">img = img * std + mean</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">64</span>):</span><br><span class="line">    print(lables[i], end=<span class="string">&quot; &quot;</span>)</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> i%<span class="number">8</span> <span class="keyword">is</span> <span class="number">0</span>:</span><br><span class="line">        print(end=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor(4) tensor(7) tensor(6) tensor(6) tensor(1) tensor(2) tensor(1) tensor(3) </span><br><span class="line">tensor(2) tensor(9) tensor(6) tensor(2) tensor(5) tensor(0) tensor(7) tensor(1) </span><br><span class="line">tensor(6) tensor(2) tensor(2) tensor(3) tensor(7) tensor(2) tensor(2) tensor(3) </span><br><span class="line">tensor(4) tensor(6) tensor(3) tensor(3) tensor(8) tensor(3) tensor(6) tensor(6) </span><br><span class="line">tensor(7) tensor(4) tensor(3) tensor(0) tensor(2) tensor(1) tensor(2) tensor(0) </span><br><span class="line">tensor(3) tensor(9) tensor(2) tensor(2) tensor(4) tensor(5) tensor(7) tensor(0) </span><br><span class="line">tensor(5) tensor(0) tensor(5) tensor(8) tensor(3) tensor(9) tensor(8) tensor(2) </span><br><span class="line">tensor(7) tensor(5) tensor(8) tensor(2) tensor(6) tensor(8) tensor(9) tensor(1) </span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1584031660/blog/deep-learning/MNIST_21_1.png" alt="png"></p>
<h1 id="Step2-网络配置"><a href="#Step2-网络配置" class="headerlink" title="Step2.网络配置"></a><strong>Step2.网络配置</strong></h1><p>网络结构是两个卷积层，3个全连接层。</p>
<p>Conv2d参数</p>
<ul>
<li>in_channels(int) – 输入信号的通道数目</li>
<li>out_channels(int) – 卷积产生的通道数目</li>
<li>kerner_size(int or tuple) - 卷积核的尺寸</li>
<li>stride(int or tuple, optional) - 卷积步长</li>
<li>padding(int or tuple, optional) - 输入的每一条边补充0的层数</li>
</ul>
<p>1.定义一个CNN网络</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN,self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>,<span class="number">32</span>,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">64</span>*<span class="number">7</span>*<span class="number">7</span>,<span class="number">1024</span>)<span class="comment">#两个池化，所以是7*7而不是14*14</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">1024</span>,<span class="number">512</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">512</span>,<span class="number">10</span>)</span><br><span class="line"><span class="comment">#         self.dp = nn.Dropout(p=0.5)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">             </span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span>* <span class="number">7</span>)<span class="comment">#将数据平整为一维的 </span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line"><span class="comment">#         x = self.fc3(x)</span></span><br><span class="line"><span class="comment">#         self.dp(x)</span></span><br><span class="line">        x = F.relu(self.fc2(x))   </span><br><span class="line">        x = self.fc3(x)  </span><br><span class="line"><span class="comment">#         x = F.log_softmax(x,dim=1) NLLLoss()才需要，交叉熵不需要</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line">net = CNN()        </span><br></pre></td></tr></table></figure>
<p>2.定义损失函数和优化函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment">#也可以选择Adam优化方法</span></span><br><span class="line"><span class="comment"># optimizer = torch.optim.Adam(net.parameters(),lr=1e-2)   </span></span><br></pre></td></tr></table></figure>
<h1 id="Step3-模型训练"><a href="#Step3-模型训练" class="headerlink" title="Step3.模型训练"></a><strong>Step3.模型训练</strong></h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_accs = []</span><br><span class="line">train_loss = []</span><br><span class="line">test_accs = []</span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">net = net.to(device)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i,data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader,<span class="number">0</span>):<span class="comment">#0是下标起始位置默认为0</span></span><br><span class="line">        <span class="comment"># data 的格式[[inputs, labels]]       </span></span><br><span class="line"><span class="comment">#         inputs,labels = data</span></span><br><span class="line">        inputs,labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line">        <span class="comment">#初始为0，清除上个batch的梯度信息</span></span><br><span class="line">        optimizer.zero_grad()         </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#前向+后向+优化     </span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs,labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># loss 的输出，每个一百个batch输出，平均的loss</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">            print(<span class="string">&#x27;[%d,%5d] loss :%.3f&#x27;</span> %</span><br><span class="line">                 (epoch+<span class="number">1</span>,i+<span class="number">1</span>,running_loss/<span class="number">100</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">        train_loss.append(loss.item())</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 训练曲线的绘制 一个batch中的准确率</span></span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total = labels.size(<span class="number">0</span>)<span class="comment"># labels 的长度</span></span><br><span class="line">        correct = (predicted == labels).<span class="built_in">sum</span>().item() <span class="comment"># 预测正确的数目</span></span><br><span class="line">        train_accs.append(<span class="number">100</span>*correct/total)</span><br><span class="line">        </span><br><span class="line">print(<span class="string">&#x27;Finished Training&#x27;</span>)            </span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1,  100] loss :2.292</span><br><span class="line">[1,  200] loss :2.261</span><br><span class="line">[1,  300] loss :2.195</span><br><span class="line">[1,  400] loss :1.984</span><br><span class="line">[1,  500] loss :1.337</span><br><span class="line">[1,  600] loss :0.765</span><br><span class="line">[1,  700] loss :0.520</span><br><span class="line">[1,  800] loss :0.427</span><br><span class="line">[1,  900] loss :0.385</span><br><span class="line">[2,  100] loss :0.339</span><br><span class="line">[2,  200] loss :0.301</span><br><span class="line">[2,  300] loss :0.290</span><br><span class="line">[2,  400] loss :0.260</span><br><span class="line">[2,  500] loss :0.250</span><br><span class="line">[2,  600] loss :0.245</span><br><span class="line">[2,  700] loss :0.226</span><br><span class="line">[2,  800] loss :0.218</span><br><span class="line">[2,  900] loss :0.206</span><br><span class="line">[3,  100] loss :0.183</span><br><span class="line">[3,  200] loss :0.176</span><br><span class="line">[3,  300] loss :0.174</span><br><span class="line">[3,  400] loss :0.156</span><br><span class="line">[3,  500] loss :0.160</span><br><span class="line">[3,  600] loss :0.147</span><br><span class="line">[3,  700] loss :0.146</span><br><span class="line">[3,  800] loss :0.130</span><br><span class="line">[3,  900] loss :0.115</span><br><span class="line">Finished Training</span><br></pre></td></tr></table></figure>
<p>模型的保存</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PATH = <span class="string">&#x27;./mnist_net.pth&#x27;</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br></pre></td></tr></table></figure>
<h1 id="Step4-模型评估"><a href="#Step4-模型评估" class="headerlink" title="Step4.模型评估"></a><strong>Step4.模型评估</strong></h1><p>画图</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_train_process</span>(<span class="params">title,iters,costs,accs,label_cost,lable_acc</span>):</span></span><br><span class="line">    plt.title(title, fontsize=<span class="number">24</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;iter&quot;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;acc(\%)&quot;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.plot(iters, costs,color=<span class="string">&#x27;red&#x27;</span>,label=label_cost) </span><br><span class="line">    plt.plot(iters, accs,color=<span class="string">&#x27;green&#x27;</span>,label=lable_acc) </span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_iters = <span class="built_in">range</span>(<span class="built_in">len</span>(train_accs))</span><br><span class="line">draw_train_process(<span class="string">&#x27;training&#x27;</span>,train_iters,train_loss,train_accs,<span class="string">&#x27;training loss&#x27;</span>,<span class="string">&#x27;training acc&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1584031660/blog/deep-learning/MNIST_35_0.png" alt="png"></p>
<p>检验一个batch的分类情况</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataiter = <span class="built_in">iter</span>(test_loader)</span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print images</span></span><br><span class="line">test_img = utils.make_grid(images)</span><br><span class="line">test_img = test_img.numpy().transpose(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)</span><br><span class="line">std = [<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>]</span><br><span class="line">mean =  [<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>]</span><br><span class="line">test_img = test_img*std+<span class="number">0.5</span></span><br><span class="line">plt.imshow(test_img)</span><br><span class="line">plt.show()</span><br><span class="line">print(<span class="string">&#x27;GroundTruth: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%d&#x27;</span> % labels[j] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">64</span>)))</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1584031660/blog/deep-learning/MNIST_37_0.png" alt="png"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GroundTruth:  0 6 4 8 3 8 4 8 0 0 6 3 9 8 2 3 4 4 6 0 5 7 6 3 1 6 6 3 9 4 7 5 0 2 5 0 0 8 8 9 3 0 8 2 4 1 2 1 0 6 5 5 7 3 9 5 1 5 7 6 4 2 7 7</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_net = CNN()</span><br><span class="line">test_net.load_state_dict(torch.load(PATH))</span><br><span class="line">test_out = test_net(images)</span><br></pre></td></tr></table></figure>
<p>输出的是每一类的对应概率，所以需要选择max来确定最终输出的类别<br>dim=1 表示选择行的最大索引</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_, predicted = torch.<span class="built_in">max</span>(test_out, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Predicted: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%d&#x27;</span> % predicted[j]</span><br><span class="line">                              <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">64</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Predicted:  0 6 4 8 3 8 4 8 0 0 6 3 9 8 2 3 4 4 6 0 6 7 6 3 1 6 6 3 9 6 7 5 0 2 5 0 0 2 8 9 3 0 8 2 4 1 2 1 0 6 5 5 9 3 9 5 1 5 7 6 4 2 7 7</span><br></pre></td></tr></table></figure>
<p>测试集上面整体的准确率</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():<span class="comment"># 进行评测的时候网络不更新梯度</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = test_net(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)<span class="comment"># labels 的长度</span></span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item() <span class="comment"># 预测正确的数目</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Accuracy of the network on the  test images: %d %%&#x27;</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Accuracy of the network on the  test images: 96 %</span><br></pre></td></tr></table></figure>
<p>10个类别的准确率</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">class_correct = <span class="built_in">list</span>(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line">class_total = <span class="built_in">list</span>(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = test_net(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels)</span><br><span class="line"><span class="comment">#         print(predicted == labels)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">&#x27;Accuracy of %d : %2d %%&#x27;</span> % (</span><br><span class="line">        i, <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Accuracy of 0 : 99 %</span><br><span class="line">Accuracy of 1 : 98 %</span><br><span class="line">Accuracy of 2 : 96 %</span><br><span class="line">Accuracy of 3 : 91 %</span><br><span class="line">Accuracy of 4 : 97 %</span><br><span class="line">Accuracy of 5 : 95 %</span><br><span class="line">Accuracy of 6 : 96 %</span><br><span class="line">Accuracy of 7 : 93 %</span><br><span class="line">Accuracy of 8 : 94 %</span><br><span class="line">Accuracy of 9 : 92 %</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://blog.csdn.net/weixin_45885232/article/details/103950992">https://blog.csdn.net/weixin_45885232/article/details/103950992</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/30249139">https://zhuanlan.zhihu.com/p/30249139</a></p>
<p><a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py">https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>MNIST</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>使用SVM对鸢尾花分类</title>
    <url>/2020-03-05-%E4%BD%BF%E7%94%A8SVM%E5%AF%B9%E9%B8%A2%E5%B0%BE%E8%8A%B1%E5%88%86%E7%B1%BB.html</url>
    <content><![CDATA[<h1 id="使用SVM对鸢尾花分类"><a href="#使用SVM对鸢尾花分类" class="headerlink" title="使用SVM对鸢尾花分类"></a>使用SVM对鸢尾花分类</h1><p>百度AI Studio中的一个入门项目，增加了自己在实践时的一些注释，对小白来说阅读更顺畅。源码和数据在<a href="https://github.com/BraveY/AI-with-code/tree/master/iris-classification">github</a>上。</p>
<h2 id="任务描述："><a href="#任务描述：" class="headerlink" title="任务描述："></a><strong>任务描述：</strong></h2><p>构建一个模型，根据鸢尾花的花萼和花瓣大小将其分为三种不同的品种。</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/dd74666475b549fcae99ac2aff67488f015cdd76569d4d208909983bcf40fe3c" alt=""></p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a><strong>数据集</strong></h2><p>总共包含150行数据</p>
<p>每一行数据由 4 个特征值及一个目标值组成。</p>
<p>4 个特征值分别为：萼片长度、萼片宽度、花瓣长度、花瓣宽度</p>
<p>目标值为三种不同类别的鸢尾花，分别为：    Iris Setosa、Iris Versicolour、Iris Virginica</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/8bdc417331ef45d5a380d2769f3a8bcd7b361212b20d4e78b2a32ee9c7a7b1fa" alt=""></p>
<p><strong>首先导入必要的包：</strong></p>
<p><strong>numpy</strong>：python第三方库，用于科学计算</p>
<p><strong>matplotlib</strong>:python第三方库，主要用于进行可视化</p>
<p><strong>sklearn</strong>:python的重要机器学习库，其中封装了大量的机器学习算法，如：分类、回归、降维以及聚类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np                </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> colors     </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm            </span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br></pre></td></tr></table></figure>
<h2 id="Step1-数据准备"><a href="#Step1-数据准备" class="headerlink" title="Step1.数据准备"></a><strong>Step1.数据准备</strong></h2><p>(1)从指定路径下加载数据</p>
<p>(2)对加载的数据进行数据分割，x_train,x_test,y_train,y_test分别表示训练集特征、训练集标签、测试集特征、测试集标签</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#*************将字符串转为整型，便于数据加载***********************</span></span><br><span class="line"><span class="comment">#在函数中建立一个对应字典就可以了，输入字符串，输出字符串对应的数字。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iris_type</span>(<span class="params">s</span>):</span></span><br><span class="line"><span class="comment">#     print(type(s))</span></span><br><span class="line"><span class="comment">#字符串加个b是指btypes 字节串类型</span></span><br><span class="line">    it = &#123;<span class="string">b&#x27;Iris-setosa&#x27;</span>:<span class="number">0</span>, <span class="string">b&#x27;Iris-versicolor&#x27;</span>:<span class="number">1</span>, <span class="string">b&#x27;Iris-virginica&#x27;</span>:<span class="number">2</span>&#125;</span><br><span class="line">    <span class="keyword">return</span> it[s]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加载数据</span></span><br><span class="line">data_path=<span class="string">&#x27;./iris.data&#x27;</span>          <span class="comment">#数据文件的路径</span></span><br><span class="line">data = np.loadtxt(data_path,                                <span class="comment">#数据文件路径</span></span><br><span class="line">                  dtype=<span class="built_in">float</span>,                              <span class="comment">#数据类型</span></span><br><span class="line">                  delimiter=<span class="string">&#x27;,&#x27;</span>,                            <span class="comment">#数据分隔符</span></span><br><span class="line">                  converters=&#123;<span class="number">4</span>:iris_type&#125;)                 <span class="comment">#将第5列使用函数iris_type进行转换</span></span><br><span class="line"><span class="comment"># print(data)                                                 #data为二维数组，data.shape=(150, 5)</span></span><br><span class="line"><span class="comment"># print(data.shape)</span></span><br><span class="line"><span class="comment">#数据分割</span></span><br><span class="line">x, y = np.split(data,                                       <span class="comment">#要切分的数组</span></span><br><span class="line">                (<span class="number">4</span>,),                                       <span class="comment">#沿轴切分的位置，第5列开始往后为y</span></span><br><span class="line">                axis=<span class="number">1</span>)                                     <span class="comment">#1代表纵向分割，按列分割</span></span><br><span class="line"></span><br><span class="line">x = x[:, <span class="number">0</span>:<span class="number">2</span>] </span><br><span class="line"><span class="comment">#第一个逗号之前表示行，只有冒号表示所有行，第二个冒号0:2表是0,1两列</span></span><br><span class="line"><span class="comment">#在X中我们取前两列作为特征，为了后面的可视化，原始的四维不好画图。x[:,0:4]代表第一维(行)全取，第二维(列)取0~2</span></span><br><span class="line"><span class="comment"># print(x)</span></span><br><span class="line">x_train,x_test,y_train,y_test=model_selection.train_test_split(x,              <span class="comment">#所要划分的样本特征集</span></span><br><span class="line">                                                               y,              <span class="comment">#所要划分的样本结果</span></span><br><span class="line">                                                               random_state=<span class="number">1</span>, <span class="comment">#随机数种子确保产生的随机数组相同</span></span><br><span class="line">                                                               test_size=<span class="number">0.3</span>)  <span class="comment">#测试样本占比</span></span><br></pre></td></tr></table></figure>
<p>random_state=1确保了每次运行程序时用的随机数都是一样的，也就是每次重新运行后所划分的训练集和测试集的样本都是一致的，相当于只在第一次运行的时候进行随机划分。如果不设置的话，每次重新运行的种子不一样，产生的随机数也不一样就会导致每次随机生成的训练集和测试集不一致。</p>
<h2 id="Step2-模型搭建"><a href="#Step2-模型搭建" class="headerlink" title="Step2.模型搭建"></a><strong>Step2.模型搭建</strong></h2><p>C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样对训练集测试时准确率很高，但泛化能力弱。<br>C值小，对误分类的惩罚减小，允许容错，将他们当成噪声点，泛化能力较强。</p>
<p>kernel=’linear’时，为线性核</p>
<p>decision_function_shape=’ovr’时，为one v rest，即一个类别与其他类别进行划分，</p>
<p>decision_function_shape=’ovo’时，为one v one，即将类别两两之间进行划分，用二分类的方法模拟多分类的结果。<br>ovr是多类情况1和ovo是多类情况2，可以在<a href="https://bravey.github.io/2019-12-08-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0.html">我个人博客-线性判别函数</a> 上查看详细说明。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#**********************SVM分类器构建*************************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifier</span>():</span></span><br><span class="line">    <span class="comment">#clf = svm.SVC(C=0.8,kernel=&#x27;rbf&#x27;, gamma=50,decision_function_shape=&#x27;ovr&#x27;)</span></span><br><span class="line">    clf = svm.SVC(C=<span class="number">0.5</span>,                         <span class="comment">#误差项惩罚系数,默认值是1</span></span><br><span class="line">                  kernel=<span class="string">&#x27;linear&#x27;</span>,               <span class="comment">#线性核 kenrel=&quot;rbf&quot;:高斯核</span></span><br><span class="line">                  decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>) <span class="comment">#决策函数</span></span><br><span class="line">    <span class="keyword">return</span> clf</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.定义模型：SVM模型定义</span></span><br><span class="line">clf = classifier()</span><br></pre></td></tr></table></figure>
<h2 id="Step3-模型训练"><a href="#Step3-模型训练" class="headerlink" title="Step3.模型训练"></a><strong>Step3.模型训练</strong></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_train.ravel()<span class="comment">#ravel()扁平化，将原来的二维数组转换为一维数组</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">array([2., 0., 0., 0., 1., 0., 0., 2., 2., 2., 2., 2., 1., 2., 1., 0., 2.,</span><br><span class="line">       2., 0., 0., 2., 0., 2., 2., 1., 1., 2., 2., 0., 1., 1., 2., 1., 2.,</span><br><span class="line">       1., 0., 0., 0., 2., 0., 1., 2., 2., 0., 0., 1., 0., 2., 1., 2., 2.,</span><br><span class="line">       1., 2., 2., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 2., 2., 2., 0.,</span><br><span class="line">       0., 1., 0., 2., 0., 2., 2., 0., 2., 0., 1., 0., 1., 1., 0., 0., 1.,</span><br><span class="line">       0., 1., 1., 0., 1., 1., 1., 1., 2., 0., 0., 2., 1., 2., 1., 2., 2.,</span><br><span class="line">       1., 2., 0.])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#***********************训练模型*****************************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">clf,x_train,y_train</span>):</span></span><br><span class="line">    clf.fit(x_train,         <span class="comment">#训练集特征向量，fit表示输入数据开始拟合</span></span><br><span class="line">            y_train.ravel()) <span class="comment">#训练集目标值 ravel()扁平化，将原来的二维数组转换为一维数组</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练SVM模型</span></span><br><span class="line">train(clf,x_train,y_train)</span><br></pre></td></tr></table></figure>
<h2 id="Step4-模型评估"><a href="#Step4-模型评估" class="headerlink" title="Step4.模型评估"></a><strong>Step4.模型评估</strong></h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#**************并判断a b是否相等，计算acc的均值*************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_accuracy</span>(<span class="params">a, b, tip</span>):</span></span><br><span class="line">    acc = a.ravel() == b.ravel()</span><br><span class="line">    print(<span class="string">&#x27;%s Accuracy:%.3f&#x27;</span> %(tip, np.mean(acc)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_accuracy</span>(<span class="params">clf,x_train,y_train,x_test,y_test</span>):</span></span><br><span class="line">    <span class="comment">#分别打印训练集和测试集的准确率  score(x_train,y_train):表示输出x_train,y_train在模型上的准确率</span></span><br><span class="line">    print(<span class="string">&#x27;trianing prediction:%.3f&#x27;</span> %(clf.score(x_train, y_train)))</span><br><span class="line">    print(<span class="string">&#x27;test data prediction:%.3f&#x27;</span> %(clf.score(x_test, y_test)))</span><br><span class="line">    <span class="comment">#原始结果与预测结果进行对比   predict()表示对x_train样本进行预测，返回样本类别</span></span><br><span class="line">    show_accuracy(clf.predict(x_train), y_train, <span class="string">&#x27;traing data&#x27;</span>)</span><br><span class="line">    show_accuracy(clf.predict(x_test), y_test, <span class="string">&#x27;testing data&#x27;</span>)</span><br><span class="line">    <span class="comment">#计算决策函数的值，表示x到各分割平面的距离,3类，所以有3个决策函数，不同的多类情况有不同的决策函数？</span></span><br><span class="line">    print(<span class="string">&#x27;decision_function:\n&#x27;</span>, clf.decision_function(x_train))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4.模型评估</span></span><br><span class="line">print_accuracy(clf,x_train,y_train,x_test,y_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">trianing prediction:0.819</span><br><span class="line">test data prediction:0.778</span><br><span class="line">traing data Accuracy:0.819</span><br><span class="line">testing data Accuracy:0.778</span><br><span class="line">decision_function:</span><br><span class="line"> [[-0.5         1.20887337  2.29112663]</span><br><span class="line"> [ 2.06328814 -0.0769677   1.01367956]</span><br><span class="line"> [ 2.16674973  0.91702835 -0.08377808]</span><br><span class="line"> [ 2.11427813  0.99765248 -0.11193061]</span><br><span class="line"> [ 0.9925538   2.06392138 -0.05647518]</span><br><span class="line"> [ 2.11742969  0.95255534 -0.06998503]</span><br><span class="line"> [ 2.05615004 -0.041847    0.98569697]</span><br><span class="line"> [-0.31866596  1.02685964  2.29180632]</span><br><span class="line"> [-0.27166251  1.09150338  2.18015913]</span><br><span class="line"> [-0.37827567  1.14260447  2.2356712 ]</span><br><span class="line"> [-0.22150749  1.11104997  2.11045752]</span><br><span class="line"> [-0.18331208  2.10066724  1.08264485]</span><br><span class="line"> [-0.05444966  0.99927764  2.05517201]</span><br><span class="line"> [-0.46977766  1.17853774  2.29123992]</span><br><span class="line"> [-0.05760122  2.04437478  1.01322644]</span><br><span class="line"> [ 2.1747228   0.93698124 -0.11170404]</span><br><span class="line"> [-0.13315707  2.12021384  1.01294323]</span><br><span class="line"> [-0.21752096  2.12102642  1.09649454]</span><br><span class="line"> [ 2.11427813  0.99765248 -0.11193061]</span><br><span class="line"> [ 2.16359817  0.96212549 -0.12572366]</span><br><span class="line"> [-0.21038286  1.08590572  2.12447714]</span><br><span class="line"> [ 2.21291822  0.9265985  -0.13951672]</span><br><span class="line"> [-0.13399204  1.06514025  2.06885179]</span><br><span class="line"> [-0.18016052  1.0555701   2.12459042]</span><br><span class="line"> [-0.2334671   1.08112064  2.15234646]</span><br><span class="line"> [-0.08782356  2.0747104   1.01311315]</span><br><span class="line"> [-0.20324476  1.05078502  2.15245974]</span><br><span class="line"> [-0.11489433  1.05994888  2.05494545]</span><br><span class="line"> [ 2.17787437 -0.1081159   0.93024154]</span><br><span class="line"> [-0.23578369  2.18129137  1.05449232]</span><br><span class="line"> [-0.20639632  1.09588216  2.11051416]</span><br><span class="line"> [-0.21038286  1.08590572  2.12447714]</span><br><span class="line"> [-0.02969547  2.11420989  0.91548558]</span><br><span class="line"> [-0.12685394  1.03001955  2.09683439]</span><br><span class="line"> [-0.09496166  2.1098311   0.98513056]</span><br><span class="line"> [ 2.10547008 -0.07737399  0.97190391]</span><br><span class="line"> [ 2.11029159  0.98767604 -0.09796763]</span><br><span class="line"> [ 2.20411017 -0.14842797  0.9443178 ]</span><br><span class="line"> [-0.20324476  1.05078502  2.15245974]</span><br><span class="line"> [ 2.19066895  0.97688701 -0.16755596]</span><br><span class="line"> [-0.16022784  2.10545232  1.05477553]</span><br><span class="line"> [-0.23661866  1.12621778  2.11040088]</span><br><span class="line"> [-0.09579663  2.05475752  1.04103911]</span><br><span class="line"> [ 2.11344315 -0.05742111  0.94397795]</span><br><span class="line"> [ 2.10231852  0.96772315 -0.07004167]</span><br><span class="line"> [-0.12203243  2.09506958  1.02696285]</span><br><span class="line"> [ 2.11029159  0.98767604 -0.09796763]</span><br><span class="line"> [-0.41248455  1.16296364  2.2495209 ]</span><br><span class="line"> [-0.16820091  1.08549943  2.08270149]</span><br><span class="line"> [-0.42045762  1.14301076  2.27744686]</span><br><span class="line"> [-0.24857827  1.09628845  2.15228982]</span><br><span class="line"> [-0.27796564  2.18169766  1.09626798]</span><br><span class="line"> [-0.09264507  1.00966038  2.08298469]</span><br><span class="line"> [-0.25339978  1.03123843  2.22216135]</span><br><span class="line"> [-0.05361468  2.05435123  0.99926346]</span><br><span class="line"> [ 2.15395516 -0.16797456  1.01401941]</span><br><span class="line"> [-0.12203243  2.09506958  1.02696285]</span><br><span class="line"> [ 2.06579305  1.08825305 -0.15404611]</span><br><span class="line"> [-0.11007283  2.12499891  0.98507392]</span><br><span class="line"> [-0.27166251  1.09150338  2.18015913]</span><br><span class="line"> [ 2.13652739  0.94736397 -0.08389137]</span><br><span class="line"> [-0.29789831  1.13181544  2.16608287]</span><br><span class="line"> [ 2.15163856  0.93219616 -0.08383473]</span><br><span class="line"> [ 2.1747228   0.93698124 -0.11170404]</span><br><span class="line"> [-0.11174277  1.01485174  2.09689103]</span><br><span class="line"> [-0.06872585  2.06951904  0.99920682]</span><br><span class="line"> [-0.23745364  1.0711442   2.16630944]</span><br><span class="line"> [ 2.12141623  0.96253178 -0.08394801]</span><br><span class="line"> [ 2.1627632  -0.09294809  0.93018489]</span><br><span class="line"> [-0.06557429  1.0244219   2.04115239]</span><br><span class="line"> [ 2.16758471  0.97210193 -0.13968664]</span><br><span class="line"> [-0.12203243  2.09506958  1.02696285]</span><br><span class="line"> [ 2.1293893   0.98248467 -0.11187396]</span><br><span class="line"> [-0.21038286  1.08590572  2.12447714]</span><br><span class="line"> [ 2.01962457  1.0786829  -0.09830747]</span><br><span class="line"> [ 2.18269588  0.95693412 -0.13963   ]</span><br><span class="line"> [-0.16106282  1.05037873  2.11068408]</span><br><span class="line"> [ 2.20976665  0.97169564 -0.1814623 ]</span><br><span class="line"> [-0.03850351  2.03918342  0.9993201 ]</span><br><span class="line"> [ 2.17555778  0.99205482 -0.1676126 ]</span><br><span class="line"> [-0.11007283  2.12499891  0.98507392]</span><br><span class="line"> [-0.07502898  2.15971332  0.91531566]</span><br><span class="line"> [ 2.13254086  0.93738753 -0.06992839]</span><br><span class="line"> [ 2.09518042  1.00284385 -0.09802427]</span><br><span class="line"> [ 1.0045134   2.09385071 -0.09836411]</span><br><span class="line"> [ 2.24314055  0.89626288 -0.13940344]</span><br><span class="line"> [-0.09579663  2.05475752  1.04103911]</span><br><span class="line"> [-0.14910321  1.08030806  2.06879515]</span><br><span class="line"> [ 2.13652739  0.94736397 -0.08389137]</span><br><span class="line"> [-0.2334671   1.08112064  2.15234646]</span><br><span class="line"> [-0.07271239  2.05954259  1.0131698 ]</span><br><span class="line"> [-0.2739791   2.1916741   1.082305  ]</span><br><span class="line"> [-0.27564905  1.08152693  2.19412211]</span><br><span class="line"> [-0.12203243  2.09506958  1.02696285]</span><br><span class="line"> [ 2.06013657 -0.03187056  0.97173399]</span><br><span class="line"> [ 2.07608272  1.00803521 -0.08411793]</span><br><span class="line"> [-0.19443672  2.12581149  1.06862523]</span><br><span class="line"> [-0.16421438  2.09547587  1.06873851]</span><br><span class="line"> [-0.3440668   1.12224529  2.22182151]</span><br><span class="line"> [-0.1180459   2.10504603  1.01299987]</span><br><span class="line"> [-0.20240979  1.10585861  2.09655118]</span><br><span class="line"> [-0.17617399  1.06554654  2.11062744]</span><br><span class="line"> [-0.2477433   2.15136204  1.09638126]</span><br><span class="line"> [-0.2334671   1.08112064  2.15234646]</span><br><span class="line"> [ 2.11029159  0.98767604 -0.09796763]]</span><br></pre></td></tr></table></figure>
<h2 id="Step5-模型使用"><a href="#Step5-模型使用" class="headerlink" title="Step5.模型使用"></a><strong>Step5.模型使用</strong></h2><p><code>np.mgrid</code>的作用是用前两个特征生成其对应最大最小范围所能组合出的所有200*200的样本，也就是遍历了这两个特征所能组合出的所有可能性，只是粒度是1/200</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span>(<span class="params">clf, x</span>):</span></span><br><span class="line">    iris_feature = <span class="string">&#x27;sepal length&#x27;</span>, <span class="string">&#x27;sepal width&#x27;</span>, <span class="string">&#x27;petal lenght&#x27;</span>, <span class="string">&#x27;petal width&#x27;</span></span><br><span class="line">    <span class="comment"># 开始画图</span></span><br><span class="line">    x1_min, x1_max = x[:, <span class="number">0</span>].<span class="built_in">min</span>(), x[:, <span class="number">0</span>].<span class="built_in">max</span>()               <span class="comment">#第0列的范围</span></span><br><span class="line">    x2_min, x2_max = x[:, <span class="number">1</span>].<span class="built_in">min</span>(), x[:, <span class="number">1</span>].<span class="built_in">max</span>()               <span class="comment">#第1列的范围</span></span><br><span class="line">    x1, x2 = np.mgrid[x1_min:x1_max:<span class="number">200j</span>, x2_min:x2_max:<span class="number">200j</span>]   <span class="comment">#生成网格采样点 开始坐标：结束坐标（不包括）：步长</span></span><br><span class="line">    <span class="comment">#flat将二维数组转换成1个1维的迭代器，然后把x1和x2的所有可能值给匹配成为样本点</span></span><br><span class="line">    grid_test = np.stack((x1.flat, x2.flat), axis=<span class="number">1</span>)            <span class="comment">#stack():沿着新的轴加入一系列数组，竖着（按列）增加两个数组，grid_test的shape：(40000, 2)</span></span><br><span class="line">    print(<span class="string">&#x27;grid_test:\n&#x27;</span>, grid_test)</span><br><span class="line">    <span class="comment"># 输出样本到决策面的距离</span></span><br><span class="line">    z = clf.decision_function(grid_test)</span><br><span class="line">    print(<span class="string">&#x27;the distance to decision plane:\n&#x27;</span>, z)</span><br><span class="line">    </span><br><span class="line">    grid_hat = clf.predict(grid_test)                           <span class="comment"># 预测分类值 得到【0,0.。。。2,2,2】</span></span><br><span class="line">    print(<span class="string">&#x27;grid_hat:\n&#x27;</span>, grid_hat)  </span><br><span class="line">    grid_hat = grid_hat.reshape(x1.shape)                       <span class="comment"># reshape grid_hat和x1形状一致</span></span><br><span class="line">                                                                <span class="comment">#若3*3矩阵e，则e.shape()为3*3,表示3行3列   </span></span><br><span class="line"> 	<span class="comment">#light是网格测试点的配色，相当于背景</span></span><br><span class="line">    <span class="comment">#dark是样本点的配色</span></span><br><span class="line">    cm_light = mpl.colors.ListedColormap([<span class="string">&#x27;#A0FFA0&#x27;</span>, <span class="string">&#x27;#FFA0A0&#x27;</span>, <span class="string">&#x27;#A0A0FF&#x27;</span>])</span><br><span class="line">    cm_dark = mpl.colors.ListedColormap([<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;r&#x27;</span>])</span><br><span class="line">     <span class="comment">#画出所有网格样本点被判断为的分类，作为背景</span></span><br><span class="line">    plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light)                                   <span class="comment"># pcolormesh(x,y,z,cmap)这里参数代入</span></span><br><span class="line">                                                                                      <span class="comment"># x1，x2，grid_hat，cmap=cm_light绘制的是背景。</span></span><br><span class="line">    <span class="comment">#squeeze()把y的个数为1的维度去掉，也就是变成一维。</span></span><br><span class="line">    plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=np.squeeze(y), edgecolor=<span class="string">&#x27;k&#x27;</span>, s=<span class="number">50</span>, cmap=cm_dark) <span class="comment"># 样本点</span></span><br><span class="line">    plt.scatter(x_test[:, <span class="number">0</span>], x_test[:, <span class="number">1</span>], s=<span class="number">200</span>, facecolor=<span class="string">&#x27;yellow&#x27;</span>, zorder=<span class="number">10</span>, marker=<span class="string">&#x27;+&#x27;</span>)       <span class="comment"># 测试点</span></span><br><span class="line">    plt.xlabel(iris_feature[<span class="number">0</span>], fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.ylabel(iris_feature[<span class="number">1</span>], fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.xlim(x1_min, x1_max)</span><br><span class="line">    plt.ylim(x2_min, x2_max)</span><br><span class="line">    plt.title(<span class="string">&#x27;svm in iris data classification&#x27;</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5.模型使用</span></span><br><span class="line">draw(clf,x)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grid_test:</span><br><span class="line"> [[4.3       2.       ]</span><br><span class="line"> [4.3       2.0120603]</span><br><span class="line"> [4.3       2.0241206]</span><br><span class="line"> ...</span><br><span class="line"> [7.9       4.3758794]</span><br><span class="line"> [7.9       4.3879397]</span><br><span class="line"> [7.9       4.4      ]]</span><br><span class="line">the distance to decision plane:</span><br><span class="line"> [[ 2.04663576  1.0980928  -0.14472856]</span><br><span class="line"> [ 2.04808477  1.09663836 -0.14472313]</span><br><span class="line"> [ 2.04953377  1.09518392 -0.1447177 ]</span><br><span class="line"> ...</span><br><span class="line"> [-0.21454554  0.96016146  2.25438408]</span><br><span class="line"> [-0.21309653  0.95870702  2.25438951]</span><br><span class="line"> [-0.21164753  0.95725258  2.25439495]]</span><br><span class="line">grid_hat:</span><br><span class="line"> [0. 0. 0. ... 2. 2. 2.]</span><br></pre></td></tr></table></figure>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1583399979/blog/machine-learning/output_22_1.png" alt="png"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>项目来自<a href="https://aistudio.baidu.com/aistudio/projectdetail/78918">课程2-机器学习入门实践-鸢尾花分类</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>分类</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>ICPP 2018 ImageNet Training in Minutes</title>
    <url>/2020-02-08-ICPP-2018-ImageNet-Training-in-Minutes.html</url>
    <content><![CDATA[<h2 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h2><p> <a href="https://dl.acm.org/doi/abs/10.1145/3225058.3225069">ICPP2018 best paper</a></p>
<h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><p>分布式机器学习，神经网络加速</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>文章对大规模计算机的DNN加速能力进行了研究。通过使用LARS算法提供的Large batch来充分利用海量的计算资源，该方法在ImageNet-1k数据集上训练出的AlexNet和ResNet-50这两个网络达到了SOTA的精读。和Facebook以往提出的Baseline相比，该方法在batch size超过16k上有着更高的精度。使用2,048 Intel Xeon Platinum 8160处理器将100 epoch的AlexNext的训练时间从几小时下降到了11分钟，使用2,048 Intel Xeon Phi 7250处理器将90 epoch的训练时间从几小时下降到了20分钟。</p>
<a id="more"></a>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>大意同摘要，文中讨论了大规模计算机来加速的优点与困难。</p>
<hr>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>问题1：对于深度学习应用，更大的数据集和更复杂的模型带来了准确率的显著提高，随之而来的是<strong>更长的训练时间。</strong></p>
<p>思路1：如果把神经网络的训练使用超级计算机来实现，短时间完成。</p>
<p>同步SGD算法在目前取得了最好的效果，将batch size提高可以更好的利用机器。每个处理器处理更多的样本。增加处理器与batch size 来减少训练时间，但是large batch会带来测试准确率的下降。使用warm up和linear scaling rule 方法来扩大batch size 并减少训练时间。使用Layer-wise Adaptive Rate Scaling (LARS)可以扩大到32k。</p>
<p>本文的出发点：使用LARS算法可以在ImageNet-1k上面将DNN的batch size扩大到多少？</p>
<p>本文贡献：</p>
<ul>
<li>使用LARS，扩展到上千个CPU上训练DNN网络。</li>
<li>检验了LARS在AlexNet和ResNet-50上的通用性。</li>
<li>LARS在32k的batch size上展现了更好的鲁棒性</li>
<li>工作已经开源</li>
</ul>
<p>其他相关工作：数据并行的SGD和模型并行的两种分布式并行DNN训练方法</p>
<hr>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>关注数据平行的随机梯度下降方法SGD。因此通信开销与模型的复杂度（参数的数量）成比例。</p>
<p>大批量large batch 不改变总的计算量，但是会减少通信的开销。</p>
<p>模型选择了AlexNet and ResNet-50作为代表，因为二者有不同scaling ratio，ResNet-50更高的scaling ratio因此比AlexNet更容易扩大batch size</p>
<p>目标：在测试准确率不降低的前提下来增大batch size</p>
<p>保持准确率的现有的方法：</p>
<ul>
<li>Linear Scaling：学习率同batch size一样线性增长</li>
<li>Warmup scheme：在前几代中从小学习率逐渐增长到大学习率</li>
</ul>
<p>本文使用的方法LARS+warmup算法：不同层需要使用不同的学习率，标准的SGD是所有层都一样的。调整的方式就是LARS：</p>
<script type="math/tex; mode=display">
\eta=l\times\gamma\times\frac{||w||_2}{||\nabla w||_2}</script><p>$l$是scaling 因子，$\gamma$是可调参数。</p>
<p>LARS方法流程：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1581581809/blog/paper/LARS1.jpg" alt=""></p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1581581804/blog/paper/lars2.jpg" alt=""></p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>使用LARS方法与warmup在AlexNet上的测试精度</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1581581802/blog/paper/result1.jpg" alt=""></p>
<p>ResNet50 在ImageNet上面才用不同16k和32kbatch size 的结果。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1581581802/blog/paper/figure3.jpg" alt=""></p>
<p>不同batch size 的精度比较：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1581581845/blog/paper/figure4.jpg" alt=""></p>
<p>使用large batch可以减少训练时间与迭代次数：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1581581802/blog/paper/fig78.jpg" alt=""></p>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><p>Large batch 的优点：</p>
<ul>
<li>缩短训练时间。总得计算时间未变，但是通信时间会减少。</li>
<li>保持机器的高使用率</li>
</ul>
<p>将DNN扩展到多机的主要开销：多机之间的通信，Resnet50的scaling ratio是AlexNet的12.5倍，AlexNet要简单一些。</p>
<p>Large batch的挑战</p>
<ul>
<li><p>简单使用同步SGD和Large batch会使得测试准确率下降。</p>
<p>矫正准确率降低的方法</p>
<ul>
<li>Linear Scaling：学习率同batch size一样线性增长</li>
<li>Warmup scheme：在前几代中从小学习率逐渐增长到大学习率</li>
</ul>
</li>
</ul>
<hr>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ol>
<li>可否验证LARS方法在其他数据集与其他网络上的性能？从而得到更全面的结果</li>
</ol>
]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>分布式机器学习</tag>
        <tag>神经网络加速</tag>
      </tags>
  </entry>
  <entry>
    <title>C/C++拾遗</title>
    <url>/2020-02-05-c++%E6%8B%BE%E9%81%97.html</url>
    <content><![CDATA[<h1 id="C-C-拾遗"><a href="#C-C-拾遗" class="headerlink" title="C/C++拾遗"></a>C/C++拾遗</h1><h2 id="虚函数和纯虚函数"><a href="#虚函数和纯虚函数" class="headerlink" title="虚函数和纯虚函数"></a>虚函数和纯虚函数</h2><p>虚函数：virtual void fun (){………..};定义后了可以实现，并且主要是为了子类会对这个函数进行重写，从而实现多态性。声明一个基类指针对象，但指向子类实例对象，这样如果基类是虚函数，则可以根据指向的子类的不同而实现不同的方法。不使用虚函数的话，将无法使用子类重写过的函数。</p>
<p>纯虚函数：virtual +函数+ =0 。只声明，不会在基类完成定义，需要在子类中定义实现的方法。</p>
<a id="more"></a>
<hr>
<h2 id="重写和重载"><a href="#重写和重载" class="headerlink" title="重写和重载"></a>重写和重载</h2><p><strong>重载</strong>：</p>
<ul>
<li>相同的范围（同一个作用域中）比如同一个类中</li>
<li>函数的名字相同</li>
<li>函数的参数不同</li>
<li>virtual可有可无</li>
<li>返回值可以不同（不能仅只有返回值不同，否则编译器无法分辨调用的是哪一个函数）</li>
</ul>
<p><strong>重写覆盖</strong>：</p>
<ul>
<li>不同范围 （基类与子类）</li>
<li>名字相同</li>
<li>参数相同</li>
<li>基类函数必须有virtual</li>
</ul>
<p><strong>重写隐藏</strong>：</p>
<ul>
<li>不同范围</li>
<li>名字相同</li>
<li>参数不同</li>
<li>virtual可有可无</li>
</ul>
<h2 id="结构体和类的区别"><a href="#结构体和类的区别" class="headerlink" title="结构体和类的区别"></a>结构体和类的区别</h2><p><strong>结构体</strong> ：使用struct声明，可以包含构造函数，常数，字段，方法，属性，索引器，运算符和嵌套类型等，不过，结构是值类型。</p>
<p>区别：类的成员默认是私有的，而结构体的成员则是公有的。</p>
<h2 id="继承类的构造顺序"><a href="#继承类的构造顺序" class="headerlink" title="继承类的构造顺序"></a>继承类的构造顺序</h2><figure class="highlight cc"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">class</span>  <span class="title">Base</span>&#123;</span></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        Base()</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;Constructing Base\n&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    ~Base()</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;Destructing Base\n&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> <span class="keyword">public</span> Base&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Derived()</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;Constructing Derived\n&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    ~Derived()</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;Destructing Derived\n&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Derived* d = <span class="keyword">new</span> Derived();</span><br><span class="line">    Base* b = d;</span><br><span class="line">    <span class="keyword">delete</span> b;</span><br><span class="line">    <span class="keyword">delete</span> d;</span><br><span class="line">    <span class="comment">//std::cout &lt;&lt; &quot;Hello, World!&quot; &lt;&lt; std::endl;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Constructing Base   </span><br><span class="line">Constructing Derived</span><br><span class="line">Destructing Base    </span><br><span class="line">Destructing Derived </span><br><span class="line">Destructing Base</span><br></pre></td></tr></table></figure>
<p>需要先调用基类的构造函数，然后是派生类的构造函数，析构顺序则类似于出栈，刚好相反</p>
<h2 id="explicit关键字"><a href="#explicit关键字" class="headerlink" title="explicit关键字"></a>explicit关键字</h2><p><code>explicit</code>的作用是用来声明类构造函数是显式调用的，禁止隐式调用。所以只用于修饰单参构造函数。因为无参构造函数和多参构造函数本身就是显式调用的。</p>
<p>隐式转换：<code>A a=True</code> ; (原来的的构造函数是<code>A(bool para)&#123;&#125;</code>)  这样的隐式转换使代码的可读性变差。使用<code>explicit</code> 后调用构造函数则必须使用 <code>A a(True)</code>这种显式方式来调用。</p>
<h2 id="构造函数初始化列表"><a href="#构造函数初始化列表" class="headerlink" title="构造函数初始化列表"></a>构造函数初始化列表</h2><p><code>compressed_sparse_row_graph(const ProcessGroup&amp; pg = ProcessGroup())</code><br>    <code>: m_process_group(pg), m_distribution(parallel::block(pg, 0)) &#123;&#125; //fun():使用初始化列表进行初始</code></p>
<p>构造函数为<code>compressed_sparse_row_graph()</code>在（）里面申明pg，然后在：后面使用pg来对<code>m_process_group</code>和<code>m_distribution()</code>进行初始化。</p>
<h2 id="const-引用形参"><a href="#const-引用形参" class="headerlink" title="const 引用形参"></a>const 引用形参</h2><p>使用const来修饰引用参数，无法通过修改形参来改变实参。</p>
<p><code>const int &amp;r = a</code>  无法通过r这个引用去修改a的值。 </p>
<h2 id="STL容器"><a href="#STL容器" class="headerlink" title="STL容器"></a>STL容器</h2><h3 id="容器："><a href="#容器：" class="headerlink" title="容器："></a>容器：</h3><p>即是数据结构，类似于Python的pandas的DataFrame等。Python自带的dict，list、tuple等。数据结构不止是简单的array。</p>
<h3 id="顺序容器"><a href="#顺序容器" class="headerlink" title="顺序容器"></a>顺序容器</h3><ul>
<li>vector   后部插入/删除，直接访问 </li>
<li>deque  前/后部插入/删除，直接访问 </li>
<li>list：双向链表，任意位置插入/删除 </li>
</ul>
<h3 id="关联容器"><a href="#关联容器" class="headerlink" title="关联容器"></a>关联容器</h3><ul>
<li>set：快速查找，无重复元素</li>
<li>multiset：快速查找，可有重复元素</li>
<li>map : 一对一映射，无重复元素，基于关键字查找 类似python的dict </li>
<li>multmap:一对一映射，可有重复元素，基于关键字查找</li>
</ul>
<h2 id="文件流"><a href="#文件流" class="headerlink" title="文件流"></a>文件流</h2><p>头文件：<code>&lt;fstream&gt;</code> 包含三个类：</p>
<ul>
<li><code>ofstream</code> :文件写操作，从内存写入存储设备 output</li>
<li><code>ifstream</code> ： 文件读操作，从存储设备读的内存中 input</li>
<li><code>fstream</code>   :读写操作  </li>
</ul>
<h2 id="memset"><a href="#memset" class="headerlink" title="memset()"></a>memset()</h2><p>需要包含的头文件是<string.h></string.h></p>
<p>作用是在一段内存块中填充某个给定的值，它对较大的结构体或数组进行清零操作的一种最快方法。</p>
<p><code>memset(struct/array,value,size)</code> 最后一个参数是填充对象的前size个元素</p>
<h2 id="str-的find-first-of-和substr"><a href="#str-的find-first-of-和substr" class="headerlink" title="str 的find_first_of() 和substr"></a>str 的find_first_of() 和substr</h2><p><code>find_first_of(char,begin_pos)</code> 从begin_pos开始寻找第一个char</p>
<p><code>string_obj.substr(int begin_pos, int len)</code> 截取begin_pos开始长度为len的子字符串。</p>
<h2 id="指针常量和常量指针"><a href="#指针常量和常量指针" class="headerlink" title="指针常量和常量指针"></a>指针常量和常量指针</h2><p>常量指针(const pointer)：</p>
<p>​    指针是一个常量指向的地址是常量不能变，而地址存储的值可以改变。 </p>
<p>​    申明<code>int *const p</code> </p>
<p>​    从右往左读，const 限定p对象是一个常量，而常量的具体类型则是是一个指向int类型的指针</p>
<p>​    从右往左读，遇到p就替换成“p is a ”遇到*就替换成“point to”   读作p is a const pointer point to int </p>
<p>指向常量的指针 (pointer to const )</p>
<p>​    指针是一个变量，指向的类型是常量.（通常用在形参中，使得形参指针指向的数据不会被修改）</p>
<p>​    申明<code>int const *p</code> 或者<code>const int *p</code>分别读作：p is a point to int const. 与 p is a point to const int. </p>
<h2 id="成员变量的初始化顺序"><a href="#成员变量的初始化顺序" class="headerlink" title="成员变量的初始化顺序"></a>成员变量的初始化顺序</h2><p>成员变量的初始化顺序，只与在类中声明的顺序有关，与在初始化列表中的顺序无关。 先声明，先初始化。</p>
<figure class="highlight cc"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> &#123;</span></span><br><span class="line">    <span class="keyword">private</span>: </span><br><span class="line">    	<span class="keyword">int</span> n1;</span><br><span class="line">    	<span class="keyword">int</span> n2;</span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">    	A():n2(<span class="number">0</span>),n1(n2+<span class="number">2</span>)&#123;&#125;</span><br><span class="line">    	<span class="function"><span class="keyword">void</span> <span class="title">Print</span><span class="params">()</span></span>&#123;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;n1:&quot;</span>&lt;&lt;n1&lt;&lt;<span class="string">&quot;,n2:&quot;</span>&lt;&lt;n2&lt;&lt;<span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span></span>&#123;</span><br><span class="line">    A a;</span><br><span class="line">    a.Print();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>先初始化n1，因为n2是随机值，所以n1也是随机值+2，之后初始化n2为0。</p>
<h2 id="容器中的end"><a href="#容器中的end" class="headerlink" title="容器中的end()"></a>容器中的end()</h2><p><code>begin()</code>迭代器指向第一个元素，<code>end()</code>迭代器指向尾元素的下一个位置,并不是末尾元素。对迭代器理解为指针，所以获得迭代器的使用*来获得指向的值。</p>
<p><code>max_element(first, last)</code> 比较的区间为[first, last）</p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>c++</tag>
        <tag>指针</tag>
      </tags>
  </entry>
  <entry>
    <title>特征选择和特征提取</title>
    <url>/2019-12-28-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E5%92%8C%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96.html</url>
    <content><![CDATA[<h1 id="特征选择和特征提取"><a href="#特征选择和特征提取" class="headerlink" title="特征选择和特征提取"></a>特征选择和特征提取</h1><h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><ol>
<li>降低维度，后续的分类器设计更容易计算，可以加快速度</li>
<li>消除特征之间可能存在的相关性，减少与分类无关的信息（理解成降噪？），从而更好的分类。</li>
</ol>
<a id="more"></a>
<h2 id="类别可分性质的度量"><a href="#类别可分性质的度量" class="headerlink" title="类别可分性质的度量"></a>类别可分性质的度量</h2><h3 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h3><ol>
<li><p>点到点之间的距离</p>
<p>欧式距离：</p>
<script type="math/tex; mode=display">
D(a, b) = || a – b ||</script><p>平方形式（向量转置相乘，点积的形式）：</p>
<script type="math/tex; mode=display">
D^{2}(a,b) = (a - b)^{T}(a - b) = \sum_{k = 1}^{n}{(a_{k} - b_{k})^{2}}</script><p>其中a和b为n维向量，其第k个特征分别是$a_k$和$b_k$。 <strong>各自对应的特征维度差的平方之和。</strong></p>
</li>
<li><p>点到点集之间的距离</p>
<p>在n维空间中，点x到点$a^{(i)}$之间的距离平方为：</p>
<script type="math/tex; mode=display">
D^{2}(x,a^{(i)}) = \sum_{k = 1}^{n}{(x_{k} - a_{k}^{(i)})^{2}}</script><p>因此x到有K个点组合的均方距离，就是到每个点均方距离相加之后取均值。</p>
<script type="math/tex; mode=display">
\overline{D^{2}(x,\{ a^{(i)}\})} = \frac{1}{K}\sum_{i = 1}^{K}{D^{2}(x,a^{(i)}) =}\frac{1}{K}\sum_{i = 1}^{K}\left\{ \sum_{k = 1}^{n}{(x_{k} - a_{k}^{(i)})^{2}} \right\}</script></li>
</ol>
<h4 id="类内距离"><a href="#类内距离" class="headerlink" title="类内距离"></a>类内距离</h4><p>   <strong>类中每个点到其他点的均方距离累加之后求均值</strong></p>
<p>   n维空间中同一类内各模式样本点集$\{a^{(i)}\}_{i=1,2,\ldots{},K}$，其内部各点的均方距离为$\overline{D^{2}(\{ a^{(j)}\},\{ a^{(i)}\})}$，其中$i,j = 1,2,\ldots,K,i \neq j$，即：</p>
<script type="math/tex; mode=display">
   \overline{D^{2}(\{ a^{(j)}\},\{ a^{(i)}\})} = \frac{1}{K}\sum_{j = 1}^{K}\left\lbrack \frac{1}{K - 1}\sum_{\begin{matrix}
    i = 1 \\
    i \neq j \\
   \end{matrix}}^{K}{\sum_{k = 1}^{n}{(a_{k}^{(j)} - a_{k}^{(i)})^{2}}} \right\rbrack</script><p>   每个点到其他点的平方距离求和，<strong>三层循环</strong>（点j，点i，维度k）</p>
<p>   可证明：</p>
<script type="math/tex; mode=display">
   \overline{D^{2}} = 2\sum_{k = 1}^{n}\sigma_{k}^{2}</script><p>   其中$\sigma_{k}^{2}$为样本集合$a^{(i)}$在第k个分量上的无偏方差，即：</p>
<script type="math/tex; mode=display">
   \sigma_{k}^{2} = \frac{1}{K - 1}\sum_{i = 1}^{K}{(a_{k}^{(i)} - \overline{a_{k}})^{2}}</script><p>   其中$\overline{a_{k}} = \frac{1}{K}\sum_{i = 1}^{K}a_{k}^{(i)}$为$a^{(i)}$在第k个分量方向上的均值。</p>
<p>   <strong>无偏估计除以的是K-1，把数据集合用一个完整的$K \times N$矩阵来理解，就可以很方便的对应这些距离公式，均值，方差的计算了。公式可以不用管。</strong></p>
<h4 id="类间距离"><a href="#类间距离" class="headerlink" title="类间距离"></a>类间距离</h4><p>   <strong>两类样本均值质心的均方距离</strong></p>
<p>   常用两类样本各自质心间的距离作为类间距离，并假设两类样本出现的概率相等，则：</p>
<script type="math/tex; mode=display">
   D^{2} = \sum_{k = 1}^{n}{(m_{1_{k}} -}m_{2_{k}})^{2}</script><p>   其中 $m_1$ 和 $m_2$ 为两类模式样本集各自的均值向量， $m_{1k}$ 和$m_{1k}$为 两个均值向量的第k个特征分量，n为维数。</p>
<h3 id="散布矩阵"><a href="#散布矩阵" class="headerlink" title="散布矩阵"></a>散布矩阵</h3><p>不管类内还是类间都涉及到与均值向量之差的向量乘以其转置。</p>
<h4 id="类内散布矩阵"><a href="#类内散布矩阵" class="headerlink" title="类内散布矩阵"></a>类内散布矩阵</h4><p><strong>每个样本向量减去均值向量后乘以结果的转置(矩阵)，<em>然后将得到的矩阵再相加</em></strong></p>
<p>考虑一类内模式点集$\{ a^{(i)}\}_{i = 1,2,\ldots,K}$，其类内散布矩阵为：</p>
<script type="math/tex; mode=display">
S_w = \sum_{i = 1}^{K}\{(a^{(i)} - m)(a^{(i)} - m)^{T}\}</script><p>其中m是样本集合的<strong>均值向量</strong></p>
<script type="math/tex; mode=display">
m = \frac{1}{K}\sum_{i = 1}^{K}a{(i)}</script><p>对属于同一类的模式样本，<strong>类内散布矩阵表示各样本点围绕其均值周围的散布情况</strong>。</p>
<h4 id="类间散布矩阵"><a href="#类间散布矩阵" class="headerlink" title="类间散布矩阵"></a>类间散布矩阵</h4><p><strong>两个质心之差形成的向量与其转置相乘而形成的矩阵</strong>，类比与类内散布矩阵，<strong>类内</strong>是每个样本向量减去该类的均值向量，是<strong>所有样本向量</strong>都要参与。</p>
<script type="math/tex; mode=display">
S_{b2} = (m_{1} - m_{2})(m_{1} - m_{2})^{T}</script><p>对三个以上的类别，类间散布矩阵常写成：</p>
<script type="math/tex; mode=display">
S_{b} = \sum_{i = 1}^{c}{P(\omega_{i})}(m_{i} - m_{0})(m_{i} - m_{0})^{T}</script><p>其中，$m_{0}$为多类模式（如共有c类）分布的总体均值向量，即：</p>
<script type="math/tex; mode=display">
m_{0} = E\{ x\} = \sum_{i = 1}^{c}{P(\omega_{i})m_{i}},\quad\forall\omega_{i},\mspace{6mu} i = 1,2,\ldots,c</script><p><strong>用质心来代表每一类，多类模式引入概率来取得总体所有样本均值。</strong>计算的形式与类内的很相似了，$m_0$对应着类内中一个类的均值向量，每个质心对应这类内中的一个样本向量。</p>
<h3 id="多类模式集散布矩阵"><a href="#多类模式集散布矩阵" class="headerlink" title="多类模式集散布矩阵"></a>多类模式集散布矩阵</h3><p>多类情况的<strong>类内散布矩阵</strong>，可写成<strong>各类的类内散布矩阵的先验概率的加权和</strong>，即：</p>
<script type="math/tex; mode=display">
S_{w} = \sum_{i = 1}^{c}{P(\omega_{i})E\{}(x - m_{i})(x - m_{i})^{T}|\omega_{i}\} = \sum_{i = 1}^{c}{P(\omega_{i})C_{i}}</script><p>其中$C_{i}$是第i类的协方差矩阵。E是求期望，也就是每个样本的矩阵计算完了之后相加还要除以样本数目N或者N-1（N-1是无偏估计）。</p>
<p><strong>协方差(两个变量各自与各自均值的差相乘)：</strong></p>
<script type="math/tex; mode=display">
Cov(X,Y)=E[(X-m_X)(Y-m_Y)]=E[XY]-E[X]\times E[Y]</script><p>协方差矩阵是不同维度之间的相关性的表达，一个d维的样本也能建立协方差矩阵尺度为$d\times d$, 所以一个类的协方差矩阵是每个样本的协方差矩阵相加后取<strong>均值</strong>得到的。</p>
<p>有时，用多类模式<strong>总体分布的散布矩阵</strong>来反映其<strong>可分性</strong>，即：</p>
<script type="math/tex; mode=display">
S_{t} = E\{(x - m_{0})(x - m_{0})^{T}\},\quad x \in \forall\omega_{i},\mspace{6mu} i = 1,2,\ldots,c</script><p>其中，$m_{0}$为多类模式分布的总体均值向量。</p>
<p>可以证明：$S_{t} = S_{w} +S_{b}$，即<strong>总体散布矩阵是各类类内散布矩阵与类间散布矩阵之和。</strong></p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>在尽量不降低分类精度的前提下，选择更少的特诊来进行分类。</p>
<h3 id="独立特征的选择准则"><a href="#独立特征的选择准则" class="headerlink" title="独立特征的选择准则"></a>独立特征的选择准则</h3><p><strong>不同类</strong>的均值向量（<strong>质心点</strong>）之间的<strong>距离最大</strong>，而<strong>同一类</strong>的样本向量的方差之和最小（与其<strong>质心</strong>的<strong>距离较近</strong>）。用一个比值来作为度量标准。</p>
<p>假设个特征之间统计独立，那么对<strong>每个特征分量都去计算那个标准</strong>，就能够知道每个特征对样本可分性的贡献了。</p>
<p>对于$ω_{i}$和$ω_{j}$两类训练样本，假设其均值向量为$m_{i}$和$m_{j}$，其k维方向的分量为$m_{ik}$和$m_{jk}$，方差为$\sigma_{\text{ik}}^{2}$和$\sigma_{\text{jk}}^{2}$，定义可分性准则函数(也就是那个标准)：</p>
<script type="math/tex; mode=display">
G_{k} = \frac{(m_{\text{ik}} - m_{\text{jk}})^{2}}{\sigma_{\text{ik}}^{2} + \sigma_{\text{jk}}^{2}},\;\;k = 1,2,\ldots,n</script><p>则$G_{K}$为正值。$G_{K}$值越大，表示测度值的第k个分量对分离$ω_{i}$和$ω_{j}$两类越有效。将$\{G_{K},k=1,2,\ldots{},n\}$按大小排队，选出最大的m个对应的测度值作为分类特征，即达到特征选择的目的。</p>
<h3 id="一般特征的散布矩阵准则"><a href="#一般特征的散布矩阵准则" class="headerlink" title="一般特征的散布矩阵准则"></a>一般特征的散布矩阵准则</h3><p>回顾类内与类间散布矩阵：</p>
<p>多类类内散布矩阵：</p>
<script type="math/tex; mode=display">
S_{w} = \sum_{i = 1}^{c}{P(\omega_{i})E\{}(x - m_{i})(x - m_{i})^{T}|\omega_{i}\}</script><p> 多类类间散布矩阵：</p>
<script type="math/tex; mode=display">
S_{b} = \sum_{i = 1}^{c}{P(\omega_{i})}(m_{i} - m_{0})(m_{i} - m_{0})^{T}</script><p>直观上，<strong>类间离散度越大且类内离散度越小</strong>，则可分性越好。因此，可推导出散布矩阵准则采用如下形式： (<strong>矩阵的逆类比于除法</strong>)</p>
<p>行列式形式：$J_{1} = \det(S_{w}^{- 1}S_{b}) = \prod_{i}^{}\lambda_{i}$</p>
<p>迹形式：$J_{2} = tr(S_{w}^{- 1}S_{b}) = \sum_{i}^{}\lambda_{i}$</p>
<p>其中，$λ_{i}$是矩阵$S_{w}^{- 1}S_{b}$的特征值。使$J_{1}$或$J_{2}$<strong>最大</strong>的子集可作为选择的分类特征。</p>
<h2 id="离散K-L变换"><a href="#离散K-L变换" class="headerlink" title="离散K-L变换"></a>离散K-L变换</h2><p>前面讨论的<strong>直接删去</strong>一些特征的做法<strong>不理想</strong>，因为一般来说，原来的n个数据各自在不同程度上反映了识别对象的某些特征，简单地删去某些特征可能会丢失较多的有用信息 。</p>
<p>将样本看成是对应维度D维随机向量的一次采样，对D维向量<em>x</em>用一个完备的<strong>正交归一向量系</strong>进行<strong>展开</strong>（理解成用一组正交的基向量来表示）。或者是对D维特征进行<strong>正交变换</strong>，用变换后的维度中选择少数的几个特征，这几个少数的特征<strong>尽可能多地反映各类模式之间的差异</strong>，而这些特征间<strong>又尽可能相互独立 。</strong></p>
<h3 id="展开式的形式"><a href="#展开式的形式" class="headerlink" title="展开式的形式"></a>展开式的形式</h3><p>设一连续的随机实函数x(t)，$T_{1} \leq t \leq T_{2}$，则x(t)可用已知的<strong>正交函数集</strong>$φ_{j}(t),j=1,2,\ldots{\infty}$的<strong>线性组合来展开</strong>，即公式1：</p>
<script type="math/tex; mode=display">
\begin{split}
x(t) &= a_{1}\varphi_{1}(t) + a_{2}\varphi_{2}(t) + \cdots + a_{j}\varphi_{j}(t) + \cdots\\
&=\sum_{j=1}a_j\varphi_j(t),\qquad T_1\le t \le T_2 \qquad (1)\\
\end{split}</script><p>式中，$a_{j}$为展开式的随机系数，$φ_{j}(t)$为一连续的正交函数，它应满足：</p>
<script type="math/tex; mode=display">
\int_{T_{1}}^{T_{2}}{\varphi_{n}^{(t)}{\tilde{\varphi}}_{m}(t)dt = \left\{ \begin{matrix}
1, & \text{if}\mspace{6mu} m = n \\
0 & \text{if}\mspace{6mu} m \neq n \\
\end{matrix} \right.\ }</script><p>其中${\tilde{\varphi}}_{m}(t)$为$φ_{m}(t)$的共轭复数式。</p>
<p>将上式写成离散的正交函数形式，使连续随机函数x(t)和连续正交函数$φ_{j}(t)$在区间$T_{1} \leq t \leq T_{2}$内被等间隔采样为n个离散点，<strong>用采n个点的样本，来描绘原来的函数</strong>即：</p>
<script type="math/tex; mode=display">
x(t) \rightarrow \{ x(1),x(2),\cdots,x(n)\}\\
\varphi_{j}(t) \rightarrow \{\varphi_{j}(1),\varphi_{j}(2),\cdots,\varphi_{j}(n)\}</script><p>写成向量形式：</p>
<script type="math/tex; mode=display">
\begin{split}
x &= (x(1),x(2),\cdots,x(n))^{T}\\
\varphi_{j}& = (\varphi_{j}(1),\varphi_{j}(2),\cdots,\varphi_{j}(n))^{T},\mspace{6mu} j = 1,2,\cdots,n
\end{split}</script><p><strong>每一个正交函数都要对应的平行采样</strong>，而不是一个函数采一个时刻的样。</p>
<p>将公式(1)<strong>取n项近似</strong>(可以理解成逼近)，并写成离散展开式：</p>
<script type="math/tex; mode=display">
x = \sum_{j = 1}^{n}{a_{j}\varphi_{j}} = \Phi a,\quad T_{1} \leq t \leq T_{2} \qquad(2)</script><p>其中，a为展开式中随机系数的向量形式，即：$a = (a_{1}, a_{2}, \ldots{},a_{j}, \ldots{},a_{n})^{T}$</p>
<p>$\Phi$为n x n维矩阵，即：</p>
<script type="math/tex; mode=display">
\Phi = (\varphi_{1},\varphi_{2},\cdots,\varphi_{n}) = \begin{bmatrix}
\varphi_{1}(1) & \varphi_{2}(1) & \cdots & \varphi_{n}(1) \\
\varphi_{1}(2) & \varphi_{2}(2) & \cdots & \varphi_{n}(2) \\
\cdots & \cdots & \cdots & \cdots \\
\varphi_{1}(n) & \varphi_{2}(n) & \cdots & \varphi_{n}(n) \\
\end{bmatrix}</script><p>其中，每一列为正交函数集中的一个函数，小括号内的序号为正交函数的采样点次序。因此，$\Phi$实质上是由$φ_{j}$向量组成的正交变换矩阵，它将x变换成a。<strong>不应该是a变换成x吗，x变成a应该是用逆矩阵来变换的？</strong></p>
<p>对于<strong>每一类正交函数是相同</strong>的，但是每一类的<strong>展开系数向量则不同</strong></p>
<h3 id="展开式的性质"><a href="#展开式的性质" class="headerlink" title="展开式的性质"></a>展开式的性质</h3><h4 id="正交向量集-varphi-j-的确定"><a href="#正交向量集-varphi-j-的确定" class="headerlink" title="正交向量集$\varphi_j$的确定"></a>正交向量集$\varphi_j$的确定</h4><p>K-L展开式的根本性质是<strong>将随机向量x展开为另一组正交向量$\varphi_j$的线性和</strong>，且其展开式系数$a_j$（即系数向量a的各个分量）具有不同的性质。</p>
<p>设随机向量x的总体<strong>自相关矩阵</strong>为$R = E\{xx^{T}\}$。由$x = \sum_{j = 1}^{n}{a_{j}\varphi_{j}} = \Phi a,\quad T_{1} \leq t \leq T_{2}$将$x=\Phi a$代入$R = E\{xx^{T}\}$，得：</p>
<script type="math/tex; mode=display">
{R = E\{Φaa^{T}Φ^{T}\}=Φ(E\{aa^{T}\})Φ^{T}}</script><p>要求<strong>系数向量a</strong>的各个<strong>不同分量</strong>应<strong>统计独立</strong>，即应使$(a_{1},a_{2}, \ldots{}, a_{j}, \ldots{},a_{n})$满足如下关系：<strong>(没搞懂)</strong></p>
<script type="math/tex; mode=display">
E(a_{j}a_{k}) = \left\{ \begin{matrix}
\lambda_{j} & \text{if}\mspace{6mu} j = k \\
0 & \text{if}\mspace{6mu} j \neq k \\
\end{matrix} \right.\</script><p>写成矩阵形式，应使：$E\{a a^{T}\} =D_{λ}$，其中$D_{λ}$为对角形矩阵，其互相关成分均为0，即:</p>
<script type="math/tex; mode=display">
D_{\lambda} = \begin{bmatrix}
\lambda_{1} & 0 & \cdots & \cdots & 0 \\
0 & \ddots & 0 & \cdots & 0 \\
0 & & \lambda_{j} & & 0 \\
0 & \cdots & 0 & \ddots & 0 \\
0 & \cdots & \cdots & 0 & \lambda_{n} \\
\end{bmatrix}</script><p>则：$R =\Phi D_{λ}Φ^{T}$</p>
<p>由于$\Phi$中的各个向量$\varphi_j$都<strong>相互归一正交有性质</strong>（$\Phi^T \Phi=\Phi^{-1}\Phi=I$）<strong>$\Phi$是正交矩阵？</strong>，故有：</p>
<script type="math/tex; mode=display">
{RΦ=ΦD_{λ}Φ^{T}Φ=ΦD_{λ}}</script><p>其中，$\varphi_j$向量对应为：$R\varphi_j=λ_j\varphi_j$</p>
<p>可以看出，<strong>$λ_j$是x的自相关矩阵R的特征值，$\varphi_j$是对应的特征向量。</strong>因为R是实对称矩阵，其不同特征值对应的特征向量应<strong>正交(向量之间的夹角为90度)</strong>，即：</p>
<script type="math/tex; mode=display">
\varphi_{j}^{T}\varphi_{k} = \left\{ \begin{matrix}
1 & \text{if}\mspace{6mu} j = k \\
0 & \text{if}\mspace{6mu} j \neq k \\
\end{matrix} \right.\</script><p>由式$x = \sum_{j = 1}^{n}{a_{j}\varphi_{j}} = \Phi a,\quad T_{1} \leq t \leq T_{2}$，<strong>K-L展开式系数应为</strong>：</p>
<script type="math/tex; mode=display">
a = \Phi^{T}x</script><p><strong>结论：正交向量集$\Phi$是样本的自相关矩阵$R = E\{xx^{T}\}$的特征向量构成的集合。</strong>如果选择$\Phi$的维度是$D\times K$即只选择K个特征向量去展开来逼近原来的样本，<strong>则应该选择特征值前K大的的K个特征向量来组成。</strong></p>
<p>这段推导没有搞懂，还是模式识别书上的展开后去前n项来逼近原始向量之后求原向量与逼近向量之间的均方误差最小，然后在等式约束下使用拉格朗日发来求极值，解得系数是是$\Phi$的特征值好理解。</p>
<h4 id="K-L展示式系数的计算步骤"><a href="#K-L展示式系数的计算步骤" class="headerlink" title="K-L展示式系数的计算步骤"></a>K-L展示式系数的计算步骤</h4><ol>
<li>求随机向量x的自相关矩阵：$R = E\{xx^{T}\}$</li>
<li>求出矩阵R的特征值$λ_j$和对应的特征向量$\varphi_j，j = 1,2,\ldots{},n$，得矩阵：$\Phi = (\varphi_{1},\varphi_{2},\cdots,\varphi_{n})$</li>
<li>计算展开式系数：$a = \Phi^{T}x$</li>
</ol>
<h3 id="按K-L展开式选择特征"><a href="#按K-L展开式选择特征" class="headerlink" title="按K-L展开式选择特征"></a>按K-L展开式选择特征</h3><p>K-L展开式用于特征选择相当于一种<strong>线性变换</strong>。 </p>
<p>若从n个特征向量中取出m个组成变换矩阵$\Phi$，即$\Phi=(\Phi_1,\Phi_2,\ldots{\Phi_m})\quad m&lt;n$  得到的$\Phi$矩阵的维度是$n\times m$,样本向量x是n为向量，则通过$a = \Phi^{T}x$得到的系数向量是m维度，<strong>系数向量就是降维后得到的新向量。</strong></p>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>选取变换矩阵$\Phi$ 后，让降维后得到的<strong>新向量a在最小均方误差条件下接近原来的向量x</strong>。（不是直接用a与x取比较的，因为维度都不同，是让a乘以变换矩阵去逼近x。换句话说，x是被其自相关矩阵展开的）</p>
<p>对于$x = \sum_{j = 1}^{n}{a_{j}\varphi_{j}}$，现仅取m项，对略去的系数项用预先选定的常数b代替，此时对x的估计值为：</p>
<script type="math/tex; mode=display">
\hat{x} = \sum_{j = 1}^{m}{a_{j}\varphi_{j}} + \sum_{j = m + 1}^{n}{b\varphi_{j}}</script><p>则产生的误差为：</p>
<script type="math/tex; mode=display">
\Delta x = x - \hat{x} = \sum_{j = m + 1}^{n}{(a_{j} - b)\varphi_{j}}</script><p>则$\Delta x$的均方误差为：(<strong>为什么引入了期望</strong>)(<strong>为什么没有了$\varphi_j$</strong>)</p>
<script type="math/tex; mode=display">
\overline{\varepsilon^{2}} = E\{||\Delta x||\}^{2} = \sum_{j = m + 1}^{n}{\{ E(a_{j} - b)^{2}\}}</script><p>要使$\overline{\varepsilon^{2}}$最小，对b的选择应满足：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial b}\lbrack E(a_{j} - b)^{2}\rbrack = \frac{\partial}{\partial b}\lbrack E(a_{j}^{2} - 2a_{j}b + b^{2})\rbrack = - 2\lbrack E(a_{j}) - b\rbrack = 0</script><p>因此，$b =E{[}a_{j}{]}$，即<strong>对省略掉的a中的分量，应使用它们的数学期望来代替</strong>，此时的误差为：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
{\overline{\varepsilon^{2}} = \sum_{j = m + 1}^{n}{E\lbrack(a_{j} - E\{ a_{j}\})^{2}\rbrack }\\
=\sum_{j = m + 1}^{n}{\varphi_{j}^{T}E\lbrack(x - E\{ x\})(x - E\{ x\})^{T}\rbrack\varphi_{j}}}= { \sum_{j = m + 1}^{n}{\varphi_{j}^{T}C_{x}}\varphi_{j}}\\
\end{split}
\end{equation}</script><p>其中，$C_{x}$为x的协方差矩阵。<strong>（第二步怎么推出来的？）</strong></p>
<p>设$λ_{j}$为$C_{x}$的第j个特征值，$\varphi_{j}$是与$λ_{j}$对应的特征向量，则$C_{x}\varphi_{j} = \lambda_{j}\varphi_{j}$由于$\varphi_{j}^{T}\varphi_{j} = 1$,从而$\varphi_{j}^{T}C_{x}\varphi_{j} = \lambda_{j}$因此:</p>
<script type="math/tex; mode=display">
\overline{\varepsilon^{2}} = \sum_{j = m + 1}^{n}{\varphi_{j}^{T}C_{x}}\varphi_{j} = \sum_{j = m + 1}^{n}\lambda_{j}</script><p>由此可以看出，$λ_{j}$值越小，误差也越小。<strong>（这里的特征值与自相关矩阵的特征值不同？，所以原来的自相关矩阵的特征应该选择大的。）</strong></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>按照最小均方差的准则来选择特征，应使得$E[a_j]=0$<strong>为什么?</strong>。因为$E[a]=E[\Phi^Tx]=\Phi^TE[x]$,所以应使的$E[x]=0$。因此在K-L变换之前，需要先将<strong>其均值作为新坐标轴的原点</strong>，采用协方差矩阵C或自相关矩阵R来计算特征值。如果<strong>$E[x] ≠0$</strong>，则只能得到<strong>“次最佳</strong>”的结果。 </p>
<p>为了使误差尽可能小，<strong>选择的特征向量对应的特征值要是最大的</strong>。</p>
<p>K-L变换是在<strong>均方误差最小的意义下</strong>获得数据压缩（<strong>降维</strong>）的<strong>最佳变换</strong>，且不受模式分布的限制。 整个变换使的整个<strong>模式分布结构尽可能保持不变</strong>。</p>
<p>通过K-L变换能获得互不相关的新特征。 主<strong>成分分析出发点是变换得到的新特征的方差最大</strong>，从而保证原始样本在改维特征上的差异更大，从而更好的区分。PCA使用的变换矩阵是样本的协方差矩阵$C_x=E[(x-E[x])(x-E[x]^T)]$，因此当<strong>样本集的均值为0</strong>即$[E[x]=0]$，或者<strong>对样本进行去均值处理</strong>的时候，<strong>K-L变换就与PCA等价</strong>了。</p>
<p>需要指出的是，采用K-L变换作为模式分类的<strong>特征提取</strong>时，要<strong>特别注意保留不同类别的模式分类鉴别信息</strong>，仅单纯考虑尽可能代表原来模式的主成分，有时并不一定有利于分类的鉴别。 （<strong>这里不同类别的模式分类鉴别信息是什么？）</strong></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>降维</tag>
        <tag>特征提取</tag>
        <tag>距离矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title>奇思妙想</title>
    <url>/2019-12-27-%E5%A5%87%E6%80%9D%E5%A6%99%E6%83%B3.html</url>
    <content><![CDATA[<h1 id="奇思妙想"><a href="#奇思妙想" class="headerlink" title="奇思妙想"></a>奇思妙想</h1><p>记录一些觉得有用的点子，公式推导等。当成便利贴来使用的。23333</p>
<a id="more"></a>
<h2 id="条件联合分布的推导"><a href="#条件联合分布的推导" class="headerlink" title="条件联合分布的推导"></a>条件联合分布的推导</h2><p>首先搞清楚逗号是与的意思，把逗号去掉，加个括号就好理解了。</p>
<p>$P(S,A|T)$:事件T发生的条件下，事件S、A均发生的概率。直接把逗号给去掉，就是$P((SA)|T)$竖线的优先级别高。因此有：$P(S,A|T)=\frac{P(SAT)}{P(T)}$</p>
<p>$P(S|A,T)$ ：事件A、T均发生的条件下，事件S发生的概率 。</p>
<script type="math/tex; mode=display">
P(X=a,Y=b|Z=c)=P(X=a|Y=b,Z=c)P(Y=b|Z=c)</script><p>上式的证明如下：</p>
<script type="math/tex; mode=display">
\begin{equation}\nonumber
\begin{split}
P(X=a,Y=b|Z=c)&=\frac{P(X=a,Y=b,Z=c)}{P(Z=c)}\\
&=\frac{P(X=a|Y=b,Z=c)P(Y=b,Z=c)}{P(Z=c)}\\
&=\frac{P(X=a|Y=b,Z=c)P(Y=b|Z=c)P(Z=c)}{P(Z=c)}\\
&=P(X=a|Y=b,Z=c)P(Y=b|Z=c)
\end{split} 
\end{equation}</script><h2 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h2><p>根据现有的数据样本集D估计一个新的样本的分布$x$即$P(x|D)$.通过D引入$\theta$,然后通过$\theta$来估计x的分布$p(x|\theta)$</p>
<script type="math/tex; mode=display">
\begin{equation}\nonumber
\begin{split}
P(x|D)&=\int P(x,\theta|D)d\theta\\
&=\int \frac{P(x,\theta,D)}{P(D)}d\theta\\
&=\int \frac{P(x|\theta,D)P(\theta,D)}{P(D)}d\theta\\
&=\int \frac{P(x|\theta,D)P(\theta|D)P(D)}{P(D)}d\theta\\
&=\int P(x|\theta,D)P(\theta|D)P(D)d\theta\\
\end{split} 
\end{equation}</script><p>关于第一步$P(x|D)=\int P(x,\theta|D)d\theta$ 的理解在D条件下的联合分布对另一个变量求积分，就得到了一个边缘分布，不过这个边缘分布是在D条件下的。</p>
<h2 id="点子"><a href="#点子" class="headerlink" title="点子"></a>点子</h2><ol>
<li>目标函数不可导的情况下优化：遗传算法</li>
<li>中文OCR：github上的开源项目：chinese_ocr</li>
<li>科研：深度学习前沿领域的bench ，NLP中：中文的Bench？ GLUE：的冗余分析？</li>
<li>从指令的角度去研究能耗的情况？微观上的指令比例与宏观应用上的差别？去判断负载是否合理？<ol>
<li>不同指令集上的常用指令的能效？</li>
<li>微观的负载与宏观上的应用的能效是否有差别？从二者的指令比例去分析。</li>
<li>量化之后通过分析指令比例就能推导出负载的能耗情况。</li>
</ol>
</li>
<li>C语言局部变量的数组（位于栈）在声明时就需要指定具体的大小值，用常量指定，而不能用变量指定。如果需要用变量指定，则只能使用malloc来分配在堆上。</li>
</ol>
<h2 id="待解决的问题"><a href="#待解决的问题" class="headerlink" title="待解决的问题"></a>待解决的问题</h2><ol>
<li>矩阵的特征值的个数怎么判断的？重根的情况。</li>
<li>算出来特征向量任意取的情况，KL变换的变换矩阵还有什么作用呢？</li>
<li>Adaboost为什么要求迭代后的更新的权重使得上一轮的学习器在这轮的性能为随机猜测呢？</li>
<li>什么是浮点运算？加减乘除都各自算一次浮点运算吗？<ol>
<li>都各自只算做一次浮点运算。</li>
</ol>
</li>
</ol>
<h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><p>sublime text 激活：</p>
<p><a href="http://wordjian.com/2019/10/03/sublime/#%E6%90%9E%E8%B5%B7%EF%BC%8C%E6%90%9E%E8%B5%B7">http://wordjian.com/2019/10/03/sublime/#%E6%90%9E%E8%B5%B7%EF%BC%8C%E6%90%9E%E8%B5%B7</a>  </p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>点子</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习模型总结</title>
    <url>/2019-12-26-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93.html</url>
    <content><![CDATA[<p>根据课程做出的总结</p>
<a id="more"></a>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th style="text-align:center">模型</th>
<th>损失函数</th>
<th>正则/先验</th>
<th>目标函数</th>
<th>参数</th>
<th>超参数</th>
<th>优化算法</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>贝叶斯判别</td>
<td style="text-align:center">y</td>
<td>$ p(\hat y\neq y</td>
<td>x)=\left\{ \begin{aligned} p(\hat y=1</td>
<td>x) &amp; , &amp; y=0\ p(\hat y=0</td>
<td>x)&amp;,&amp;y=1\ \end{aligned} \right.$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>K-L变换</td>
<td style="text-align:center">$y=\Phi^Tx$</td>
<td>$\overline{\varepsilon^{2}} = \sum_{j = m + 1}^{n}{\varphi_{j}^{T}C_{x}}\varphi_{j} = \sum_{j = m + 1}^{n}\lambda_{j}$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>$R = E\{xx^{T}\}\\R\varphi_j=λ_j\varphi_j$</td>
</tr>
<tr>
<td></td>
<td style="text-align:center"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>正则</tag>
        <tag>损失函数</tag>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title>线性判别函数</title>
    <url>/2019-12-08-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0.html</url>
    <content><![CDATA[<h1 id="线性判别函数"><a href="#线性判别函数" class="headerlink" title="线性判别函数"></a>线性判别函数</h1><p>模式识别与机器学习第三章的笔记。</p>
<a id="more"></a>
<h2 id="线性判别函数-1"><a href="#线性判别函数-1" class="headerlink" title="线性判别函数"></a>线性判别函数</h2><h3 id="n维线性判别函数的一般形式"><a href="#n维线性判别函数的一般形式" class="headerlink" title="n维线性判别函数的一般形式"></a>n维线性判别函数的一般形式</h3><p>一个n维线性判别函数的一般形式： </p>
<script type="math/tex; mode=display">
d(x)=w_1x_1+w_2x_2+\dots+w_nx_n+b=W_0^tx+b</script><p>其中$W_0=(w_1,w_2,\dots,w_n)^T$称为<strong>权向量</strong>，如果把b也加入到权向量则$d(x)=W^Tx$ 其中$x=(x_1,x_2,\dots,x_n,1)$ 称为增广模式向量 (b对应的特征维度取值为1),而权向量也对应的加入b$W=((w_1,w_2,\dots,w_n,b)^T)$ 称为增广权向量 。</p>
<h3 id="两类情况"><a href="#两类情况" class="headerlink" title="两类情况"></a>两类情况</h3><p>样本如果只有两类$\omega_1,\omega_2$则可以只用一个线性判别函数进行判别：</p>
<script type="math/tex; mode=display">
d(x)=W^Tx=\left\{
  \begin{array}{**lr**}  
             >0 &  if\;x\in \omega_1\\\\  
             \le0& if\;x\in\omega_2
             \end{array}  
\right.</script><h3 id="多类情况"><a href="#多类情况" class="headerlink" title="多类情况"></a>多类情况</h3><h4 id="多类情况1"><a href="#多类情况1" class="headerlink" title="多类情况1"></a>多类情况1</h4><p>用线性判别函数将属于$ω_i$类的模式与不属于$ω_i$类的模式分开，其判别函数为：</p>
<script type="math/tex; mode=display">
d(x)=W^Tx=\left\{
  \begin{array}{**lr**}  
             >0 &  if\;x\in \omega_i\\\\  
             \le0& if\;x\notin\omega_i
             \end{array}  
\right.</script><p>这种情况称为$w_i/\bar w_i$两分法，即把M类多类问题分成M个两类问题，因此共有<strong>M个判别函数</strong> ，每一类都有一个自己的判别函数，只负责把这一类给判别出来。</p>
<p>把样本点带入所有的判别函数，<strong>有且只有一个</strong>判别函数判定这个样本值为正，其他判别函数都判定为负，则分类成功，否则分类失败。分类失败的区域称为<strong>不确定区域</strong></p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1577451087/blog/machine-learning/%E5%A4%9A%E7%B1%BB%E6%83%85%E5%86%B51.jpg" alt=""></p>
<p>判别界面<strong>不穿过类</strong>，每个判别界面可以准确的判别出一类样本。</p>
<p>不确定区域在图上就是除了能判定3个类别以外的区域，包括三条直线围成的中间的三角形，和三个角延伸出来的区域。</p>
<h4 id="多类情况2"><a href="#多类情况2" class="headerlink" title="多类情况2"></a>多类情况2</h4><p>每个判别函数去划分一对样本，一个判别界面只能分开两种类别，称为$\omega_i/\omega_j$两分法。判别函数为：</p>
<script type="math/tex; mode=display">
d_{ij}(x)=W_{ij}^Tx</script><p>其中当$d_{ij}x&gt;0 \quad\forall j\neq i$ 则将样本划入$\omega_i$类。比如$d_{12}(x)$只能判别是第一类还是第二类样本。</p>
<p>重要的性质：$d_{ij}=-d_{ji}$</p>
<p>对于M类模式，总共需要M(M-1)/2个判别函数。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1577451087/blog/machine-learning/%E5%A4%9A%E7%B1%BB%E6%83%85%E5%86%B52.jpg" alt=""></p>
<p>判别界面会穿过类，因此一个单独的判别函数无法准确的判别出一个完整无误的类，所以需要多个判别函数来辅助判别。只有所有的判别函数都判别这个样本都属于同一类，才能确定其属于这一类。</p>
<p>需要依次根据这一类$\omega_i$的所有判别函数$d_{i1},d_{i2},\dots,d_{ij}$ 都得到正值，才判断其为$\omega_i$类。</p>
<p>不确定区域只剩下中间围成的三角区域了。因此多类情况2对模式是线性可分的可能性比多类情况1更大一些。</p>
<h4 id="多类情况3"><a href="#多类情况3" class="headerlink" title="多类情况3"></a>多类情况3</h4><p>是多类情况2没有不确定区域的特例。特例引入的条件是：</p>
<p>$d_{ij}(x)=d_i(x)-d_j(x)=(W_i^T-W_j^T)x$ 因此当$d_{ij}(x)&gt;0$有$d_i(x)&gt;d_j(x) \quad\forall j\neq i$ 这时不存在不确定区域，判别界面相交于一点。此时M类情况，对应有M个判别函数。</p>
<p>根据上面的条件，只有当$d_i(x)$大于所有$d_j(x)$的时候，才会有所有的$d_{ij}(x)&gt;0$。因此把样本带入所有的判别函数，选择值最大的一个作为划分的类。</p>
<p>该分类的特点是把M类情况分成M-1个两类问题。 </p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1577451087/blog/machine-learning/%E5%A4%9A%E7%B1%BB%E6%83%85%E5%86%B53.jpg" alt=""></p>
<h2 id="广义线性判别函数"><a href="#广义线性判别函数" class="headerlink" title="广义线性判别函数"></a>广义线性判别函数</h2><h3 id="出发点"><a href="#出发点" class="headerlink" title="出发点"></a>出发点</h3><ul>
<li>线性判别函数简单，容易实现；</li>
<li>非线性判别函数复杂，不容易实现；</li>
<li>若能将非线性判别函数转换为线性判别函数，则有利于模式分类的实现。</li>
</ul>
<h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>如果在低维度中不是线性可分的（判别函数在低维度中是非线性的），将样本用非线性的变换（比如多项式）映射到能够线性可分的高维度中去，然后在高维度中用线性判别函数来分类。</p>
<p>对于原来的样本x向量，可以变换为：</p>
<script type="math/tex; mode=display">
x^*=(f_1(x), f_2(x),\dots,f_k(x))^T, k>n</script><h3 id="总项数"><a href="#总项数" class="headerlink" title="总项数"></a>总项数</h3><p>对于n维x向量，若用r次多项式，d(x)的权系数的总项数为：</p>
<script type="math/tex; mode=display">
N_w=C_{n+r}^{r}=\dfrac{(n+r)!}{n!r!}</script><h2 id="Fisher-线性判别"><a href="#Fisher-线性判别" class="headerlink" title="Fisher 线性判别"></a>Fisher 线性判别</h2><p>求一个投影后能够分开的权重法向量$W$,从d维空间变换到一维空间。</p>
<h3 id="基本参量"><a href="#基本参量" class="headerlink" title="基本参量"></a>基本参量</h3><h4 id="d维x空间"><a href="#d维x空间" class="headerlink" title="d维x空间"></a>d维x空间</h4><ol>
<li>样本的均值向量</li>
<li>样本类内离散度矩阵 和总样本类内离散度矩阵</li>
<li>样本类间离散度矩阵</li>
</ol>
<h4 id="一维Y空间"><a href="#一维Y空间" class="headerlink" title="一维Y空间"></a>一维Y空间</h4><ol>
<li>样本的均值向量</li>
<li>样本类内离散度矩阵 和总样本类内离散度矩阵</li>
</ol>
<h2 id="感知器"><a href="#感知器" class="headerlink" title="感知器"></a>感知器</h2><h3 id="两类"><a href="#两类" class="headerlink" title="两类"></a>两类</h3><p>使用误分类的样本点到判别平面的距离作为目标函数，要求最终下降到0，通过随机梯度下降法来进行优化。每次向输入的误分类点移动一步（加上对应的值），直到对于所有的样本点都可以正确分类。负梯度的方向就是样本向量的方向。</p>
<p>感知器的解存在很多，加上b偏置就是余量。距离理解成置信度</p>
<h3 id="多类情况3-1"><a href="#多类情况3-1" class="headerlink" title="多类情况3"></a>多类情况3</h3><p>有多少类就有多少个判别函数，要求对于第i类样本其对应的$d_i(x)$判别函数一定最大的（不存在相等的），如果不是最大的则其对应的加上这个样本$x_i$超过的减去这个样本。</p>
<p>每一次同一个样本要带入所有判别函数，然后再对所有的判别函数进行调整。</p>
<h2 id="梯度法"><a href="#梯度法" class="headerlink" title="梯度法"></a>梯度法</h2><p>确定好了对误分类敏感的准则函数（目标函数），先对准则函数求得其梯度向量，对于误分类的样本点朝着负梯度方向来更新权重系数。</p>
<p>感知器算法选择准则函数是误分类点的函数间隔$y_i(wx_i+b)$</p>
<p>选择不同的准则函数，得到的梯度也是不一样的，因此权重的更新的值也不一样。</p>
<h2 id="最小平方误差算法"><a href="#最小平方误差算法" class="headerlink" title="最小平方误差算法"></a>最小平方误差算法</h2><p>最小平方误差(LMSE)算法，除了对可分模式是收敛的以外，对于类别不可分的情况也能指出来。 </p>
<h3 id="分类器的不等式方程"><a href="#分类器的不等式方程" class="headerlink" title="分类器的不等式方程"></a>分类器的不等式方程</h3><p>将所有样本增广后写入一个矩阵X，X是N*（D+1）的矩阵，每一行是一个具体的样本，N是总共的样本数，其中负类的样本乘以-1规范化，因此判别界面的要求是：$Xw&gt;0$ 其中W是一个D+1维度的权向量，乘法得到的向量是一个N行的列向量，对应的是没一个样本的$w^Tx$ 。</p>
<h3 id="H-K算法"><a href="#H-K算法" class="headerlink" title="H-K算法"></a>H-K算法</h3><p>求解的是$Xw=b$ 式子中$b=(b_1,b_2,\dots,b_n)^T$的所有分量都是正值。因为X是一个$N*(D+1)$ 通常行大于列的长方阵，属于超定方程，因此一般情况下 ，不存在确定解，但是可以求线性最小二乘解 。</p>
<p>将b看作真实的值，因此平方误差就是$(w^Tx-b)^2$ 。</p>
<p>所以可以将准则函数定义为：</p>
<script type="math/tex; mode=display">
J(w,x,b)=\dfrac{1}{2}\sum_{i=1}^{n}(w^Tx_i-b_i)^2=\dfrac{1}{2}\sum_{i=1}^{n}||w^Tx_i-b_i||^2=\dfrac{1}{2}(Xw-b)^T(Xw-b)</script><h2 id="势函数"><a href="#势函数" class="headerlink" title="势函数"></a>势函数</h2><p>非线性分类的方法。</p>
<p>判别函数由样本向量的势函数产生。每个样本点对应与一个位置所以对应一个势能。</p>
<p>在第k步迭代时的积累位势决定于在该步前所有的单独势函数的累加 。</p>
<p>以K(x)表示积累位势函数，若加入的训练样本xk+1是错误分类，则积累函数需要修改，若是正确分类，则不变。 </p>
<p>两类问题$\omega_1$的样本产生的势能为正，$\omega_2$样本产生的势能为负。</p>
<p>迭代的时候，只有错分的样本才会积累势能，正势能的如果被错分（带入积累势能函数中为负）则积累的势能需要加上这个点的势能。反之负势能的如果被错分（代入积累势能函数中为正），则积累势能减去这个点的势能。</p>
<p>积累势能函数由误分类的样本的势函数累加得到。</p>
<p>第一类势函数：可用对称的有限多项式展开。</p>
<script type="math/tex; mode=display">
K(x,x_k)=\sum_i=1^m\phi_i(x)\phi_i(x_k)</script><p>注意是先乘了再相加，第二项的$\phi_i(x_k)$可以理解成每个样本点对应的权重</p>
<p>最终的迭代关系为：</p>
<script type="math/tex; mode=display">
d_{k+1}(x)=\sum_{i=1}^mC_i(k+1)\phi_i(x)\\
C_i(k+1)=C_i(k)+r_{k+1}\phi_i(x_{k+1})</script><p>势函数确定了，每个样本都是同样的势函数形式，但是判别函数也就是积累势函数随着样本的加入而在不断的更新，特别是误分类的样本会更新势函数。</p>
<p>第二类势函数：选择双变量<strong>x</strong>和<strong>x</strong>k的对称函数作为势函数，即K(<strong>x</strong>, <strong>x</strong>k) = K(<strong>x</strong>k, <strong>x</strong>)，并且它可展开成无穷级数 。如下三种势函数</p>
<script type="math/tex; mode=display">
K(x,x_k)=e^{-\alpha||x-x_k||^2}\\
K(x,x_k)=\frac{1}{1-\alpha||x-x_k||^2} \quad \alpha >0\\
K(x,x_k)=\dfrac{sin\alpha||x-x_k||^2}{\alpha||x-x_k||^2}</script>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>课程</tag>
        <tag>模式识别</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统</title>
    <url>/2019-11-19-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.html</url>
    <content><![CDATA[<h1 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>推荐的基本思路是根据历史数据比如用户以往的喜好或者相似用户的喜好来预测用户未来的喜好。基本假设是：（1）用户的喜好是会保持的并且随着时间而缓慢改变。（2）拥有相同品味的用户对与一个项目的打分也是相似的。因此可以根据以往的历史数据来进行推荐。</p>
<p>与搜索的不同：搜索的数据不是根据用户的资料而定制的，给出的答案是与搜素查询语句相关联的结果。</p>
<a id="more"></a>
<h2 id="推荐算法"><a href="#推荐算法" class="headerlink" title="推荐算法"></a>推荐算法</h2><h3 id="基于内容的方法"><a href="#基于内容的方法" class="headerlink" title="基于内容的方法"></a>基于内容的方法</h3><p>基于内容的方法基于这样一个事实：一个用户的兴趣应该和被推荐物品的描述想匹配。核心思路是找到用户资料与物品之间的相似度，推荐相似度高的物品。</p>
<p>步骤为：</p>
<ul>
<li>描述将被推荐的物品I</li>
<li>建立用户个人资料U来描述用户喜欢的物品</li>
<li>比较物品和用户个人资料来决定推荐的物品</li>
</ul>
<p>用户个人资料与物品的描述可以根据关键词进行向量化。使用例如TF-IDF的方法将物品与个人资料向量话后，就可以计算两者的相似度了。</p>
<h4 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h4><p>使用预先相似度计算这两个向量的相似度。对与两个n维向量X和Y其余弦相似度为：</p>
<script type="math/tex; mode=display">
sim(X,Y)=\dfrac{X\cdot Y}{||X||\times ||Y||}</script><p>分子是点乘，$||X||=\sqrt {x_1^2+x_2^2+\dots+x_n^2}$ 是向量的欧几里得范数，也就是向量的长度。</p>
<h3 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h3><p>协同过滤Collaborative Filtering有两中假设</p>
<ul>
<li>基于用户的协同过滤：假设之前对各物品打分相似的用户对于一个新的物品的打分也是相似的</li>
<li>基于物品的协同过滤：假设两个物品的用户打分是相似的，那么一个新用户对二者的打分也是相似的。</li>
</ul>
<p>算法的步骤是：</p>
<ul>
<li>对所有的用户或物品根据他们与当前的用户或物品的相似度来进行赋权。</li>
<li>选择相邻的用户或者物品的一个自己作为推荐</li>
<li>对于一个用户对一个具体的物品的打分，使用相邻的用户对该物品（或者与该物品相似物品）的打分来预测。</li>
<li>推荐拥有最高预测打分值的物品。</li>
</ul>
<p>用户/物品之间的相似度可以用余弦相似度或者皮尔森相关系数。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574157674/blog/Data%20Mining/correlation_coefficient.jpg" alt=""></p>
<p>最终的打分的预测更新为：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574157674/blog/Data%20Mining/update_rating.jpg" alt=""></p>
<p>用户u对物品i的打分有原来的该用户的平均打分$\bar r_u$与和该用户最相似的用户一起决定。</p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p><img src="https://res.cloudinary.com/bravey/image/upload/v1574157674/blog/Data%20Mining/example_1.jpg" alt=""></p>
<p>为了预测Jane对Aladdin的打分，第一步首先计算每个用户对电影的平均打分。第二步计算其他4个用户与Jane 的相似度，使用的是余弦相似度并且其他用户对Aladdin的打分不算进去，因此用的是除了Aladdin外的3部电影来计算的。计算出来选择与Jane最相似的2个用户Joe、Jorge。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574157674/blog/Data%20Mining/example_2.jpg" alt=""></p>
<p>根据前面两部计算的结果计算Jane对Aladdin的打分。</p>
<h2 id="推荐系统的评测"><a href="#推荐系统的评测" class="headerlink" title="推荐系统的评测"></a>推荐系统的评测</h2><p>主要使用精确率，召回率，F1-Score3个指标来进行评价。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574157674/blog/Data%20Mining/presicion_recall.jpg" alt=""></p>
<p>F1-Score的计算方式为：</p>
<script type="math/tex; mode=display">
F1-Score = \dfrac{2Precison\times Recall}{Precision+Recall}</script><p>精确率表示所有相关的物品中被推荐出来的比例，召回率表示所有推荐的物品中实际相关的比例，F1-Score则是二者的总体评价。</p>
]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>课程</tag>
        <tag>国科大</tag>
        <tag>余弦相似度</tag>
        <tag>推荐</tag>
      </tags>
  </entry>
  <entry>
    <title>聚类方法</title>
    <url>/2019-11-19-%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95.html</url>
    <content><![CDATA[<h1 id="聚类方法"><a href="#聚类方法" class="headerlink" title="聚类方法"></a>聚类方法</h1><p>聚类属于无监督学习，因为输入的数据是没有标签的，通过算法每个样本自动的划分到相应的簇中。</p>
<h2 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h2><p>k均值是一种基于形心的技术。给定一个包含n 个数据对象的数据库，以及要生成的簇的数目k，一个划分类的算法将数据对象组织为k 个划分（k≤n），其中每个划分代表一个簇。通常会采用一个划分准则（经常称为相似度函数，similarity function），例如距离dist(i,j)，以便在同一个簇中的对象是“相似的”，而不同簇中的对象是“相异的”。</p>
<p>K-means把簇的形心定义为簇内点的均值，通过贪心的方法不断迭代形心的坐标，直到形心的坐标不再改变而结束迭代。</p>
<a id="more"></a>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>算法的步骤为：</p>
<ul>
<li>首先在数据集D中选择k个对象，每个对象代表一个簇的初试均值即形心。</li>
<li>对剩下的对象，根据其与这个k个形心的欧式距离将其分配到距离最近的形心的簇。</li>
<li>完成了分配后重新计算每个簇的均值中心点并更新。</li>
<li>使用更新后的均值中心点，重新分配每一个对象。</li>
<li>不断迭代，直到本轮的中心点与上一轮的相同，即本轮形成的簇与上一轮相同。</li>
</ul>
<h3 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h3><p>伪代码为：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574157674/blog/Data%20Mining/kmeans_code.jpg" alt=""></p>
<p>K-means对离群点敏感，因为当一个离群点被分配到一个簇的时候，可能会严重扭曲簇的均值。</p>
<p>k中心点算法是基于对象的，通过挑选实际的对象来代表簇，其余的对象被分配与其最为相似的代表对象所在的簇。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>《数据挖掘概念与技术》第3版 第十章聚类分析：基本概念和方法</p>
]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>课程</tag>
        <tag>国科大</tag>
        <tag>聚类</tag>
        <tag>K-Means</tag>
      </tags>
  </entry>
  <entry>
    <title>关联规则</title>
    <url>/2019-11-18-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99.html</url>
    <content><![CDATA[<h1 id="关联规则"><a href="#关联规则" class="headerlink" title="关联规则"></a>关联规则</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>关联规则挖掘Association rules mining 挖掘出数据库中的频繁模式，频繁项之间的关联规则。</p>
<a id="more"></a>
<p>关联规则的形式为</p>
<script type="math/tex; mode=display">
A\Rightarrow B[support=?, confident=?]</script><p>规则的支持度support和置信度confident分别反映出规则的有用性和确定性。定义为：</p>
<script type="math/tex; mode=display">
\begin{split}
support(A\Rightarrow B)&=P(A\bigcap B)=\dfrac{count(A\bigcap B)}{count(total)}\\
confident(A\Rightarrow B)&=P(B|A)=\dfrac{count(A\bigcap B)}{count(A)}
\end{split}</script><p>$A\bigcap B$ 表示A事件和B事件一起发生。使用比例的支持度称为相对支持度，使用直接的次数称为绝对支持度。当规则满足设定的最小支持度和最小置信度的时候，规则是强关联规则。强关联规则并不一定是有趣的，使用提升度lift来衡量两个事件之间的相关性</p>
<script type="math/tex; mode=display">
lift=\dfrac{P(A\bigcap B)}{P(A)P(B)}</script><p>当lift大于1的时候表示A事件和B时间是正相关的，A随着B的出现而出现，lift为1的时候二者是独立的，lift小与1的时候二者负相关，意味着一个的出现可能导致另一个不出现。</p>
<p>关联规则的挖掘分为两步：</p>
<ol>
<li>找出所有的频繁项集 一个详细的出现次数大于最小支持度的次数则是频繁的</li>
<li>由频繁项集产生强关联规则</li>
</ol>
<p>包含k个项的项集称之为k项集，如果集合中的项都是频繁的，那么集合称之为频繁k项集。</p>
<p>挖掘频繁项集的算法：Apriori和FP-Growth</p>
<h2 id="Apriori"><a href="#Apriori" class="headerlink" title="Apriori"></a>Apriori</h2><p>Apriori运用了一个先验规则：每个频繁项集的子集一定也是频繁的项集，使用这个规则来剪枝很多候选项（非频繁的子集其父集也一定不是频繁的所以可以不用考虑）。</p>
<p>算法步骤为：</p>
<ul>
<li>首先扫描一次数据库得到频繁1项集</li>
<li>根据频繁k项集$L_k$生成频繁k+1项集（连接步和剪枝步组成）<ul>
<li>先根据频繁k项集组合生成候选项</li>
<li>扫描数据库得到候选项的支持度</li>
<li>将不满足支持度的剔除</li>
</ul>
</li>
<li>当没有频繁项或者候选项可以生成的时候终止</li>
</ul>
<h3 id="连接步"><a href="#连接步" class="headerlink" title="连接步"></a>连接步</h3><p>为了找出$L_k$ ,通过$L_{k-1}$与自身连接产生候选k项集的集合$C_k$设$l_1$和$l_2$ 是$L_{k - 1}$ 中的项集。记号$l_i[j]$表示$l_i$ 的第j 项（例如，$l_1[k-2]$表示$l_1$ 的倒数第3 项）。为方便计，假定事务或项集中的项按字典次序排序即$l_i[k]&lt;l_i[k+1]$。执行连接$L_{k - 1}$ 与$L_{k - 1}$；其中，$L_{k - 1}$ 的元素是可连接的，如果它们前(k-2)个项相同；即，$L_{k - 1}$ 的元素l1 和l2 是可连接的，如果$(l_1 [1] = l_2 [1]) ∧ (l_1 [2]= l_2 [2]) ∧ … ∧ (l_1 [k-2] = l_2 [k-2]) ∧ (l_1 [k-1] &lt; l_2 [k-1])$。条件$(l_1 [k-1] &lt; l_2 [k-1])$是简单地保证不产生重复。连接$l_1$ 和$l_2$ 产生的结果项集是$l_1 [1] l_1 [2]… l_1 [k-1] l_2 [k-1]$。</p>
<p>举个例子假设频繁三项集有$l_1=[a,b,c], l_2=[a.b,d]$ 满足前缀相同最后一项不同所以可以产生候选的4项集：$C_4=[a,b,c,d]$ 把$l_2$的最后一项拼接到$l_1$后面。</p>
<h3 id="剪枝步"><a href="#剪枝步" class="headerlink" title="剪枝步"></a>剪枝步</h3><p>$C_k$是频繁k项集的$L_k$的父集，可以扫描数据库来确定每一个$C_k$中的计数。但是$C_k$可能很大，所涉及的计算量也就很大。为压缩$C_k$，可以用以下办法使用Apriori 的先验性质：任何非频繁的(k-1)-项集都不是可能是频繁k-项集的子集。因此，如果一个候选k-项集的(k-1)项子集不在$L_{k - 1}$ 中，则该候选也不可能是频繁的，从而可以由$C_k$ 中删除。这种子集测试可以使用所有频繁项集的散列树(hash tree)快速完成。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574157675/blog/Data%20Mining/Apriori_example.jpg" alt=""></p>
<p>如上图的例子，第一次扫描得到频繁1项集的候选项$C_1$，然后根据支持度去除支持度计数小于2的$\{D\}$ 得到$L_1$，然后$L_1$中的各项进行连接步操作得到$C_2$，$C_2$ 执行剪枝步操作，因为所有项的子集都在$L_1$中所以不需要剪枝。进行第二次扫描得到$C_2$各项的支持度计数，然后把低于最小支持度的$\{A,B\},\{A,E\}$ 两项删除从而得到频繁2项集$L_2$。根据$L_2$执行连接步(只能由{B,C} 和{B, E}连接，其他的前缀不同)生成的$C_3=\{B,C,E\}$ 因为对应的3个子集{B,C},｛B,E｝和{C, E}都在频繁2项集中，所以也不需要剪枝。第三次扫描后得到$C_3$中各项的支持度计数，因为大于最小支持度计数，因此不需要删去得到了频繁3项集，因为频繁3项集只有一项无法进行连接步操作生成候选项集，所以算法终止。</p>
<h3 id="产生关联规则"><a href="#产生关联规则" class="headerlink" title="产生关联规则"></a>产生关联规则</h3><p>以上图为例，得到了频繁3项集$L_3={\{B,C,E\}}$后，对其所有非空子集$\{B\},\{C\},\{E\},\{B,E\},\{B,C\},\{C,E\}$ 计算对应的关联规则$l_1 \Rightarrow l_2\space or \space l_2 \Rightarrow l_1 $的置信度，如果满足最小置信度阈值则是强关联规则。</p>
<h3 id="算法伪代码"><a href="#算法伪代码" class="headerlink" title="算法伪代码"></a>算法伪代码</h3><p><img src="https://res.cloudinary.com/bravey/image/upload/v1574157676/blog/Data%20Mining/Apriori_code.jpg" alt=""></p>
<h3 id="改进算法"><a href="#改进算法" class="headerlink" title="改进算法"></a>改进算法</h3><ul>
<li>Partition ：仅扫描数据库两次</li>
<li>DHP：降低候选项的数目</li>
<li>DIC：降低扫描次数</li>
</ul>
<h3 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h3><p>Apriori算法需要多次扫描数据库这带来的开销很大，同时会生成大量的候选项集并做子集测试也造成了很大的计算开销。</p>
<h2 id="FP-Growth"><a href="#FP-Growth" class="headerlink" title="FP-Growth"></a>FP-Growth</h2><p>频繁模式增长FP-Growth将代表频繁项集的数据库压缩到一颗频繁模式树Frequent Patterns (FP树)中。</p>
<h3 id="FP树的构建"><a href="#FP树的构建" class="headerlink" title="FP树的构建"></a>FP树的构建</h3><ul>
<li>首先扫描一次数据库，找到频繁一项集。</li>
<li>根据支持度计数降序排列频繁项记为L</li>
<li>创建一个根节点，并标记为null</li>
<li>再次扫描数据库，将每个事务按照L中的顺序排列。为每个事务新建一个分支<ul>
<li>如果事务中的项已经在分支中，则分支上的节点计数+1</li>
<li>如果项不在分支中，则在前缀路径下新建一个节点，计数为1</li>
</ul>
</li>
<li>建立一个项头表把FP树中的每一项的节点相连。</li>
</ul>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574157675/blog/Data%20Mining/FP_tree_construct.jpg" alt="FP树"></p>
<p>以上图为例，第一次扫描后得到的频繁一项集为$L_1=\{f:4,c:4,a:3,b:3,m:3,p:3\}$ 已经按照支持度计数降序排列为F-list。第二次扫描数据库，并按照F-list的顺序将每个事务排列（注意：低于支持度计数的项已经被删除了）。建立一个根节点并标记为null,第一个事务是$\{f,c,a,m,p\}$</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574157675/blog/Data%20Mining/fp_tree_1.jpg" alt=""></p>
<p>因为是第一个事务所以没有前缀路径可以共用，每个项都需要建立一个节点。第二个事务是$\{f,c,a,b,m\}$因为$\{f,c,a\}$ 是已经有的路径，所以对应的在这3个节点计数+1，$\{b,m\}$ 没有可以共享的路径所以需要新建立这两个节点。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574157674/blog/Data%20Mining/fp_tree_2.jpg" alt=""></p>
<p>当所有事务都被加入到FP树后建立项头表将相同的项给链接起来就可以得到前面的完整的FP树。</p>
<h3 id="条件模式基的构造"><a href="#条件模式基的构造" class="headerlink" title="条件模式基的构造"></a>条件模式基的构造</h3><p>从项头表中具有最低的支持度计数的项开始构造每一个项的条件模式基Conditional Pattern Base。一个项的条件模式基定义为以这个项为后缀的前缀路径。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574157675/blog/Data%20Mining/conditional_patter_base.jpg" alt=""></p>
<p>以上图为例，先构造p的条件模式基，在FP树中，以p为后缀的路径为$fcam:2 和cb:1$ 后面的计数是根据每条路径的后缀的计数来确定的因此第一条$fcamp$路径的计数为2而$cbp$为1。 依次可以得到每个项的条件模式基。对每一个项的条件模式基都可以按照前述的FP树的构造方法得到其对应的条件FP树。以m的条件模式基构建的条件FP树为：<br><img src="https://res.cloudinary.com/bravey/image/upload/v1574157675/blog/Data%20Mining/m_conditional_fptree.jpg" alt=""></p>
<p>因为b的支持度计数只有1所以删去。</p>
<h3 id="频繁模式的产生"><a href="#频繁模式的产生" class="headerlink" title="频繁模式的产生"></a>频繁模式的产生</h3><p>根据条件FP树的路径产生频繁模式，路径的每个非空子集与对应的项组合产生对应的频繁模式。比如m的FP树路径为：$\{fca\}$ 对应的子集有$\{f\},\{c\},\{a\}, \{f,c\},\{f,a\},\{a,c\},\{f,c,a\}$与m组合后得到的频繁模式有：$\{f,m\},\{c,m\},\{a.m\}, \{f,c,m\},\{f,a.m\},\{a,c,m\},\{f,c,a,m\}$</p>
<h3 id="FP树的优点"><a href="#FP树的优点" class="headerlink" title="FP树的优点"></a>FP树的优点</h3><p>完整的保留了频繁模式挖掘的所有信息，不损失每个事务可能的频繁项集。将发现长频繁模式的问题转换成递归地发现一些短模式，然后与后缀连接。它使用最不频繁的项作后缀，提供了好的选择性。该方法大大降低了搜索开销。</p>
<p>FP-Growth采用分治的方法专注于小的数据集，不产生候选项集，只扫描两次数据库。因此性能比Apriori算法好。</p>
<h3 id="FP-Growth-伪代码"><a href="#FP-Growth-伪代码" class="headerlink" title="FP-Growth 伪代码"></a>FP-Growth 伪代码</h3><p><img src="https://res.cloudinary.com/bravey/image/upload/v1574157675/blog/Data%20Mining/FP-Growth_code.jpg" alt=""></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>《数据挖掘概念与技术》第3版 第六章挖掘频繁模式、关联和相关性：基本概念和方法</p>
]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>课程</tag>
        <tag>国科大</tag>
        <tag>关联规则</tag>
        <tag>Apriori</tag>
        <tag>FP-Growth</tag>
      </tags>
  </entry>
  <entry>
    <title>分类与预测</title>
    <url>/2019-11-17-%E5%88%86%E7%B1%BB%E4%B8%8E%E9%A2%84%E6%B5%8B.html</url>
    <content><![CDATA[<h1 id="分类与预测"><a href="#分类与预测" class="headerlink" title="分类与预测"></a>分类与预测</h1><h2 id="分类与预测的差别"><a href="#分类与预测的差别" class="headerlink" title="分类与预测的差别"></a>分类与预测的差别</h2><p>分类对给定的数据集一般是离散的，确定这些数据对应类别。而预测是对连续的数据，根据历史数据来预测未知的数据或者缺失值等。</p>
<p>分类的过程分为两步：</p>
<ul>
<li><p>模型构建</p>
<p>使用训练数据集对模型进行训练，模型可以被表示为一些分类的规则集合，决策树或者是数学公式。</p>
</li>
<li><p>模型使用</p>
<p>先使用模型来对测试数据进行分类，如果准确率能够接受则使用模型去对没有标注过的数据进行分类。</p>
</li>
</ul>
<p>分类属于有监督学习，训练的数据是经过标注的。聚类属于无监督学习训练数据未经过标注不知道样本的标签。</p>
<a id="more"></a>
<p>在进行分类和预测之前需要对数据进行预处理包括数据清洗来处理噪声和缺失值，相关性分析来进行特征提取，数据转换比如归一化等。</p>
<p>对于分类方法的评测指标有：准确性，速度，鲁棒性，可规模性（硬盘数据），可解释性等</p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>决策树是一个类似于流程图的树结构；其中，每个内部结点表示在一个属性上的测试，每个分枝代表一个测试输出，而每个树叶结点代表类或类分布。树的最顶层结点是根结点。给定数据经过决策树不同节点的决策最终走到叶子节点，从而完成了对数据的分类。决策树不需要任何领域知识也不需要参数，适合探测式知识发现。</p>
<p>决策数的构建分为两步：</p>
<ul>
<li>树的构建 从根节点开始递归的选择属性进行建树</li>
<li>剪枝 ：减去反映噪声或者离群点的分枝</li>
</ul>
<h3 id="构建决策树"><a href="#构建决策树" class="headerlink" title="构建决策树"></a>构建决策树</h3><p>算法伪代码为：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574086256/blog/Data%20Mining/decision_tree_code.jpg" alt=""></p>
<p>三个参数D是输入的数据集,attribute_list是输入数据中的属性集合和Atrribute_selection_method是指定选择属性的启发式过程，可以选择信息增益或者是基尼指数Gini index。</p>
<p>步骤为：</p>
<ol>
<li><p>首先从根节点N开始，根节点中的数据是原始的需要分类的原始数据集。</p>
</li>
<li><p>如果D中的数据都属于同一类，那么根节点N变成叶子节点，标记为这一类。</p>
</li>
<li><p>否则，调用Atrribute_selection_method来选择最佳的分裂属性，并给出分裂子集。理想情况下希望分裂子集尽可能的纯，也就是希望分裂自己尽可能的都属于同一类。</p>
</li>
<li><p>用选出来的分裂属性在节点N上进行划分，并输出划分的数据子集。并把分裂属性从属性列表中删除。</p>
<ol>
<li>选出来的属性是离散值的话，有多少个取值就产生多少个分支</li>
<li>连续值的话选择一个分裂值，大于这个分裂值为一个分支，小于等于为另外一个分支</li>
<li>离散值而且必须是二叉树的话：属于这个值为一个分支，否则为另外一个分支</li>
</ol>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574086256/blog/Data%20Mining/split_choice.jpg" alt=""></p>
</li>
<li><p>对于每一个删除了最佳分裂属性的输出的子集$D_j$ 递归的调用算法。</p>
</li>
<li><p>递归的终止条件：</p>
<ol>
<li>数据集D都属于同一类</li>
<li>属性列表为空，使用数据集中的多数类来标记节点。<strong>（多数投票）</strong></li>
<li>如果一个分支的数据$D_j$为空，则新增加一个叶子节点，用父节点数据集D中的多数类来标记它。</li>
</ol>
</li>
<li><p>返回决策数的节点N</p>
</li>
</ol>
<h3 id="属性选择度量"><a href="#属性选择度量" class="headerlink" title="属性选择度量"></a>属性选择度量</h3><p>很明显，决策树的关键点是怎么选择分裂的属性。有如下三种方式。</p>
<h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>ID3这种决策树方法使用。</p>
<p>先计算数据集D的熵：</p>
<script type="math/tex; mode=display">
Info(D)=-\sum_{i=1}^{m}p_ilog_2(p_i)</script><p>假设数据集中D有m个类别${C_1,C_2\dots C_m}$，那么每一类的概率可以用所占的比例$p_i=\dfrac{count(C_i)}{count(D)}$来估计。</p>
<p>假设属性A 具有v 个不同值${a_1 ,…, a_v}$。可以用属性A 将S 划分为v 个子集${S_1 ,…, S_v}$；其中，<br>$S_j$ 包含S 中这样一些样本，它们在A 上具有值$a_j$。如果A 选作测试属性,则获得的信息增益,也就是划分成了子集后的熵为：</p>
<script type="math/tex; mode=display">
Info_A(D)=\sum_{j=1}^{v}\dfrac{|D_j|}{|D|}\times Info(D_j)</script><p>信息增益定义为原来的的信息需求（近基于类比例）与新的信息需求（对A划分后）之间的差：</p>
<script type="math/tex; mode=display">
Gain(A)=Info(D)-Info_A(D)</script><p>具有最高信息增益的属性是最佳分裂属性。可以这样理解，划分后越纯那么整个状态越不混乱，也就是熵越低。所以选择划分后熵最低而信息增益最高的属性。</p>
<h4 id="信息增益率"><a href="#信息增益率" class="headerlink" title="信息增益率"></a>信息增益率</h4><p>信息增益偏向有许多输出的测试，它倾向于选择具有大量值的属性。比如按照唯一的标识符id来划分，每个划分出来的都只包含一个数据都是纯的，但是这样的划分显然没有作用。</p>
<p>C4.5使用增益率来进行选择分裂属性。增益率用分裂信息值将信息增益归一化。</p>
<script type="math/tex; mode=display">
SplitInfo_A(D)=-\sum_{j=1}^{v}\dfrac{|D_j|}{|D|}\times log_2(\dfrac{|D_j|}{D})</script><p>分裂信息值代表由训练数据集D根据属性A划分v个分区后，这v个分区的熵。前面的$Info_A(D)$是对每个分区还计算了一下分区里面的熵然后进行加权，而这里是全局地看待这v个分局而进行的计算。</p>
<p>增益率定义为：</p>
<script type="math/tex; mode=display">
GainRate(A)=\dfrac{Gain(A)}{SplitInfo_A(D)}</script><p>选择具有最大增益率的属性作为分裂属性。</p>
<h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h4><p>基尼指数在CART中使用，基尼指数度量数据集的不纯度，定义为：</p>
<script type="math/tex; mode=display">
Gini(D)=1-\sum_{i=1}^{m}p_i^2</script><p>$p_i=\dfrac{|C_i|}{|D|}$依然用每一类的比例来估计。基尼指数考虑每个属性的二元划分，根据属性A划分成两个两个子集$D_1,D_2$在这种划分下的基尼指数为：</p>
<script type="math/tex; mode=display">
Gini_A(D)=\dfrac{|D_1|}{|D|}Gini(D_1)+\dfrac{|D_2|}{|D|}Gini(D_2)</script><p>选择具有最低的基尼指数作为分裂属性。  </p>
<h3 id="决策树决策规则"><a href="#决策树决策规则" class="headerlink" title="决策树决策规则"></a>决策树决策规则</h3><p>构建好了决策树后，决策规则使用IF-THEN的语句来表示。<img src="https://res.cloudinary.com/bravey/image/upload/v1574086256/blog/Data%20Mining/decision_tree_rules.jpg" alt=""></p>
<p>比如上图的决策规则是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">IF age is Middle_aged THEN buys_computer &#x3D; yes</span><br><span class="line">IF age is youth AND student &#x3D; no THEN buys_computer &#x3D; no</span><br><span class="line">IF age is youth AND student &#x3D; yes THEN buys_computer &#x3D; yes</span><br><span class="line">IF age is senior AND credit_rating&#x3D;excellent THEN buys_computer &#x3D; no</span><br><span class="line">IF age is senior AND credit_rating&#x3D;fair THEN buys_computer &#x3D; yes</span><br></pre></td></tr></table></figure>
<p>有多少个叶子节点相应的有多少决策规则，在到达叶子节点的路径上有多少个节点就有多少个并列的条件。</p>
<h3 id="过拟合与树剪枝"><a href="#过拟合与树剪枝" class="headerlink" title="过拟合与树剪枝"></a>过拟合与树剪枝</h3><p>在创建决策树时，由于数据中的噪声和离群点，许多分支反映的是训练数据中的异常。使用剪枝的方法来处理这种过拟合的问题。</p>
<h4 id="先剪枝"><a href="#先剪枝" class="headerlink" title="先剪枝"></a>先剪枝</h4><p>先剪枝prepruning方法提前停止树的创建。比如设定一个阈值当信息增益等度量超过阈值则分裂，不超过就停止分裂，用投票的方法确定标签。但是阈值的设置比较困难，所以不常用。</p>
<h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><p>后剪枝postpruning方法从完全创建好了的决策树进行剪枝。剪去后也依然使用投票的方法来确定标签。</p>
<p>CART使用<strong>代价复杂度</strong>剪枝算法，C4.5使用<strong>悲观剪枝</strong>的方法。二者都是根据错误率，简单说就是如果减去这个分支后错误率提升不大就可以减去。</p>
<h3 id="决策树评价"><a href="#决策树评价" class="headerlink" title="决策树评价"></a>决策树评价</h3><p>比其他的分类方法有着相对更快的学习速度，能够转换成容易理解的决策规则，准确率也能接受，而且也可以适用于大规模的数据。</p>
<h2 id="贝叶斯分类"><a href="#贝叶斯分类" class="headerlink" title="贝叶斯分类"></a>贝叶斯分类</h2><p>朴素贝叶斯分类发假定一个属性值在给定类上的影响独立于其他属性的值，这个假定称为类条件独立性。做这个假定是为了简化计算，因而称之为朴素的。</p>
<h3 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h3><p>贝叶斯定理用来求解后验概率。公式为：</p>
<script type="math/tex; mode=display">
P(H|X)=\dfrac{P(H)P(X|H)}{P(X)}</script><p>X是数据样本，分类未知。H是对X的分类的假设，比如X属于C类。P(H)是猜测的类的概率，P(X)是观察到的样本X的概率，而P(H|X)是在给定H的假设下，观察到X样本的概率。</p>
<h3 id="朴素贝叶斯分类"><a href="#朴素贝叶斯分类" class="headerlink" title="朴素贝叶斯分类"></a>朴素贝叶斯分类</h3><h4 id="工作过程"><a href="#工作过程" class="headerlink" title="工作过程"></a>工作过程</h4><ol>
<li><p>训练数据集D中一个样本X向量用一个n维的列向量$[x_1,x_2\dots x_n]^T$来表示它对应的n个属性$[A_1,A_2\dots A_n]$的测量值。</p>
</li>
<li><p>假定有m个类$C_1,C_2\dots C_n$,将X预测为拥有最高后验概率的类。</p>
<script type="math/tex; mode=display">
max(P(C_i|X))=max(\dfrac{P(X|C_i)P(C_i)}{P(X)})</script></li>
<li><p>因为$P(X)$是常数，所以只用最大化$P(X|C_i)P(C_i)$ $P(C_i)$用类的频率来估计</p>
</li>
<li><p>在类条件独立的假定下有：</p>
<script type="math/tex; mode=display">
P(X|C_i)=\prod_{k=1}^nP(x_k|C_i)=P(x_1|C_i)P(x_2|C_i)\dots P(x_n|C_i)</script><p>$x_k$表示样本X向量对应的第k个属性$A_k$的值。</p>
<ol>
<li><p>如果$A_k$是分类属性，那么$P(x_k|C_i)$是D中属性$A_k$的值为$x_k$的$C_i$类的元组数除以D中$C_i$的数目 </p>
</li>
<li><p>如果$A_k$是连续值，则假定连续值服从均值为$\mu$,标准差为$\sigma$的高斯分布。</p>
<script type="math/tex; mode=display">
g(x,\mu,\sigma)=\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}</script><p>所以可以得到：</p>
<script type="math/tex; mode=display">
P(x_k|C_i)=g(x_k,\mu_{C_i},\sigma_{C_i})</script></li>
</ol>
</li>
<li><p>对每一类都分别计算$P(X|C_i)P(C_i)$ 最后选择最大的作为这个样本的分类。</p>
</li>
</ol>
<h4 id="零概率值的解决"><a href="#零概率值的解决" class="headerlink" title="零概率值的解决"></a>零概率值的解决</h4><p>在做连乘的时候如果有一个的概率为0，那么整个计算结果都为0。可以假设训练数据集很大，将每个取值的计数都加1而造成的概率估计可以忽略不计。这样就可以避免出现零概率的情况，称之为<strong>拉普拉斯估计或这拉普拉斯校准法</strong></p>
<h3 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h3><p>朴素贝叶斯的优势：比较容易实现，在大多数情况可以取得好结果。缺点是：因为做了属性互相独立的假设，因此会降低准确率。而且实际上，大部分属性之间并不互相独立。</p>
<p>可以使用贝叶斯信念网络来处理。</p>
<h2 id="BP神经网络"><a href="#BP神经网络" class="headerlink" title="BP神经网络"></a>BP神经网络</h2><h3 id="多层前馈神经网络"><a href="#多层前馈神经网络" class="headerlink" title="多层前馈神经网络"></a>多层前馈神经网络</h3><p>后向传播算法Back propagation在多层前馈神经网络上学习。由输入层、一个或多个隐藏层和一个输出层组成。</p>
<p><img src="/2019-11-17-%E5%88%86%E7%B1%BB%E4%B8%8E%E9%A2%84%E6%B5%8B.htm/Users\BraveY\Documents\BraveY\blog\images\data mining\multi_layer_feed_forward_network.jpg" alt=""></p>
<p>输入层的单元称作输入单元，隐藏层和输出层单元称作输出单元或者神经节点。输入向量输入进输入单元，然后加权同时地提供给隐藏层的输出单元。在全连接的情况下每一层单元与前后层单元都会有相应的权重，因为权重不会回馈到输入节点，所以称为前馈的。因此一次输入就完成输出。</p>
<h3 id="网络拓扑的定义"><a href="#网络拓扑的定义" class="headerlink" title="网络拓扑的定义"></a>网络拓扑的定义</h3><p>应该设计多少层隐藏层，每一层有多少单元，输入层单元应该有多少，是否全连接等都是网络拓扑需要考虑的。</p>
<p>需要先对输入值进行归一化，将值落入[0,1]之间。离散值的属性可以进行重新编码，让每一个域值都有一个输入单元。如果属性A有3个可能的值${a_0, a_1, a_2}$那么就需要设计3个输入单元$I_0, I_1,I_2$，输入单元$I_1$为1对应属性$A=a_1$。</p>
<p>如果进行分类的话如果只有两类那么输出单元只用一个就可以了，多于2类的情况每一类都有一个输出单元。</p>
<p>隐藏层单元数目的设计，没有明确的规定，通过反复实验的过程，来确定。一般选择一个隐藏层就可以了，权重的初始值也会影响结果的准确率。</p>
<h2 id="后向传播"><a href="#后向传播" class="headerlink" title="后向传播"></a>后向传播</h2><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>后向传播通过迭代地处理训练数据，把每个样本的网络预测值与实际已知的目标值相比较。对每个训练样本，修改网络的权重使得网络预测和实际目标值之间的均方误差最小。修改后向进行，从输出层一直传递到输入层的，因此叫做后向传播。</p>
<p>算法伪代码为：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574086256/blog/Data%20Mining/bp_code.jpg" alt=""></p>
<p>步骤为：</p>
<p><strong>初始化权重：</strong>权重被初始化为小随机数（比如-1到1，或者-0.5到0.5），偏倚bias也初始化为小随机数。</p>
<p><strong>向前传播输入</strong>：输入单元的输出就是输入值。隐藏层和输出层单元的净输入用上一层输入的线性组合计算。输出单元的输出用激活函数计算，激活函数是logistic或这S型sigmoid函数,激活函数将较大的输入值域映射到区间[0,1]</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574086256/blog/Data%20Mining/bpnn_io.jpg" alt=""></p>
<p>隐藏层或者输出层单元j其输入为如下，其中$w_{ij}$是上层单元i到单元j的权重，$O_i$是上一层单元的输出，$\theta_j$是单元j的偏倚。偏倚用来充当阈值，改变单元的活性</p>
<script type="math/tex; mode=display">
I_j=\sum_{i}w_{ij}O_i+\theta_j</script><p>单元j的输出$O_j$为：</p>
<script type="math/tex; mode=display">
O_j=\dfrac{1}{1+e^{I_j}}</script><p><strong>向后传播误差</strong>：通过更新权重和反映网络预测误差的偏倚，向后传播误差。对于输出层单元j，误差如下，其中$O_j$ 是单元j的实际输出， $T_j$ 是j给定训练样本的实际目标值，$O_j(1-O_j)$是逻辑斯提函数的导数。</p>
<script type="math/tex; mode=display">
Err_j=O_j(1-O_j)(T_j-O_j)</script><p>隐藏层单元j的误差，考虑下一层中j连接的单元的误差加权和。误差如下：</p>
<script type="math/tex; mode=display">
Err_j=O_j(1-O_j)\sum_kErr_kw_{jk}</script><p>其中$w_{jk}$是下一层单元k与单元j的连接权重，而$Err_k$是单元k的误差。</p>
<p>更新权重和偏倚，以反映误差的传播。权重更新公式如下，其中$\Delta w_{ij}$是权重的改变量。</p>
<script type="math/tex; mode=display">
\begin{split}
\Delta w_{ij}&=l\times Err_jO_i\\
w_{ij}&=w_{ij}+\Delta w_{ij}
\end{split}</script><p>变量l是<strong>学习率</strong>，通常去[0,1]之间的常数值。学习率帮助避免陷入决策空间的局部极小，并有助于找到全局最小。学习率太低的话，学习将进行的很慢。学习率太高的话，可能出现在不适当的解之间的摆动。一种调整规则是将学习率设置为迭代次数的倒数。</p>
<p>偏倚的更新公式为：其中$\Delta \theta_{ij}$是偏倚的改变量。</p>
<script type="math/tex; mode=display">
\begin{split}
\Delta \theta_{ij}&=l\times Err_j\\
\theta_{ij}&=\theta_{ij}+\Delta \theta_{ij}
\end{split}</script><p>更新的策略有实例更新：每处理一个样本就更新权重和偏倚。周期更新为处理玩所有样本后再更新。实例更新通常产生更准确的结果。</p>
<p><strong>终止条件：</strong></p>
<ul>
<li>前一周期所有的$\Delta w_{ij}$都太小，小于某个阈值</li>
<li>前一周其误分类的数据百分比小于某个阈值</li>
<li>超过预先指定的周期数。实践中权重收敛可能需要数十万个周期。</li>
</ul>
<p>和决策树一样，神经网络也可以剪枝，去除那些影响很小的连接。</p>
<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p>优点：</p>
<ul>
<li>对噪声的高容忍</li>
<li>对连续值的良好适配</li>
<li>对真实数据的良好处理</li>
<li>效果很好</li>
</ul>
<p>缺点：</p>
<ul>
<li>很长的训练时间</li>
<li>要求大量的参数</li>
<li>解释性很差</li>
</ul>
<h2 id="其他分类方法"><a href="#其他分类方法" class="headerlink" title="其他分类方法"></a>其他分类方法</h2><p>包括KNN、集成学习比如（Baggin、Boosting）</p>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>线性回归模型、多项式回归模型、广义线性模型、对数线性模型（针对分类数据）</p>
<h2 id="准确率和误差的衡量"><a href="#准确率和误差的衡量" class="headerlink" title="准确率和误差的衡量"></a>准确率和误差的衡量</h2><p>使用混淆矩阵confusion matrix来表示分类的情况。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1574086256/blog/Data%20Mining/confusion_matrix.jpg" alt=""></p>
<p>准确率和错误率表示为：</p>
<script type="math/tex; mode=display">
\begin{split}
Accuracy&=\dfrac{(true\_pos+true\_neg)}{pos+neg}\\
Error rate&=1-Accuracy
\end{split}</script><p>还有其他几个指标</p>
<script type="math/tex; mode=display">
\begin{split}
sensitivity&=\dfrac{true\_pos}{pos} /* 真阳的识别率*/\\
specificity&=\dfrac{true\_neg}{neg} /* 真阴的识别率*/\\
precision&=\dfrac{true\_pos}{true\_pos+true\_neg} /* 预测的精度*/\\
\end{split}</script><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>《数据挖掘概念与技术》第3版 第八章分类：基本概念 第九章 分类高级方法</p>
]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>课程</tag>
        <tag>国科大</tag>
        <tag>分类</tag>
        <tag>贝叶斯</tag>
        <tag>神经网络</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>数据预处理</title>
    <url>/2019-11-16-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86.html</url>
    <content><![CDATA[<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><h2 id="为什么数据预处理"><a href="#为什么数据预处理" class="headerlink" title="为什么数据预处理"></a>为什么数据预处理</h2><p>原始数据可能掺杂着噪音、空值或者不正确、不一致、充满冗余值等。没有高质量的数据，很难挖掘出高质量的规则，因此需要数据预处理。数据预处理主要有：数据清洗、数据集成、数据归约、数据离散等几个任务。</p>
<a id="more"></a>
<h2 id="数据描述"><a href="#数据描述" class="headerlink" title="数据描述"></a>数据描述</h2><p>得到一份数据后，需要先对数据进行一个大概的认识，主要从中心趋势和发散特征两方面统计进行描述。</p>
<h3 id="中心趋势度量"><a href="#中心趋势度量" class="headerlink" title="中心趋势度量"></a>中心趋势度量</h3><h4 id="平均值-mean"><a href="#平均值-mean" class="headerlink" title="平均值 mean"></a>平均值 mean</h4><p>均值又分为：</p>
<ul>
<li><p>算术均值</p>
<p> $\bar x=\frac{1}{n}\sum_{i=1}^{n}x_i$</p>
</li>
<li><p>加权均值</p>
<p>$\bar x= \frac{\sum_{i=1}^{n}w_ix_i}{\sum_{i=1}^{n}w_i}$</p>
</li>
<li><p>截尾均值 Trimmed mean</p>
<p>均值对极端数据比如离群点很敏感。因此截取高低两端的一部分数据后再计算均值，但是截去的部分不能太多不超过$20\%$</p>
</li>
</ul>
<h4 id="中位数-median"><a href="#中位数-median" class="headerlink" title="中位数 median"></a>中位数 median</h4><p>对于倾斜（非对称）数据，数据中心的更好度量是中位数。</p>
<p>假定数据有序，则当n为奇数时中位数是中间的值，当n为偶数的时候，中位数不唯一，可以是中间两个值和之间的任意值。如果属性是数值属性，那么一般约定取中间两个值的平均值。</p>
<h4 id="众数-mode"><a href="#众数-mode" class="headerlink" title="众数 mode"></a>众数 mode</h4><p>另一种中心趋势的度量方法，众数是集合中出现最频繁的值。数据可能会有多个众数，如果最高频的值有多个的时候。如果每个数据值近出现一次，是没有众数的。</p>
<p>有一个、两个、三个众数的分别称为：单峰的(unimodal)、双峰的（bimodal）和三峰的（trimodal）</p>
<h4 id="中列数-midrange"><a href="#中列数-midrange" class="headerlink" title="中列数 midrange"></a>中列数 midrange</h4><p>中列数是数据集的最大和最小值的平均值。</p>
<h4 id="对称与倾斜"><a href="#对称与倾斜" class="headerlink" title="对称与倾斜"></a>对称与倾斜</h4><p>完全对称的数据分布的单峰频率曲线中，均值、中位数和众数三者是相同的中心值。</p>
<p>不对称的情况分为：</p>
<ul>
<li>正倾斜：众数小于中位数</li>
<li>负倾斜：众数大于中位数</li>
</ul>
<p>如下所示,最上方是对称的，左下是正倾斜的，右下是负倾斜的。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573992640/blog/Data%20Mining/symmetric_skewed.jpg" alt=""></p>
<h3 id="数据散布度量"><a href="#数据散布度量" class="headerlink" title="数据散布度量"></a>数据散布度量</h3><h4 id="极差、四分位数和四分位数极差"><a href="#极差、四分位数和四分位数极差" class="headerlink" title="极差、四分位数和四分位数极差"></a>极差、四分位数和四分位数极差</h4><p><strong>极差（range）</strong>：最大值与最小值之差。</p>
<p>有序数据值下的数据集合的第k 个百分位数是具有如下性质的值x：数据项的百分之k 在x 上或低于x。在中位数M上或低于M 的值对应于第50 个百分位数。</p>
<p><strong>四分位数</strong>将数据分为4段总共有3个四分位数，Q1是第25个百分位数，Q2即是Median是第50个百分位数，Q3是第75个百分位数。</p>
<p>100-分位数通常称为百分位数，它们把数据分成100个大小相等的连贯集。中位数、四分位数和百分位数是使用最广泛的分位数。</p>
<p>第一个和第三个四分位数之间的距离是分布的一种简单度量，它给出被数据的中间一半<br>所覆盖的范围。该距离称为中间<strong>四分位数极差（IQR）</strong>：$IQR=Q_3-Q_1$</p>
<h4 id="五数概括、盒图与离群点"><a href="#五数概括、盒图与离群点" class="headerlink" title="五数概括、盒图与离群点"></a>五数概括、盒图与离群点</h4><p><strong>离群点(Outlier)</strong>：与$Q_1$或者$Q_3$这两个分位数的值超过$1.5IQR$</p>
<p>五数概括使用最小值、$Q_1$ 、中位数、$Q_3$、最大值来概述数据的中心与散布。</p>
<p>可以使用盒图来体现五数概括 。</p>
<ul>
<li>盒的端点在四分位数上，使得盒的长度是中间四分位数区间IQR</li>
<li>中位数用盒内的线标记</li>
<li>盒外的两条线（称作胡须）延伸到最小（Minimum）和最大（Maximum）观测值。如果最大最小值超过$1.5IQR$，那么只延伸到这个部分。</li>
</ul>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573992640/blog/Data%20Mining/box_plot.jpg" alt=""></p>
<p>如上图所示，最大值超过了$1.5IQR$所以只延伸到了$1.5IQR$，而超过$1.5IQR$部分的被标记为离群点。</p>
<h4 id="方差、标准差"><a href="#方差、标准差" class="headerlink" title="方差、标准差"></a>方差、标准差</h4><p>方差Variance与标准差Standard deviation指出数据分布的散布程度。</p>
<p>总体方差的计算：$\sigma^2=\frac{1}{n}\sum_{i=1}^{n}(x_i-\mu)^2={\frac{1}{n}\sum_{i=1}^{n}x_i^2}-{\mu}^2$</p>
<p>样本方差的计算：$s^2=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar x)^2={\frac{1}{n-1}（\sum_{i=1}^{n}x_i^2}-\frac{1}{n}\sum_{i=1}^{n}x_i^2）$</p>
<p>$\mu$是总体数据的平均值，而$\bar x$是总体数据中的一部分样本的均值，可以用样本的均值来估计总体的均值。样本方差是无偏估计的，<a href="https://www.zhihu.com/question/20099757">样本方差与总体方差的差别</a></p>
<p>标准差是方差的平方根，就是$\sigma$或者s。它的性质是：</p>
<ul>
<li>$\sigma$<strong>度量关于平均值的发散</strong>，仅当选择平均值作为中心度量时使用。</li>
<li>仅当不存在发散时，即当所有的观测值都相同时，$\sigma=0$。否则，$\sigma&gt;0$。</li>
</ul>
<h3 id="数据描述的图形表示"><a href="#数据描述的图形表示" class="headerlink" title="数据描述的图形表示"></a>数据描述的图形表示</h3><h4 id="直方图"><a href="#直方图" class="headerlink" title="直方图"></a>直方图</h4><p>直方图Histogram 或者频率直方图Frequency histograms 针对单变量，对于比较单变量观测组，它可能不如分位数图、q-q 图和盒图方法有效。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573992639/blog/Data%20Mining/histogram.jpg" alt=""></p>
<h4 id="分位数图"><a href="#分位数图" class="headerlink" title="分位数图"></a>分位数图</h4><p>分位数图Quantile Plot每个观测值$x_i$与一个百分数$f_i$配对，指出大约$f_i \times 100\%$的数据小于值$x_i$，“大约”是因为可能没有一个精确的小数值$f_i$，使得数据的$f_i\%$小于或等于$x_i$。0.25 分位数对应于Q1，0.50 分位数对应于中位数，而0.75 分位数对应于Q3。</p>
<p>$f_i$的定义：$f_i=\frac{i-0.5}{N}$这些数由$\frac{1}{2N}$（稍大于0）到$1-\frac{1}{2N}$（稍小于1），以相同的步长1/n 递增。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573992640/blog/Data%20Mining/Quantile_Plot.jpg" alt=""></p>
<h4 id="分位数-分位数图"><a href="#分位数-分位数图" class="headerlink" title="分位数-分位数图"></a>分位数-分位数图</h4><p>分位数-分位数图Quantile-Quantile Plot，或q-q 图对着另一个的对应分位数，绘制一个单变量分布的分位数。它是一种强有力的直观表示工具，使得用户可以观察从一个分布到另一个是否有漂移。![]<a href="https://res.cloudinary.com/bravey/image/upload/v1573992640/blog/Data%20Mining/qq_plot.jpg">https://res.cloudinary.com/bravey/image/upload/v1573992640/blog/Data%20Mining/qq_plot.jpg</a>)</p>
<p>部门1的分布相对于部门2有一个漂移，更趋向于部门2，说明部门2的单价趋向于比部门1高。</p>
<h4 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h4><p>散点图(scatter plot)是确定两个量化变量之间看上去是否有联系、模式或趋势的最有效的图形方法之一。</p>
<p>它观察的是<strong>双变量</strong>，可以观察点簇和离群点，或者考察相关性。正相关x随着y的增加而增加，负相关x随着y的增长而减少。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573992640/blog/Data%20Mining/scatter_plot.jpg" alt=""></p>
<h2 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h2><h4 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h4><ul>
<li>忽略元组</li>
<li>人工填写缺值</li>
<li>使用全局常量填充 ：比如Unknown </li>
<li>所有样本的中心值填充：均值或者中位数 均值要求数据对称分布，倾斜分布用中位数</li>
<li>给定元组的分类相同的样本的均值或者中位数</li>
<li>最可能的值：使用贝叶斯推理、回归、决策数等进行预测。 前面几种是有偏的，这种方法最常用。</li>
</ul>
<h4 id="噪声数据"><a href="#噪声数据" class="headerlink" title="噪声数据"></a>噪声数据</h4><h5 id="分箱"><a href="#分箱" class="headerlink" title="分箱"></a>分箱</h5><p>分箱方法通过考察“邻居”（即，周围的值）来平滑存储数据的值。存储的值被分布到一些“桶”或箱中。由于分箱方法导致值相邻，因此它进行局部平滑。</p>
<p>要求数据有序因此需要<strong>先进行排序</strong>，有三种方法：</p>
<ul>
<li><strong>箱均值光滑</strong>：箱中的每一个值被替换为箱中的均值</li>
<li><strong>箱中位数光滑：</strong>箱中的每一个值被替换为箱中的中位数</li>
<li><strong>箱边界光滑：</strong>边界为箱中的最大与最小值，每个值被替换为距离其最近的边界值。</li>
</ul>
<p>而箱子的分法有<strong>等频（等深）</strong>：每个箱子中的样本数目一样。<strong>等宽</strong>：按照取值范围将样本划分，每个箱子中的取值范围一致，比如样本数据取值为[0,10]那么可以按照等宽为2，把[0,2),[2,4)这样的取值范围来把样本划分到对应区间所在的范围中。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573992639/blog/Data%20Mining/bin.jpg" alt=""></p>
<h5 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h5><p>用函数拟合数据来光滑数据，将离拟合曲线远的数据标记为噪声数据。</p>
<h5 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h5><p>使用聚类分析后，检测出离群点，然后把离群点标记为噪声数据。</p>
<h2 id="数据转换"><a href="#数据转换" class="headerlink" title="数据转换"></a>数据转换</h2><h3 id="归一化-规范化"><a href="#归一化-规范化" class="headerlink" title="归一化/规范化"></a>归一化/规范化</h3><h4 id="最大-最小归一化"><a href="#最大-最小归一化" class="headerlink" title="最大-最小归一化"></a>最大-最小归一化</h4><p>对原始数据进行线性变换，映射到新的区间[new_min, new_max]中去。这种方法保持了原始数据之间的联系。</p>
<script type="math/tex; mode=display">
v_i'=\frac{v_i-min_A}{max_A-min_A}(new\_max_A-new\_min_A)</script><h4 id="z-score归一化"><a href="#z-score归一化" class="headerlink" title="z-score归一化"></a>z-score归一化</h4><p>新的值使用均值和标准差进行映射。$\bar A$和$\sigma_A$ 分别是样本的均值和他的标准差。</p>
<script type="math/tex; mode=display">
v_i'=\frac{v_i-\bar A}{\sigma_A}</script><h4 id="小数定标归一化"><a href="#小数定标归一化" class="headerlink" title="小数定标归一化"></a>小数定标归一化</h4><p>小数定标归一化 Normalization by decimal scaling。移动原来的小数点的位置来进行归一化。</p>
<script type="math/tex; mode=display">
v_i'=\dfrac{v_i}{10^j}</script><p>j是让$Max(|v_i’|&lt;1)$的最小整数，比如如果是254，那么j取3就可以让其为0.254小于1。</p>
<h2 id="冗余与相关性分析"><a href="#冗余与相关性分析" class="headerlink" title="冗余与相关性分析"></a>冗余与相关性分析</h2><p>一个属性可以由另外的属性导出，那么另外的属性就是冗余的。有些冗余可以通过<strong>相关性分析</strong>来检测</p>
<h3 id="数值数据的相关系数"><a href="#数值数据的相关系数" class="headerlink" title="数值数据的相关系数"></a>数值数据的相关系数</h3><p>相关系数Correlation coefficient也称作皮尔森积矩系数Pearson’s product moment coefficient估计两个属性A,B的相关度。</p>
<script type="math/tex; mode=display">
r_{A.B}=\dfrac{\sum_{i=1}^{n}(a_i-\bar A)(b_i-\bar B)}{(n-1)\sigma_A\sigma_B}=\dfrac{\sum_{i=1}^{n}(a_ib_i)-n\bar A\bar B}{(n-1)\sigma_A\sigma_b}</script><p>$-1\le r_{A,B}\le 1$ ,$r_{A,B}&gt;0$ A和B两个属性正相关，A的值随着B的值增长而增长。$r_{A,B}&lt;0$为负相关，A的值随着B的值增长而减少。该值越大，一个属性蕴涵另一个的可能性越大。因此，一个很大的值表明A（或B）可以作为冗余而被去掉。如果结果值等于0，则A 和B 是独立的，它们之间不相关。可以参见前述的散点图。</p>
<h3 id="分类数据的-chi-2-相关检验"><a href="#分类数据的-chi-2-相关检验" class="headerlink" title="分类数据的$\chi^2$相关检验"></a>分类数据的$\chi^2$相关检验</h3><p>分类数据Categorical Data中，两类数据可以通过$\chi^2$卡方检验来发现它们的相关性。计算公式为：</p>
<script type="math/tex; mode=display">
\begin{split}
&\chi^2=\sum_{i=1}^{c}\sum_{j=1}^{r}\dfrac{o_{ij}-e_{ij}}{e_{ij}}\\\\
&e_{ij}=\dfrac{count(A=a_i)\times count(B=b_j)}{n}
\end{split}</script><p>$o_{ij}$ 是联合事件$(A_i,B_j)$的观测频度也就是实际频数，而$e_{ij}$ 是$(A_i,B_j)$的期望值。n是总的数据样本数。使用相依表来表示数据。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573992639/blog/Data%20Mining/rely_table.jpg" alt=""></p>
<p>图中括号中的是每个单元的预测值。总共抽取1500个样本，因此n=1500。A类是否喜欢看小说有两类取值，B类性别也有两类取值，因此c=2，r=2。计算单元（男，小说）的预测值有：</p>
<script type="math/tex; mode=display">
e_{11}=\dfrac{count(男)\times count(小说)}{n}=\dfrac{300\times450}{1500}=90</script><p>其他几个单元的预测值都在括号中。因此可以计算出：</p>
<script type="math/tex; mode=display">
\chi^2=\dfrac{(250-90)^2}{90}+\dfrac{(50-210)^2}{210}+\dfrac{(200-360)^2}{360}+\dfrac{(1000-840)^2}{840}=507.93</script><p>对于这个2*2的表，自由度为（2-1)(2-1)=1。对于自由度1，在0.001的置信水平下，查表得到拒绝假设的值是10.828。算出来的值大与它，因此认为性别和爱看小说不是独立的，是强相关的。</p>
<h2 id="数据归约"><a href="#数据归约" class="headerlink" title="数据归约"></a>数据归约</h2><p>数据归约技术 Data Reduction可以用来得到数据集的归约表示，它小得多，但仍接近地保持原数据的完整性。</p>
<p>分为：</p>
<ul>
<li>维规约：降维</li>
<li>数量规约：用替代的、较小的的数据形式替换原始数据。比如只存放数据的模型参数</li>
<li>数据压缩：通过变换将原始数据压缩，不损失原来的信息叫做无损，否则是有损</li>
</ul>
<h3 id="维规约"><a href="#维规约" class="headerlink" title="维规约"></a>维规约</h3><h4 id="小波变换"><a href="#小波变换" class="headerlink" title="小波变换"></a>小波变换</h4><p>DWT）是一种线性信号处理技术，当用于数据向量D 时，将它转换成不同的数值向量小波系数D’。两个向量具有相同的长度。虽然变换后向量维度一样，但是可以仅存放一小部分最强的小波系数，就能保留近似的压缩数据。DWT提供比离散傅利叶DFT更好的有损压缩，DWT 将提供原数据更精确的近似。因此，对于等价的近似，DWT 比DFT 需要的空间小。不像DFT，小波空间局部性相当好，有助于保留局部细节。</p>
<p>该方法如下：</p>
<ol>
<li>输入数据向量的长度L 必须是2 的整数幂。必要时，通过在数据向量后添加0，这一条件可以满足。</li>
<li>每个变换涉及应用两个函数。第一个使用某种数据平滑，如求和或加权平均。第二个进行加权差分，产生数据的细节特征。</li>
<li>两个函数作用于输入数据对，产生两个长度为L/2 的数据集。一般地，它们分别代表输入数据的平滑后或低频的版本和它的高频内容。</li>
<li>两个函数递归地作用于前面循环得到的数据集，直到结果数据集的长度为2。</li>
<li>由以上迭代得到的数据集中选择值，指定其为数据变换的小波系数。</li>
</ol>
<h4 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h4><p>主成分分析PCA 又称Karhunen-Loeve 或K-L 方法）搜索c 个最能代表数据的k-维正交向量；这里c ≤ k。这样，原来的数据投影到一个较小的空间，导致数据压缩。PCA 可以作为一种维归约形式使用。然而，不象属性子集选择通过保留原属性集的一个子集来减少属性集的大小。PCA 通过创建一个替换的、较小的变量集“组合”属性的本质。原数据可以投影到该较小的集合中。</p>
<p>详解的过程暂时不记录，查看模式识别的教材。<strong>只适用数值数据</strong></p>
<h4 id="属性-特征子集选择"><a href="#属性-特征子集选择" class="headerlink" title="属性/特征子集选择"></a>属性/特征子集选择</h4><p>也就是降维。使用压缩搜索空间的启发式的算法，典型的是贪心算法。每次找到一个局部的好的属性，剔除掉差的属性。</p>
<p>属性子集选择的基本启发式方法包括以下技术：</p>
<ul>
<li>逐步向前选择：该过程由空属性集开始，选择原属性集中最好的属性，并将它添加到该集合中。</li>
<li>逐步向后删除：该过程由整个属性集开始。在每一步，删除掉尚在属性集中的最坏属性。</li>
<li>向前选择和向后删除的结合：向前选择和向后删除方法可以结合在一起，每一步选择一个最好的属性，并在剩余属性中删除一个最坏的属性。</li>
<li>判定树归纳：判定树算法，如ID3 和C4.5 。</li>
</ul>
<h3 id="数量规约"><a href="#数量规约" class="headerlink" title="数量规约"></a>数量规约</h3><h4 id="数据立方体集成"><a href="#数据立方体集成" class="headerlink" title="数据立方体集成"></a>数据立方体集成</h4><p>将数据整理成之前介绍过的数据立方体，把感兴趣的数据整理到基本立方体base cuboid上面。比如如果只关注每个季度的销售数据，那么可以将原来的每天的数据整理成每个季度的销售数据。这样就可以大大减少原来的数据量了。</p>
<h4 id="回归-1"><a href="#回归-1" class="headerlink" title="回归"></a>回归</h4><p>因为展开内容很多，只记录下有这些方法。当把数据拟合为某一种模型后只用记录这些模型的参数就可以了</p>
<ul>
<li>线性回归</li>
<li>多元线性回归</li>
<li>对数线性模型</li>
</ul>
<h4 id="直方图-1"><a href="#直方图-1" class="headerlink" title="直方图"></a>直方图</h4><p>就是前面叙述的分箱的方法，使用一个桶来记录一个属性的频次。或者等宽的方法，用区间来记录每个区间中的频次。</p>
<h4 id="聚类-1"><a href="#聚类-1" class="headerlink" title="聚类"></a>聚类</h4><p>聚类后，用数据的簇代表替换实际的数据。即只记录簇的中心点。</p>
<h4 id="抽样"><a href="#抽样" class="headerlink" title="抽样"></a>抽样</h4><p>假定大的数据集D 包含N 个元组。我们看看对D 的可能选样。</p>
<p>简单选择n 个样本，不回放(<strong>SRSWOR</strong>)：由D 的N 个元组中抽取n 个样本（n &lt; N）；其中， D中任何元组被抽取的概率均为1/N。即，所有元组是等可能的。</p>
<p>简单选择n 个样本，回放(<strong>SRSWR</strong>)：该方法类似于SRSWOR，不同在于当一个元组被抽取后，记录它，然后放回去。这样，一个元组被抽取后，它又被放回D，以便它可以再次被抽取。</p>
<h3 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h3><p>主要使用编码机制来进行压缩。包括字符串、音频、视频的压缩。</p>
<h3 id="数据离散和概念分层"><a href="#数据离散和概念分层" class="headerlink" title="数据离散和概念分层"></a>数据离散和概念分层</h3><p>可以使用分箱来、聚类、决策树和相关分析来进行离散化。而对于标称数据可以进行概念分层，前面的数据立方体就是用的这个思路。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>《数据挖掘概念与技术》第3版 第三章数据预处理</p>
]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>课程</tag>
        <tag>国科大</tag>
        <tag>降维</tag>
        <tag>相关性分析</tag>
      </tags>
  </entry>
  <entry>
    <title>数据仓库</title>
    <url>/2019-11-15-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93.html</url>
    <content><![CDATA[<h1 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h1><h2 id="什么是数据仓库"><a href="#什么是数据仓库" class="headerlink" title="什么是数据仓库"></a>什么是数据仓库</h2><p>有多种定义，按照一位数据仓库系统构造方面的领衔设计师William H. Inmon的说法：数据仓库是一个面向主题的、集成的、时变的、非易失的数据集合，支持管理者的决策过程。</p>
<p>一句话总结：通过数据仓库来完成对一个项目的相关需求的快速分析，是一个联机分析处理（Online Analytical Processing System）系统，具体的定义见<a href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93">百度百科</a>。</p>
<a id="more"></a>
<h3 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h3><ul>
<li><strong>面向主题的</strong>：数据仓库围绕一些主题，如顾客、供应商、产品和销售组织。数据仓库关注决策者的数据建模与分析，而不是构造组织机构的日常操作和事务处理。</li>
<li><strong>集成的：</strong>通常，构造数据仓库是将多个异种数据源，如关系数据库、一般文件和联机事务处理记录，集成在一起。</li>
<li><strong>时变的</strong>：数据存储从历史的角度（例如，过去5-10 年）提供信息。</li>
<li><strong>非易失的：</strong>数据仓库总是。物理地分离存放数据；这些数据源于操作环境下的应用数据</li>
</ul>
<h3 id="操作数据库系统与数据仓库的区别"><a href="#操作数据库系统与数据仓库的区别" class="headerlink" title="操作数据库系统与数据仓库的区别"></a>操作数据库系统与数据仓库的区别</h3><p>我们常见的关系数据库就是联机操作数据库，联机操作数据库系统的主要任务是执行联机事务和查询处理。这种系统称为联机事务处理（OLTP）系统。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573875850/blog/OLTP_OLAP.jpg" alt=""></p>
<h2 id="数据立方体"><a href="#数据立方体" class="headerlink" title="数据立方体"></a>数据立方体</h2><p>数据仓库基于<strong>多维数据模型</strong>，这种模型将数据看做数据立方体形式。</p>
<p>数据立方体的定义：在一个多维的平台上面对数据建模和观察，由<strong>维和事实</strong>定义。</p>
<p>每个维都可以有一个与之相关联的表，称作维表，在表中进一步的描述维更具体的属性（比如名字、类型等）。</p>
<p>多维数据模型围绕一个具体的主题（比如销售）这样的中心主题来组织。主题用事实表表示。事实是数值度量的，事实表包括事实的<strong>名称或度量</strong>。</p>
<p>一个具体的例子如下所示，表示的是关于销售这个事实的数据立方体。其中左边是一个二维的数据立方体，右边是一个三维的数据立方体。可以扩展到n维。以3d这个数据立方体，讲解下682这个值的含义，表示Toronto市，security项目在Q2季度的销售量。<img src="https://res.cloudinary.com/bravey/image/upload/v1573875852/blog/data_cube.jpg" alt=""></p>
<h2 id="数据仓库建模"><a href="#数据仓库建模" class="headerlink" title="数据仓库建模"></a>数据仓库建模</h2><p>数据仓库最流行的数据模型是多维数据模型，这种模型有三种形式：星形模式（Star schema）、雪花模式（snowflake schema）、事实星座（fact constellation）</p>
<h3 id="星形模式"><a href="#星形模式" class="headerlink" title="星形模式"></a>星形模式</h3><p>最常见的模式，在这种模式下，数据仓库包括：</p>
<ul>
<li>一个大的事实表，里面又包含<ul>
<li>事实的维度</li>
<li>事实的度量</li>
</ul>
</li>
<li>多组小的维表，详细记录一个维度的属性</li>
</ul>
<p>例子如下，展开了很像星光四射，因而得名。<img src="https://res.cloudinary.com/bravey/image/upload/v1573875853/blog/start_schema.jpg" alt=""></p>
<p>最中间的是销售事实表，总共有4个维度：时间、商品、部门（branch）、地点，3个度量：销售数量、销售金额、平均销售量。</p>
<h3 id="雪花模式"><a href="#雪花模式" class="headerlink" title="雪花模式"></a>雪花模式</h3><p>雪花模式是星形模式的变种，其中某些维表被规范化，因而把数据进一步分解到附加的表中。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573875852/blog/snowfake_schema.jpg" alt=""></p>
<p>星形模式是只有一层的，而雪花模式可以延伸。图中在item维度又延伸出了供应商这个维度，在地点维度又延伸出了城市这个维度。</p>
<h3 id="事实星座"><a href="#事实星座" class="headerlink" title="事实星座"></a>事实星座</h3><p>对于一些复杂的应用可能需要多个事实表共享维表，这种模式可以看成星形模式的汇集，所以称做事实星座。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573875852/blog/fact_schema.jpg" alt=""></p>
<p>上面总共有两个事实表：销售与运输(Shipping)。因为两个事实表有些维度是共享的，所以同时指向相同的维度就可以了。</p>
<h3 id="概念分层"><a href="#概念分层" class="headerlink" title="概念分层"></a>概念分层</h3><p>概念分层(Concept Hierarchy)，对一个维度在概念上进行分层。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573875852/blog/location_hierarchy.jpg" alt=""></p>
<p>比如在地点上可以从办公室上升到城市上升到国家一直到最后的最高层所有。每个维度的最高层都是all所有这个级别。上图地点的分层是基于全序的层次结构，也可以组织成基于偏序的格结构，如下对时间的分层。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573875851/blog/time_hierarchy.jpg" alt=""></p>
<p>对于连续的属性值或或者维度，通过将其离散化来定义概念分层。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573875851/blog/numeric_hierarchy.jpg" alt=""></p>
<h3 id="OLAP操作"><a href="#OLAP操作" class="headerlink" title="OLAP操作"></a>OLAP操作</h3><p>进行实际分析的时候需要执行OLAP操作，典型的有下面四种。</p>
<h4 id="上卷（roll-up）"><a href="#上卷（roll-up）" class="headerlink" title="上卷（roll-up）"></a>上卷（roll-up）</h4><p>上卷操作沿着一个维度的概念分层向上攀升。如下图所示的第1个操作，对location维度从城市上卷到国家，则原来的立方体的location维度从原来有4个城市变成只有两个国家。</p>
<h4 id="下钻（drill-down）"><a href="#下钻（drill-down）" class="headerlink" title="下钻（drill-down）"></a>下钻（drill-down）</h4><p>下钻是上卷的逆操作，它由不太详细的数据到更详细的数据。</p>
<p>比如从第1个上卷的操作得到的数据立方体做从国家到城市的逆操作就回到了原来的数据立方体。第二个操作就是下钻操作，在time维度上从季度下钻到了月。</p>
<h4 id="切片（slice）和切块（dice）"><a href="#切片（slice）和切块（dice）" class="headerlink" title="切片（slice）和切块（dice）"></a>切片（slice）和切块（dice）</h4><p>切片操作在给定的立方体上一个维度上面进行选择，形成一个新的子立方体。比如第3个操作，切边选择了time=Q1，因而得到的子立方体都是在Q1季度上的数据。</p>
<p>切块操作则是在多个维度上进行选择。比如第4个操作就是在location维度选择多伦多与温哥华，而在time维度上选择Q1与Q2季度，而在item维度上选择家庭娱乐与计算机。</p>
<h4 id="转轴（pivot）"><a href="#转轴（pivot）" class="headerlink" title="转轴（pivot）"></a>转轴（pivot）</h4><p>转轴又称作旋转(rotate)，是一种目视操作，移动了数据的视角。比如第5个操作，将原来的item维度在下，location维度在上。旋转后变成了item维度在上，而location维度在下。类似于矩阵的转置。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573875851/blog/olap_operation.jpg" alt=""></p>
<h4 id="操作实例"><a href="#操作实例" class="headerlink" title="操作实例"></a>操作实例</h4><p> Starting with the base cuboid [day, doctor, patient],what OLAP operations should be performed in order<br>to list the total fee collected by each doctor in 1999?</p>
<p>首先要求所有的费用，因此对patient维度做上卷操作，上卷到all所有这个层次。然后时间限定在了1999年，因此对day维度做上卷操作，上卷到年这个层次。最后进行切边操作，选择1999年这个年份。</p>
<h2 id="数据仓库实现"><a href="#数据仓库实现" class="headerlink" title="数据仓库实现"></a>数据仓库实现</h2><h3 id="数据立方体的有效计算"><a href="#数据立方体的有效计算" class="headerlink" title="数据立方体的有效计算"></a>数据立方体的有效计算</h3><p>前述的OLAP操作依赖于对数据立方体的计算操作，既是针对不同维度上面的聚合，SQL的术语是分组（group-by）。对于n个维度的数据立方体，不考虑每个维度的概念分层，则总共可以构成的分组（不同维度组合的集合）有$2^n$种。如下所示，一个（city，item，year）的三维数据立方体可以有8种分组方式。</p>
<p>其实不考虑概念分层意味着每个维度可以有最高层all和最低层两个分层。因此如果考虑概念分层，假设有n个维度</p>
<p>，每个维度都有m个分层，则这样来说可有$m^n$个方体。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573875850/blog/cube_group_by.jpg" alt=""></p>
<p>为了实现快速返回OLAP的操作，需要提前将这些分组的结果存储下来，这样当指定OLAP操作，直接去访问这些分组就可以了。这种提前计算方体结果的方法，称作<strong>物化（materialization）</strong>但是考虑到概念分层，存储所有的分组结果是不现实的。采用<strong>部分物化（partial materaialiaztion）</strong>预先计算一部分适当的常用的子集。</p>
<h3 id="OLAP索引"><a href="#OLAP索引" class="headerlink" title="OLAP索引"></a>OLAP索引</h3><h4 id="位图索引（bitmap-indexing）"><a href="#位图索引（bitmap-indexing）" class="headerlink" title="位图索引（bitmap indexing）"></a>位图索引（bitmap indexing）</h4><p>在给定属性的位图索引中，属性域中的每个值v，有一个不同的位向量bit vector(即一列值)Bv。如果给定的属性域包含n 个值，则位图索引中每项需要n 位（即，n 位向量）。如果数据表中给定行的属性值为v，则在位图索引的对应行，表示该值的位为1，该行的其它位均为0。可以理解成one hot 编码。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573875851/blog/index_map.jpg" alt=""></p>
<p>Region有3种取值，所以3个位向量，也就是对应3列值；Type有2种取值，所以对应2列值。</p>
<h5 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h5><p>优点：与散列和树索引相比，位图索引有优势。对于基数较小的域它特别有用，因为比较、连接、和聚集操作都变成了位算术运算，大大减少了运行时间。由于字符串可以用单个位表示，位图索引大大降低了空间和I/O 开销。对于基数较高的域，使用压缩技术，这种方法可以接受。但是不是很适合。</p>
<p>缺点：因为值域中的每一个可能的值都需要一个位向量来记录，所以当基数较大时，会需要开辟很大的存储空间，因为每一个记录其实只使用到了一位，因此造成了很大的浪费。同时当可取的值连续时无法使用位图来记录。</p>
<h4 id="连接索引"><a href="#连接索引" class="headerlink" title="连接索引"></a>连接索引</h4><p>如果两个关系R(RID,A)和S(B,SID)在属性A和B 上连接，则连接索引记录包含(RID,SID)对，其中RID 和SID 分别为来自R 和S 的记录标识符。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573875850/blog/join_index.jpg" alt=""></p>
<p>在左边的销售事实表中有同location和item两个维度的链接，那么就可以有右边3中连接索引。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>《数据挖掘概念与技术》第3版 第四章数据仓库与联机分析处理</p>
]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>OLAPS</tag>
        <tag>课程</tag>
        <tag>国科大</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo博客Next主题优化</title>
    <url>/2019-11-08-Hexo%E5%8D%9A%E5%AE%A2Next%E4%B8%BB%E9%A2%98%E4%BC%98%E5%8C%96.html</url>
    <content><![CDATA[<h1 id="Hexo博客Next主题优化"><a href="#Hexo博客Next主题优化" class="headerlink" title="Hexo博客Next主题优化"></a>Hexo博客Next主题优化</h1><p>博客设置中的一些参考链接</p>
<a id="more"></a>
<h2 id="汇总"><a href="#汇总" class="headerlink" title="汇总"></a>汇总</h2><p><a href="https://tding.top/archives/aad98408.html">https://tding.top/archives/aad98408.html</a> </p>
<h2 id="latex支持"><a href="#latex支持" class="headerlink" title="latex支持"></a>latex支持</h2><p>直接设置</p>
<p><a href="https://jdhao.github.io/2017/10/06/hexo-markdown-latex-equation/">https://jdhao.github.io/2017/10/06/hexo-markdown-latex-equation/</a> </p>
<h2 id="Disquse评论支持"><a href="#Disquse评论支持" class="headerlink" title="Disquse评论支持"></a>Disquse评论支持</h2><p><a href="http://www.cylong.com/blog/2017/03/26/hexo-next-disqus/">http://www.cylong.com/blog/2017/03/26/hexo-next-disqus/</a> </p>
<p><a href="https://chenlifei.tech/posts/8838407/">https://chenlifei.tech/posts/8838407/</a> </p>
<h2 id="动态主题"><a href="#动态主题" class="headerlink" title="动态主题"></a>动态主题</h2><p>在config文件中设置，有个 git pull 更新的过程</p>
<h2 id="建站时间"><a href="#建站时间" class="headerlink" title="建站时间"></a>建站时间</h2><p><a href="https://xian6ge.cn/posts/82ce1911/">https://xian6ge.cn/posts/82ce1911/</a> </p>
<h2 id="网站头像"><a href="#网站头像" class="headerlink" title="网站头像"></a>网站头像</h2><p><a href="https://www.jianshu.com/p/82c1d33420ba">https://www.jianshu.com/p/82c1d33420ba</a> </p>
<p><a href="https://tool.lu/favicon/">https://tool.lu/favicon/</a> </p>
<h2 id="社交链接"><a href="#社交链接" class="headerlink" title="社交链接"></a>社交链接</h2><p>在config文件直接设置</p>
<h2 id="阅读统计"><a href="#阅读统计" class="headerlink" title="阅读统计"></a>阅读统计</h2><p><a href="https://hexo-guide.readthedocs.io/zh_CN/latest/third-service/[%E4%B8%8D%E8%92%9C%E5%AD%90]%E8%AE%BF%E5%AE%A2%E4%BA%BA%E6%95%B0.html">https://hexo-guide.readthedocs.io/zh_CN/latest/third-service/[%E4%B8%8D%E8%92%9C%E5%AD%90]%E8%AE%BF%E5%AE%A2%E4%BA%BA%E6%95%B0.html</a> </p>
<h2 id="SEO优化"><a href="#SEO优化" class="headerlink" title="SEO优化"></a>SEO优化</h2><p><a href="https://hoxis.github.io/Hexo+Next%20SEO%E4%BC%98%E5%8C%96.html">https://hoxis.github.io/Hexo+Next%20SEO%E4%BC%98%E5%8C%96.html</a> </p>
<h2 id="版权声明"><a href="#版权声明" class="headerlink" title="版权声明"></a>版权声明</h2><p>把js库给替换成了https</p>
<p><a href="https://blog.pangao.vip/Hexo%E5%8D%9A%E5%AE%A2NexT%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96%E4%B9%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E7%AB%A0%E5%BA%95%E9%83%A8%E7%89%88%E6%9D%83%E5%A3%B0%E6%98%8E/">https://blog.pangao.vip/Hexo%E5%8D%9A%E5%AE%A2NexT%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96%E4%B9%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E7%AB%A0%E5%BA%95%E9%83%A8%E7%89%88%E6%9D%83%E5%A3%B0%E6%98%8E/</a> </p>
<h2 id="代码复制功能"><a href="#代码复制功能" class="headerlink" title="代码复制功能"></a>代码复制功能</h2><p><a href="https://blog.csdn.net/dataiyangu/article/details/88879328">https://blog.csdn.net/dataiyangu/article/details/88879328</a> </p>
<p><strong>加载不安全的脚本</strong> 问题 失败 </p>
<h2 id="升级至新版本"><a href="#升级至新版本" class="headerlink" title="升级至新版本"></a>升级至新版本</h2><p><a href="https://sevencho.github.io/archives/14534beb.html">https://sevencho.github.io/archives/14534beb.html</a></p>
<p><a href="https://tding.top/archives/42c38b10">https://tding.top/archives/42c38b10</a> </p>
<p><a href="https://www.jianshu.com/p/e8d433a2c5b7">https://www.jianshu.com/p/e8d433a2c5b7</a> </p>
<h2 id="搜索无效"><a href="#搜索无效" class="headerlink" title="搜索无效"></a>搜索无效</h2><p><a href="https://blog.csdn.net/Aoman_Hao/article/details/86713171">https://blog.csdn.net/Aoman_Hao/article/details/86713171</a> </p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title>Clementine教程</title>
    <url>/2019-11-07-Clementine%E6%95%99%E7%A8%8B.html</url>
    <content><![CDATA[<h1 id="Clementine教程"><a href="#Clementine教程" class="headerlink" title="Clementine教程"></a>Clementine教程</h1><p>数据挖掘课程要求使用这个软件Clementine来进行实验，之前完全没听说过这个软件。网上搜到的资料也比较少，特别是CSDN上面有个博客名字叫做Clementine完整教程，然后内容也是Clementine教程这几个字，把我给惊呆了，这也能写博客？现在实验都已经做完了因此记录下使用方法，希望对其他人能有帮助。</p>
<a id="more"></a>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="页面"><a href="#页面" class="headerlink" title="页面"></a>页面</h3><p>在Clementine软件中只需要简单的像画图一样，把整个数据挖掘的操作流程对应的节点给连接起来就可以使用了。先对页面简单的介绍下，页面如下。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573095648/blog/clementine.jpg" alt=""></p>
<p>在最下面的一栏是相对应的各个节点：Favorites中是常用的节点，Sources中是输入的数据节点，Field中是数据的属性设置节点，Modeling中是一些数据挖掘模型节点比如决策树神经网络等模型，Output中是输出节点用来查看数据。</p>
<p>在右上角中的Streams显示整个工作台中的所有Stream，Outputs中是每次运行后得到输出结果，Models中是流程所用到的模型。</p>
<p>数据挖掘的过程可以简单的分为两步：第一步对模型进行训练 第二步对训练好的模型进行使用。而一个完整的数据挖掘流程Stream中在Clementine中需要包含：数据输入节点-&gt;输入属性设置节点-&gt;模型节点-&gt;输出节点。在第一步模型训练中可以不使用输出节点。</p>
<h2 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h2><p>在下方节点栏中的Sources中选择Var.file节点，拖到中间的画布。然后双击设置输入数据的文件导入，如下所示：<br><img src="https://res.cloudinary.com/bravey/image/upload/v1573097265/blog/varfile1.jpg" alt=""></p>
<p>在导入数据后选择分隔符我的数据是用tab分割的)，之后点击Apply完成设置。</p>
<h2 id="数据显示"><a href="#数据显示" class="headerlink" title="数据显示"></a>数据显示</h2><p>在导入了数据后可以使用输出节点来输出数据，选择Outputs中的Table节点来输出数据。如下所示：</p>
<p><img src="/2019-11-07-Clementine%E6%95%99%E7%A8%8B.htm/Users\BraveY\AppData\Local\Temp\1573097747688.png" alt=""></p>
<p>对输入节点训练集.txt邮件有个connect可以进行连接。或者使用快捷键F2来连接。连接好后右键execute就可以得到上图显示了。</p>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>以使用决策树模型为例子，给定的输入数据集是一个在线测试系统中学生做的各项测试的数据，final是期末考试是否及格。也就是通过学生在测试系统的数据，来预测学生期末考试是否会及格。在选择模型前，需要在Fidel Ops中选择一个type节点来设置输入数据中各个属性。如下所示：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573098328/blog/type.jpg" alt=""></p>
<p>第一个PersonId设置为Typeless不进行输入，决策树输出的是final，并设置为flag类型。其他数据都设置为range。</p>
<p>接下来选择模型，在Modeling中选择C5.0这个决策树模型。并与前面的type节点连接。如下所示：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573098962/blog/train_tree.jpg" alt=""></p>
<p>可以选择专家模式自定义剪枝率等参数。设置好后执行就可以得到训练好的模型了，将会显示在右上角的model中，这样就完成模型训练这个Stream了。</p>
<p>在右上角的models中选择训练好的final模型，右键browse，然后选择viewer就可以查看训练好的决策树模型了。如下所示。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1573099406/blog/model.jpg" alt=""><br><img src="https://res.cloudinary.com/bravey/image/upload/v1573099392/blog/viewer.jpg" alt=""></p>
<h3 id="Apriori"><a href="#Apriori" class="headerlink" title="Apriori"></a>Apriori</h3><p>补充Apriori模型的例子，整体的架构为<br><img src="https://res.cloudinary.com/bravey/image/upload/v1581417277/blog/Clementine/Apriori_model.jpg" alt=""></p>
<p>数据的导入设置为：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1581416895/blog/Clementine/data.jpg" alt=""></p>
<p>type节点的设置为：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1581416896/blog/Clementine/type.jpg" alt=""></p>
<p>模型的设置为：</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1581416895/blog/Clementine/model_setting.jpg" alt=""></p>
<p>其中支持度和置信度这些参数是可以自己调的。</p>
<h2 id="模型使用"><a href="#模型使用" class="headerlink" title="模型使用"></a>模型使用</h2><p>同样导入数据节点验证集.txt，设置好type节点(测试集的flag属性也设置为输入了)，再把训练好的模型从右上角给拖进来，最后设置输出节点。整个流程与前述相同，得到的验证流程如下：<br><img src="https://res.cloudinary.com/bravey/image/upload/v1573099826/blog/test_stream.jpg" alt=""><br>输出节点使用混淆矩阵来查看模型的准确性，具体设置为：<br><img src="https://res.cloudinary.com/bravey/image/upload/v1573099827/blog/confuse_matrix.jpg" alt=""><br>最后执行就可以得到最终的显示了如下：<br><img src="https://res.cloudinary.com/bravey/image/upload/v1573099826/blog/matrix_out.jpg" alt=""></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其他模型比如神经网络等的使用流程大同小异，如果需要数据集与测试集以及英文教程可以联系我。</p>
]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>Clementine</tag>
      </tags>
  </entry>
  <entry>
    <title>内嵌汇编</title>
    <url>/2019-10-31-%E5%86%85%E5%B5%8C%E6%B1%87%E7%BC%96.html</url>
    <content><![CDATA[<h1 id="内嵌汇编"><a href="#内嵌汇编" class="headerlink" title="内嵌汇编"></a>内嵌汇编</h1><p>操作系统高级教程上面需要阅读Linux内核0.11的源码，其中在书《Linux内核设计的一书》第2.5节异常处理类中段服务程序挂接的部分，遇到了嵌入在C语言中的汇编代码，之前从来没有学习过汇编，因此记录下。</p>
<a id="more"></a>
<h2 id="AT-amp-T基础知识"><a href="#AT-amp-T基础知识" class="headerlink" title="AT&amp;T基础知识"></a>AT&amp;T基础知识</h2><p>内嵌汇编使用的是AT&amp;T汇编，所以首先稍微讲解下AT&amp;T的汇编指令的基础知识。</p>
<h3 id="操作数前缀"><a href="#操作数前缀" class="headerlink" title="操作数前缀"></a>操作数前缀</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">movl   $8,%eax </span><br><span class="line">movl   $0xffff,%ebx </span><br><span class="line">int     $0x80 </span><br></pre></td></tr></table></figure>
<p>看到在AT%T汇编中诸如”%eax”、”%ebx”之类的寄存器名字前都要加上”%”；”$8”、”$0xffff”这样的立即数之前都要加上”$”。  </p>
<h3 id="源-目的操作数顺序"><a href="#源-目的操作数顺序" class="headerlink" title="源/目的操作数顺序"></a>源/目的操作数顺序</h3><p>   在Intel语法中，第一个操作数是目的操作数，第二个操作数源操作数。而在AT&amp;T中，第一个数是源操作数，第二个数是目的操作数。 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; INTEL语法</span><br><span class="line">MOV EAX,8 &#x2F;&#x2F;EAX是目的操作数， 8是源操作数</span><br><span class="line">&#x2F;&#x2F; AT&amp;T语法</span><br><span class="line">movl   $8,%eax &#x2F;&#x2F;8是源操作数 EAX是目的操作数</span><br></pre></td></tr></table></figure>
<h3 id="标识长度的操作码后缀"><a href="#标识长度的操作码后缀" class="headerlink" title="标识长度的操作码后缀"></a>标识长度的操作码后缀</h3><p>在AT&amp;T的操作码后面有时还会有一个后缀，其含义就是指出操作码的大小。“l”表示长整数（32位），“w”表示字（16位），“b”表示字节（8位）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">movb    %bl,%al</span><br><span class="line">movw   %bx,%ax</span><br><span class="line">movl     %ebx,%eax</span><br><span class="line">movl     (%ebx),%eax</span><br></pre></td></tr></table></figure>
<h2 id="GCC内嵌汇编"><a href="#GCC内嵌汇编" class="headerlink" title="GCC内嵌汇编"></a>GCC内嵌汇编</h2><p>Linux操作系统内核代码绝大部分使用C语言编写，只有一小部分使用汇编语言编写，例如与特定体系结构相关的代码和对性能影响很大的代码。GCC提供了内嵌汇编的功能，可以在C代码中直接内嵌汇编语言语句，大大方便了程序设计。 </p>
<h3 id="基本行内汇编"><a href="#基本行内汇编" class="headerlink" title="基本行内汇编"></a>基本行内汇编</h3><p>基本行内汇编很容易理解，一般是按照下面的格式：</p>
<p>  <code>asm(“statements”);</code></p>
<p>在“asm”后面有时也会加上“<strong>volatile</strong>”表示编译器不要优化代码，后面的指令保留原样 </p>
<p> <code>__asm__  __volatile__(&quot;hlt&quot;);</code> </p>
<p>如果有很多行汇编，则每一行后要加上“\n\t” ：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> asm( &quot;pushl %eax\n\t&quot;</span><br><span class="line"> &quot;movl $0,%eax\n\t&quot;</span><br><span class="line">&quot;popl %eax&quot;); </span><br></pre></td></tr></table></figure>
<p>或者我们也可以分成几行来写，如： </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">asm(&quot;movl %eax,%ebx&quot;);</span><br><span class="line">asm(&quot;xorl %ebx,%edx&quot;);</span><br><span class="line">asm(&quot;movl $0,_booga); </span><br></pre></td></tr></table></figure>
<p>通常使用汇编语句最方便的方式是把它们放在一个宏内，而宏语句需要在一行上定义，因此使用反斜杠<code>\</code>将这些语句连成一行，所以上述语句如果在宏中定义的话就是：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"> <span class="keyword">asm</span>( <span class="string">&quot;pushl %eax; \</span></span><br><span class="line"><span class="string"> movl $0,%eax; \</span></span><br><span class="line"><span class="string">popl %eax&quot;</span>); </span><br></pre></td></tr></table></figure>
<h3 id="扩展的行内汇编"><a href="#扩展的行内汇编" class="headerlink" title="扩展的行内汇编"></a>扩展的行内汇编</h3><p>在扩展的行内汇编中，可以将C语言表达式（比如C语言中的变量）指定为汇编指令的操作数，而且不用去管如何将C语言表达式的值读入寄存器，以及如何将计算结果写回C变量，你只要告诉程序中C语言表达式与汇编指令操作数之间的对应关系即可， GCC会自动插入代码完成必要的操作。 </p>
<p>使用内嵌汇编，要先编写汇编指令模板，然后将C语言表达式与指令的操作数相关联，并告诉GCC对这些操作有哪些限制条件。例如下面的内嵌汇编语句： </p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> input = <span class="number">8</span>;</span><br><span class="line">	<span class="keyword">int</span> result = <span class="number">0</span>;</span><br><span class="line">    __asm__ __violate__  (<span class="string">&quot;movl %1,%0&quot;</span> : <span class="string">&quot;=r&quot;</span> (result) : <span class="string">&quot;r&quot;</span> (input));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>,result);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p><code>“movl %1,%0”</code>是指令模板；“%0”和“%1”代表指令的操作数，称为占位符，“=r”代表它之后是输入变量且需用到寄存器，指令模板后面用小括号括起来的是C语言表达式 ，其中input是输入变量，该指令会完成把input的值复制到result中的操作 。</p>
<h3 id="扩展的行内汇编的语法"><a href="#扩展的行内汇编的语法" class="headerlink" title="扩展的行内汇编的语法"></a>扩展的行内汇编的语法</h3><p>内嵌汇编语法如下：</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">asm(&quot;汇编语句模块&quot;</span><br><span class="line"></span><br><span class="line">     :输出寄存器 </span><br><span class="line"></span><br><span class="line">     :输入寄存器 </span><br><span class="line"></span><br><span class="line">     :会被修改的寄存器);</span><br></pre></td></tr></table></figure>
<p>即格式为<code>asm ( &quot;statements&quot; : output_regs : input_regs : clobbered_regs)</code></p>
<p>汇编语句模块必不可少，其他三部分可选，如果使用了后面的部分，而前面部分为空，也需要用“:”格开，相应部分内容为空。 </p>
<h4 id="汇编语句模块"><a href="#汇编语句模块" class="headerlink" title="汇编语句模块"></a>汇编语句模块</h4><p>汇编语句模块由汇编语句序列组成，语句之间使用“;”、“\n”或“\n\t”分开。指令中的操作数可以使用占位符引用C语言变量，<strong>操作数占位符</strong>最多10个，名称如下：%0，%1…，%9。指令中使用占位符表示的操作数，总被视为long型（4，个字节），但对其施加的操作根据指令可以是字或者字节，当把操作数当作字或者字节使用时，默认为低字或者低字节。对字节操作可以显式的指明是低字节还是次字节。方法是在%和序号之间插入一个字母，“b”代表低字节，“h”代表高字节，例如：%h1。 </p>
<p>占位符的理解：将汇编输出寄存器与输入寄存器从输出寄存器行开始左到右从上到下进行编号分别为：%0，%1…，%9。比如有代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#define get_seg_byte(seg,addr) \</span><br><span class="line">(&#123; \</span><br><span class="line">register char __res; \</span><br><span class="line">__asm__(&quot;push %%fs; \</span><br><span class="line">mov %%ax,%%fs; \</span><br><span class="line">movb %%fs:%2,%%al; \</span><br><span class="line">pop %%fs&quot; \</span><br><span class="line">:&quot;&#x3D;a&quot; (__res) \</span><br><span class="line">:&quot;&quot; (seg),&quot;m&quot; (*(addr))); \</span><br><span class="line">__res;&#125;)</span><br></pre></td></tr></table></figure>
<p>输出寄存器”=a”eax记为%0,输入寄存器””(依然是eax)记为%1,输入寄存器”m”为%2。</p>
<h4 id="输出寄存器"><a href="#输出寄存器" class="headerlink" title="输出寄存器"></a>输出寄存器</h4><p>描述输出操作数，不同的操作数描述符之间用逗号格开，每个操作数描述符由<strong>限定字符串和C语言变量</strong>组成。每个<strong>输出操作数的限定字符串必须包含“=”</strong>，表示它是一个输出操作数。例如：</p>
<p><code>__asm__   __volatile__ (&quot;pushfl ; popl %0 ; cli&quot;:&quot;=g&quot; (x) )</code></p>
<p>在这里“x”便是最终存放输出结果的C程序变量，而“=g”则是限定字符串，限定字符串表示了对它之后的变量的限制条件 。</p>
<h4 id="输入寄存器"><a href="#输入寄存器" class="headerlink" title="输入寄存器"></a>输入寄存器</h4><p>描述输入操作数，不同的操作数描述符之间使用逗号格开，每个操作数描述符同样也由限定字符串和C语言表达式或者C语言变量组成。例：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">__asm__ <span class="title">volatile__</span> <span class="params">(<span class="string">&quot;lidt %0&quot;</span> : : <span class="string">&quot;m&quot;</span> (real_mode_idt))</span></span>;</span><br></pre></td></tr></table></figure>
<p>其中%0是占位操作符，而输出寄存器为空，输入寄存器的值为C语言表达式real_mode_idt。</p>
<h4 id="限定字符串"><a href="#限定字符串" class="headerlink" title="限定字符串"></a>限定字符串</h4><p>又叫做寄存器加载代码</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>限定字符</th>
<th>描述</th>
<th>限定字符</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>使用寄存器eax</td>
<td>m</td>
<td>使用内存地址</td>
</tr>
<tr>
<td>b</td>
<td>使用寄存器ebx</td>
<td>o</td>
<td>使用内存地址并可以加偏移值</td>
</tr>
<tr>
<td>m、o、V、p</td>
<td>使用寄存器ecx</td>
<td>I</td>
<td>使用常数0~31 立即数</td>
</tr>
<tr>
<td>g、X</td>
<td>寄存器或内存</td>
<td>J</td>
<td>使用常数0~63 立即数</td>
</tr>
<tr>
<td>I、J、N、i、n</td>
<td>立即数</td>
<td>K</td>
<td>使用常数0~255立即数</td>
</tr>
<tr>
<td>D</td>
<td>使用edi</td>
<td>L</td>
<td>使用常数0~65535 立即数</td>
</tr>
<tr>
<td>q</td>
<td>使用动态分配字节可寻址寄存器（eax、ebx、ecx或edx）</td>
<td>M</td>
<td>使用常数0~3  立即数</td>
</tr>
<tr>
<td>r</td>
<td>使用任意动态分配的寄存器</td>
<td>N</td>
<td>使用1字节常数（0~255）立即数</td>
</tr>
<tr>
<td>g</td>
<td>使用通用有效的地址即可（eax、ebx、ecx、edx或内存变量）</td>
<td>O</td>
<td>使用常数0~31 立即数</td>
</tr>
<tr>
<td>A</td>
<td>使用eax与edx联合（64位）</td>
<td>i</td>
<td>立即数</td>
</tr>
</tbody>
</table>
</div>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>直接摘抄自《Linux内核完全注释》第5.2.2 traps.c程序的第82页</p>
<ul>
<li>例子1：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">01 #define get_seg_byte(seg,addr) \</span><br><span class="line">02 (&#123; \</span><br><span class="line">03 register char __res; \</span><br><span class="line">04 __asm__(&quot;push %%fs; \</span><br><span class="line">05 mov %%ax,%%fs; \</span><br><span class="line">06 movb %%fs:%2,%%al; \</span><br><span class="line">07 pop %%fs&quot; \</span><br><span class="line">08 :&quot;&#x3D;a&quot; (__res) \</span><br><span class="line">09 :&quot;&quot; (seg),&quot;m&quot; (*(addr))); \</span><br><span class="line">10 __res;&#125;)</span><br></pre></td></tr></table></figure>
<p> 第1 行定义了宏的名称，也即是宏函数名称 <code>get_seg_byte(seg,addr)</code> 。第 3 行定义了一个寄存器变量 res 。第 4 行上的 <code>__asm__</code>表示嵌入汇编语句的开始。从第 4 行到第 7 行的 4 条语句是 AT&amp;T 格式的汇编语句。</p>
<p>第 8 行即是输出寄存器，这句的含义是在这段代码运行结束后将 eax 所代表的寄存器的值放入<code>__res</code>变量中，作为本函数的输出值， “=a” 中的 “a” 称为加载代码， “=” 表示这是输出寄存器。第 9 行表示在这段代码开始运行时将 seg 放到 eax 寄存器中， “” 表示使用与上面同个位置的输出相同的寄存器。而 (<em>(addr))表示一个内存偏移地址值。为了在上面汇编语句中使用该地址值，嵌入汇编程序规定把输出和输入寄存器统一按顺序编号，顺序是从输出寄存器序列从左到右从上到下以 “%0” 开始，分别记为 %0 、 %1 、 …%9 。因此，输出寄存器的编号是 %0 （这里只有一个输出寄存器），输入寄存器前一部分 (“” (seg)) 的编号是 %1 ，而后部分的编号是 %2 。上面第 6 行上的 %2 即代表 (</em>(addr)) 这个内存偏移量。现在我们来研究 4—7 行上的代码的作用。第一句将 fs 段寄存器的内容入栈；第二句将 eax 中的段值赋给 fs 段寄存器；第三句是把 fs:(*(addr)) 所指定的字节放入 al 寄存器中。当执行完汇编语句后，输出寄存器 eax 的值将被放入 <code>__res</code> ，作为该宏函数（块结构表达式）的返回值。</p>
<p>通过上面分析，我们知道，宏名称中的 seg 代表一指定的内存段值，而 addr 表示一内存偏移地址量。到现在为止，我们应该很清楚这段程序的功能了吧！该宏函数的功能是从指定段和偏移值的内存地址处取一个字节。</p>
<p>通过上面的例子说明，阅读这段代码时应该像CPU处理指令时的逻辑一样，先从输出输入寄存器语句开始知道输入输出是什么，然后再阅读汇编语句，处理完后再看最后的返回是什么。</p>
<ul>
<li>例子2</li>
</ul>
<p>再来看下Linux 内核中main()中对中断异常挂接的trap_init()中的设计到的一个GCC内嵌汇编。代码路径为；include\asm\system.h</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 #define _set_gate(gate_addr,type,dpl,addr) \</span><br><span class="line">2 __asm__ (&quot;movw %%dx,%%ax\n\t&quot; \</span><br><span class="line">3	&quot;movw %0,%%dx\n\t&quot; \</span><br><span class="line">4	&quot;movl %%eax,%1\n\t&quot; \</span><br><span class="line">5	&quot;movl %%edx,%2&quot; \</span><br><span class="line">6	: \</span><br><span class="line">7	: &quot;i&quot; ((short) (0x8000+(dpl&lt;&lt;13)+(type&lt;&lt;8))), \</span><br><span class="line">8	&quot;o&quot; (*((char *) (gate_addr))), \</span><br><span class="line">9	&quot;o&quot; (*(4+(char *) (gate_addr))), \</span><br><span class="line">10	&quot;d&quot; ((char *) (addr)),&quot;a&quot; (0x00080000)) </span><br></pre></td></tr></table></figure>
<p>首先从第6行输出寄存器开始阅读：输出寄存器为空。然后是第7至10行的输入寄存器。第%0个输入寄存器使用“i”表示输入立即数，第%1个输入寄存器使用“o”表示使用内存地址并可以加偏移值，第%2个输入寄存器依然使用“o”代码，第%3个寄存器使用“d”表示使用寄存器edx，第%4个寄存器使用“a”表示使用寄存器eax。</p>
<p>之后再看下汇编语句：依次进行值的传递。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>《Linux内核设计的艺术》新设计团队 著</p>
<p>《Linux内核完全注释》赵炯 编著</p>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>汇编</tag>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title>OJ输入输出</title>
    <url>/2019-09-08-OJ%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA.html</url>
    <content><![CDATA[<h1 id="OJ输入输出"><a href="#OJ输入输出" class="headerlink" title="OJ输入输出"></a>OJ输入输出</h1><p>算法课要求打UOJ，实际操作后发现与leetcode，牛客这些只用写解决类不一样，OJ要求自己编写输入输出。所以对于输入输出还是很头痛，在此总结下。</p>
<a id="more"></a>
<h2 id="C-输入输出"><a href="#C-输入输出" class="headerlink" title="C++输入输出"></a>C++输入输出</h2><p>输入输出不是对文件进行操作的，可以理解成是在命令行中进行输入与输出。所以主要使用标准输入流cin进行数据的输入，标准输出流cout进行输出。因为有多组测试样例，所以一般需要放在while循环中来读取数据并进行操作。总思路是输入一组输入对应的输出一组输出，边输入边输出。</p>
<p>需要注意的是cin 会自动跳过空格、tab、换行符等不可见的符号，所以可以在同一行中输入a,b两个值，而不用自己去分割空格。</p>
<h3 id="只有一组输入输出"><a href="#只有一组输入输出" class="headerlink" title="只有一组输入输出"></a>只有一组输入输出</h3><p>直接从键盘获取一组输入，随后输出，以计算a+b为例。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt; iostream &gt;   </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">     <span class="keyword">int</span> a,b; </span><br><span class="line">     <span class="built_in">cin</span> &gt;&gt; a &gt;&gt; b;</span><br><span class="line">     <span class="built_in">cout</span> &lt;&lt; a+b &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="有多组测试数据，直到读至输入文件结尾为止"><a href="#有多组测试数据，直到读至输入文件结尾为止" class="headerlink" title="有多组测试数据，直到读至输入文件结尾为止"></a><strong>有多组测试数据，直到读至输入文件结尾为止</strong></h2><p>有多组测试数据，需要在while循环中读取数据并进行处理。当输入</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt; iostream &gt;    </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">       <span class="keyword">int</span> a,b;</span><br><span class="line">       <span class="keyword">while</span>(<span class="built_in">cin</span> &gt;&gt; a &gt;&gt; b)</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; a+b &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">       <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="在开始的时候输入一个N，接下来是N组数据"><a href="#在开始的时候输入一个N，接下来是N组数据" class="headerlink" title="在开始的时候输入一个N，接下来是N组数据"></a><strong>在开始的时候输入一个N，接下来是N组数据</strong></h2><p>在while循环中进行数据读入，需要注意的是如果后面需要用到n这个参数，需要使用临时变量来存储n，否则n在循环后会变成0.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a, b, n;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; n;</span><br><span class="line">    <span class="keyword">while</span> (n--) &#123;</span><br><span class="line">        <span class="built_in">cin</span>&gt;&gt;a&gt;&gt;b;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; a + b &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="未知输入数据量，但以某个特殊输入为结束标志"><a href="#未知输入数据量，但以某个特殊输入为结束标志" class="headerlink" title="未知输入数据量，但以某个特殊输入为结束标志"></a>未知输入数据量，但以某个特殊输入为结束标志</h2><p>当a或者b为0的时候结束输入，否则读入一组a，b并输出二者之和。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a ,b;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span>&gt;&gt;a&gt;&gt;b&amp;&amp;(a||b))&#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;a+b&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="重定向输入"><a href="#重定向输入" class="headerlink" title="重定向输入"></a>重定向输入</h2><p>将输入从控制台重定向到文件，从文件进行输入。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;  </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span>  </span></span><br><span class="line"><span class="function"></span>&#123;  </span><br><span class="line">    freopen(<span class="string">&quot;input.txt&quot;</span>,<span class="string">&quot;r&quot;</span>,<span class="built_in">stdin</span>);  <span class="comment">//输入将被重定向到文件</span></span><br><span class="line">    <span class="keyword">int</span> a,b;  </span><br><span class="line">    <span class="built_in">cin</span>&gt;&gt;a&gt;&gt;b;  </span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;a+b&lt;&lt;<span class="built_in">endl</span>;  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<h2 id="字符串输入"><a href="#字符串输入" class="headerlink" title="字符串输入"></a>字符串输入</h2><p>使用<code>cin.getline()</code>函数，其原型为：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">istream&amp; <span class="title">getline</span><span class="params">(<span class="keyword">char</span> line[], <span class="keyword">int</span> size, <span class="keyword">char</span> endchar = <span class="string">&#x27;\n&#x27;</span>)</span></span>;</span><br><span class="line"><span class="keyword">char</span> line[]： 就是一个字符数组，用户输入的内容将存入在该数组内。</span><br><span class="line"><span class="keyword">int</span> size : 最多接受几个字符，用户超过size的输入都将不被接受。</span><br><span class="line"><span class="keyword">char</span> endchar :当用户输入endchar指定的字符时，自动结束，默认是回车符。</span><br></pre></td></tr></table></figure>
<p>所以输入指定数目的字符串可以写成：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> buf[ <span class="number">255</span> ];</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span>.getline( buf, <span class="number">255</span> ));</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>也可以使用string类型来进行输入，如下程序循环输入pair组字符串，每组字符串有两个字符串用空格分开。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> <span class="keyword">const</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> <span class="built_in">pair</span>;</span><br><span class="line">    <span class="built_in">string</span> str1, str2;</span><br><span class="line">	<span class="keyword">while</span>(<span class="built_in">cin</span>&gt;&gt;<span class="built_in">pair</span>)&#123;</span><br><span class="line">		<span class="keyword">while</span>(<span class="built_in">pair</span>--)&#123;</span><br><span class="line">			<span class="built_in">cin</span>&gt;&gt;str1;</span><br><span class="line">			<span class="built_in">cin</span>&gt;&gt;str2;</span><br><span class="line">			<span class="built_in">cout</span>&lt;&lt;str1&lt;&lt;str2&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的输入样式为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2</span><br><span class="line">ABCD AEFC</span><br><span class="line">SCFEZ BNI</span><br><span class="line">3</span><br><span class="line">ABCD AEFC</span><br><span class="line">SCFEZ BNI</span><br><span class="line">ABCD XVC</span><br></pre></td></tr></table></figure>
<p>即第一次输入2组字符串，第一组字符串为：<code>ABCD 与 AEFC</code>这两个字符串，cin会跳过空格即自动把空格前的ABCD这个字符串作为str1的输入而把空格后面的<code>AEFC</code>作为str2的输入。第二组字符串为：<code>SCFEZ BNI</code> ,与前面同理不赘述。</p>
<p>第二次输入为3组字符串，与第一次同理。</p>
<h3 id="多组数据每组多行字符串"><a href="#多组数据每组多行字符串" class="headerlink" title="多组数据每组多行字符串"></a>多组数据每组多行字符串</h3><p>首先输入一个t，代表t组输入每组会有多行字符串输入。</p>
<figure class="highlight cc"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> t;</span><br><span class="line"><span class="built_in">cin</span> &gt;&gt; t;</span><br><span class="line"><span class="built_in">cin</span>.ignore();<span class="comment">// 忽略输入int 后的换行符</span></span><br><span class="line"><span class="keyword">char</span> c;</span><br><span class="line"><span class="built_in">string</span> a = <span class="string">&quot;&quot;</span>; </span><br><span class="line"><span class="built_in">string</span> b = <span class="string">&quot;&quot;</span>;</span><br><span class="line"><span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span>(t--)&#123;</span><br><span class="line">    getline(<span class="built_in">cin</span>, a);</span><br><span class="line">    getline(<span class="built_in">cin</span>, b);</span><br><span class="line">    <span class="keyword">bool</span> rtn = solution(a, b);</span><br><span class="line">    <span class="keyword">if</span> (rtn) <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;YES&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;NO&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    a = <span class="string">&quot;&quot;</span>;</span><br><span class="line">    b = <span class="string">&quot;&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要配合<code>cin.ignore()</code>来使用否则会让a读入空。</p>
<h2 id="不定数量的字符串-特定分隔符"><a href="#不定数量的字符串-特定分隔符" class="headerlink" title="不定数量的字符串 特定分隔符"></a>不定数量的字符串 特定分隔符</h2><p>读取不定数量的字符串，按逗号分割转换为数字并转到数组中。</p>
<p>stringstream 对象类似于cout ,cin方便完成字符串到数字的互相转换。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;sstream&gt;</span><br><span class="line">void input()&#123;</span><br><span class="line"></span><br><span class="line">    string str; </span><br><span class="line">    vector&lt;vector&lt;int&gt;&gt; in_vec_list;</span><br><span class="line"></span><br><span class="line">    while(cin &gt;&gt; str) &#123; &#x2F;&#x2F; 按行读取到string</span><br><span class="line">        in_vec_list.push_back(&#123;&#125;);</span><br><span class="line">        stringstream ss_line(str);&#x2F;&#x2F;string 转换到stringstream</span><br><span class="line">        string temp;</span><br><span class="line">        while (getline(ss_line, temp, &#39;,&#39;)) &#123; &#x2F;&#x2F; 对stringstream 按照&#39;,&#39;作为分隔符，把数据读取到temp 字符串</span><br><span class="line">            &#x2F;&#x2F;需要的话可以判断temp的字符串长度是不是为0，否则重复输入num。</span><br><span class="line">            stringstream ss_int(temp);</span><br><span class="line">            int num;</span><br><span class="line">            ss_int &gt;&gt; num; &#x2F;&#x2F; 字符串转到int</span><br><span class="line">            in_vec_list.back().push_back(num);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    for(int i &#x3D; 0; i &lt; in_vec_list.size(); ++i) &#123;</span><br><span class="line">        cout_vector(in_vec_list[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/qiao1245/article/details/53020326">https://blog.csdn.net/qiao1245/article/details/53020326</a> </p>
<p><a href="https://www.cnblogs.com/emerson027/articles/9319135.html">https://www.cnblogs.com/emerson027/articles/9319135.html</a> </p>
]]></content>
      <categories>
        <category>题解</category>
      </categories>
      <tags>
        <tag>OJ</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>实模式寻址方式</title>
    <url>/2019-09-08-%E5%AE%9E%E6%A8%A1%E5%BC%8F%E5%AF%BB%E5%9D%80%E6%96%B9%E5%BC%8F.html</url>
    <content><![CDATA[<h1 id="实模式寻址方式"><a href="#实模式寻址方式" class="headerlink" title="实模式寻址方式"></a>实模式寻址方式</h1><p>在上《操作系统高级教程》课时有BIOS启动时的实模式，讲到 CS与IP这两个寄存器，但是始终不能理解为什么说CS:IP可以表示一个物理地址，查询后才理解到与实模式的寻址方式有关。</p>
<a id="more"></a>
<h2 id="8086的寻址方式"><a href="#8086的寻址方式" class="headerlink" title="8086的寻址方式"></a>8086的寻址方式</h2><p>为了理解实模式，首先需要理解Intel 8086这个16位CPU的寻址模式。</p>
<p>8086的CPU是16位，即它的所有寄存器和寄存器之间的数据总线都是16位的，而其外部数据总线却是20位的。那么如何才能访问到20位也就是1MB的地址空间呢？显然一个寄存器是不够的所以就用两个寄存器来存储，这也就是段式寻址（内存分段）。段式寻址需要使用一个寄存器作为段寄存器比如CS代码段寄存器，使用另外一个寄存器比如IP指令指针寄存器作为偏移寄存器。计算物理地址的时候首先将段寄存器的16位地址向左移动4位（也就是最开始段寄存器的地址x16），然后将左移4位后的段地址与偏移地址相加就可以得到一个20位的物理地址了。也可以理解成最开始的段寄存器的地址默认后面还有4位，只是都省略成0，这样就可以放在16位寄存器中了。</p>
<h2 id="实模式寻址方式-1"><a href="#实模式寻址方式-1" class="headerlink" title="实模式寻址方式"></a>实模式寻址方式</h2><p>实模式（Real mode） 是Intel 80286和之后的80x86兼容CPU的操作模式，所有的80x86CPU的开机状态都是实模式。实模式的特性是一个20位的存储器地址空间（即1MB的存储器）可以被寻址。为了兼容，也为了解决最开始的启动问题，Intel将所有80x86系列的CPU（包括最新型号的64位CPU）的硬件都设计为加电即进入16位实模式状态运行。在实模式状态下寄存器是16位的，地址总线是20位的，也就是说实模式下寻址空间为1MB。因此与8086这个16位机器一样，为了实现20位寻址依然采取段式寻址方式。</p>
<p>开机时的第一个操作就是执行BIOS程序，CPU的硬件逻辑被设计为加电瞬间强行将CS的值设置为0XF000,IP的值设置为0XFFF0，这样CS:IP 的地址就是CS值左移四位变成0xF0000后加上IP的偏移地址也就是0xF0000+0XFFF0 也就是0xFFFF0，而0xFFFF0这个地址也就是BIOS程序的地址，CPU将开始执行这个地址的BIOS代码，从而开始BIOS的启动。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>《Linux内核设计的艺术》第二版</p>
<p><a href="https://zh.wikipedia.org/wiki/X86#%E7%9C%9F%E5%AF%A6%E6%A8%A1%E5%BC%8F">https://zh.wikipedia.org/wiki/X86#%E7%9C%9F%E5%AF%A6%E6%A8%A1%E5%BC%8F</a> </p>
<p><a href="https://zh.wikipedia.org/wiki/Intel_8086#%E5%86%85%E5%AD%98%E5%88%86%E6%AE%B5">https://zh.wikipedia.org/wiki/Intel_8086#%E5%86%85%E5%AD%98%E5%88%86%E6%AE%B5</a> </p>
<p><a href="https://zhuanlan.zhihu.com/p/69504370">https://zhuanlan.zhihu.com/p/69504370</a> </p>
<p><a href="https://blog.csdn.net/unix21/article/details/8450214">https://blog.csdn.net/unix21/article/details/8450214</a> </p>
]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>内存寻址</tag>
        <tag>寄存器</tag>
        <tag>BIOS</tag>
      </tags>
  </entry>
  <entry>
    <title>shadowsocks的部署</title>
    <url>/2019-08-24-shadowsocks%E7%9A%84%E9%83%A8%E7%BD%B2.html</url>
    <content><![CDATA[<h1 id="shadowsocks-的部署"><a href="#shadowsocks-的部署" class="headerlink" title="shadowsocks 的部署"></a>shadowsocks 的部署</h1><p>部署shadowsocks主要有两个作用：</p>
<ol>
<li>可以翻墙 </li>
<li>可以白嫖校园网的ipv6 </li>
</ol>
<a id="more"></a>
<h2 id="1-购买VPS服务器"><a href="#1-购买VPS服务器" class="headerlink" title="1. 购买VPS服务器"></a>1. 购买VPS服务器</h2><p>目前使用vultr的VPS服务器，5刀一个月，使用的CENTOS 8发行版。貌似日本的服务器网速最快，而且支持IPV6。在部署服务器的时候就需要开启IPV6。</p>
<h2 id="2-服务器设置"><a href="#2-服务器设置" class="headerlink" title="2. 服务器设置"></a>2. 服务器设置</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>在/root/目录下创建文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir shadowsocks</span><br><span class="line">cd shadowsocks/</span><br></pre></td></tr></table></figure>
<p>安装python和pip工具以及git</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">centos 8 服务器自带有python3 和 pip3 所以无需再安装python</span></span><br><span class="line"><span class="meta">#</span><span class="bash">yum install python-setuptools &amp;&amp; easy_install pip</span> </span><br><span class="line">yum -y install git </span><br></pre></td></tr></table></figure>
<p>使用pip3通过git安装shadowsocks</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">pip install git+https://github.com/shadowsocks/shadowsocks.git@master</span></span><br><span class="line">pip3 install git+https://github.com/shadowsocks/shadowsocks.git@master</span><br><span class="line">ssserver</span><br></pre></td></tr></table></figure>
<p>ssserver 命令用来查看是否安装成功</p>
<h3 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi shadowsocks.json</span><br></pre></td></tr></table></figure>
<p>脚本内容为：</p>
<p>多端口账户脚本如下</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;server&quot;</span>:<span class="string">&quot;::&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;local_address&quot;</span>: <span class="string">&quot;127.0.0.1&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;local_port&quot;</span>:<span class="number">1080</span>,</span><br><span class="line">    <span class="attr">&quot;port_password&quot;</span>:&#123;</span><br><span class="line">    <span class="attr">&quot;8388&quot;</span>:<span class="string">&quot;frankfurt123&quot;</span>,</span><br><span class="line"> 	<span class="attr">&quot;2343&quot;</span>:<span class="string">&quot;password&quot;</span></span><br><span class="line">&#125;,</span><br><span class="line">    <span class="attr">&quot;timeout&quot;</span>:<span class="number">300</span>,</span><br><span class="line">    <span class="attr">&quot;method&quot;</span>:<span class="string">&quot;aes-256-cfb&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;fast_open&quot;</span>: <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中server:”::” 用两个:: 冒号来表示可以同时使用ipv4和ipv6来访问服务器，如果就只有ipv4的地址的话，只能使用ipv4来翻墙。</p>
<p>通过脚本来启动shadowsocks</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ssserver -c /root/shadowsocks/shadowsocks.json -d start</span><br><span class="line">ssserver -c /root/shadowsocks/shadowsocks.json -d status</span><br><span class="line">ssserver -c /root/shadowsocks/shadowsocks.json -d stop</span><br></pre></td></tr></table></figure>
<p>至此通过添加服务器配置就应该可以使用shadowsocks+switchomega客户端了，如果还不可以的话，多半是因为防火墙的问题。</p>
<h3 id="防火墙设置"><a href="#防火墙设置" class="headerlink" title="防火墙设置"></a>防火墙设置</h3><p>将8388添加到防火墙白名单。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">firewall-cmd --zone&#x3D;public --add-port&#x3D;8388&#x2F;tcp --permanent</span><br><span class="line">firewall-cmd --reload</span><br><span class="line">firewall-cmd --list-ports</span><br></pre></td></tr></table></figure>
<h2 id="客户端设置"><a href="#客户端设置" class="headerlink" title="客户端设置"></a>客户端设置</h2><p>我使用的是win10 可以从：<a href="https://github.com/shadowsocks/shadowsocks-windows">https://github.com/shadowsocks/shadowsocks-windows</a> 下载</p>
<p>MacOS：<a href="https://github.com/shadowsocks/ShadowsocksX-NG/releases">https://github.com/shadowsocks/ShadowsocksX-NG/releases</a> </p>
<p>在服务器设置界面依次添加IPV6的地址、端口、密码、加密方式就可以使用IPV6进行白嫖了。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/shadowsocks/shadowsocks/tree/master">https://github.com/shadowsocks/shadowsocks/tree/master</a> </p>
]]></content>
      <categories>
        <category>deploy</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
        <tag>翻墙</tag>
        <tag>ipv6</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis Makefile注解</title>
    <url>/2019-04-26-Redis-Makefile%E6%B3%A8%E8%A7%A3.html</url>
    <content><![CDATA[<h1 id="Redis-Makefile注解"><a href="#Redis-Makefile注解" class="headerlink" title="Redis Makefile注解"></a>Redis Makefile注解</h1><p>Redis的makefile是阅读源码的第一步，总共有292行，读起来也是头大，记录之。</p>
<a id="more"></a>
<p>4.02版本源码为：</p>
<figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Redis Makefile</span></span><br><span class="line"><span class="comment"># Copyright (C) 2009 Salvatore Sanfilippo &lt;antirez at gmail dot com&gt;</span></span><br><span class="line"><span class="comment"># This file is released under the BSD license, see the COPYING file</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The Makefile composes the final FINAL_CFLAGS and FINAL_LDFLAGS using</span></span><br><span class="line"><span class="comment"># what is needed for Redis plus the standard CFLAGS and LDFLAGS passed.</span></span><br><span class="line"><span class="comment"># However when building the dependencies (Jemalloc, Lua, Hiredis, ...)</span></span><br><span class="line"><span class="comment"># CFLAGS and LDFLAGS are propagated to the dependencies, so to pass</span></span><br><span class="line"><span class="comment"># flags only to be used when compiling / linking Redis itself REDIS_CFLAGS</span></span><br><span class="line"><span class="comment"># and REDIS_LDFLAGS are used instead (this is the case of &#x27;make gcov&#x27;).</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Dependencies are stored in the Makefile.dep file. To rebuild this file</span></span><br><span class="line"><span class="comment"># Just use &#x27;make dep&#x27;, but this is only needed by developers.</span></span><br><span class="line"></span><br><span class="line">release_hdr := <span class="variable">$(<span class="built_in">shell</span> sh -c &#x27;./mkreleasehdr.sh&#x27;)</span></span><br><span class="line"><span class="comment"># uname -s 获取操作系统的类型 Linux</span></span><br><span class="line">uname_S := <span class="variable">$(<span class="built_in">shell</span> sh -c &#x27;uname -s 2&gt;/dev/null || echo not&#x27;)</span></span><br><span class="line"><span class="comment">#uname -m 获取机子的架构 x86_64</span></span><br><span class="line">uname_M := <span class="variable">$(<span class="built_in">shell</span> sh -c &#x27;uname -m 2&gt;/dev/null || echo not&#x27;)</span></span><br><span class="line"><span class="comment"># 优化选项</span></span><br><span class="line">OPTIMIZATION?=-O2</span><br><span class="line"><span class="comment"># 依赖目标</span></span><br><span class="line">DEPENDENCY_TARGETS=hiredis linenoise lua</span><br><span class="line">NODEPS:=clean distclean</span><br><span class="line"></span><br><span class="line"><span class="comment"># Default settings</span></span><br><span class="line"><span class="comment"># 使用c99标准编译，-pedantic 保证代码规范满足ISO C和ISO C++标准</span></span><br><span class="line">STD=-std=c99 -pedantic -DREDIS_STATIC=&#x27;&#x27;</span><br><span class="line"><span class="comment"># 输出所有编译警告信息 ，Wno-missing-field-initializers 不输出missing-的警告信息</span></span><br><span class="line">WARN=-Wall -W -Wno-missing-field-initializers</span><br><span class="line">OPT=<span class="variable">$(OPTIMIZATION)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#默认目录</span></span><br><span class="line">PREFIX?=/usr/local</span><br><span class="line"><span class="comment">#安装的默认目录</span></span><br><span class="line">INSTALL_BIN=<span class="variable">$(PREFIX)</span>/bin</span><br><span class="line">INSTALL=install</span><br><span class="line"></span><br><span class="line"><span class="comment"># Default allocator defaults to Jemalloc if it&#x27;s not an ARM</span></span><br><span class="line"><span class="comment">#内存分配器的指定 默认libc，linux系统而且架构不是armv61和71的时候则是jemalloc，</span></span><br><span class="line">MALLOC=libc</span><br><span class="line"><span class="keyword">ifneq</span> (<span class="variable">$(uname_M)</span>,armv6l)</span><br><span class="line"><span class="keyword">ifneq</span> (<span class="variable">$(uname_M)</span>,armv7l)</span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(uname_S)</span>,Linux)</span><br><span class="line">	MALLOC=jemalloc</span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To get ARM stack traces if Redis crashes we need a special C flag.</span></span><br><span class="line"><span class="keyword">ifneq</span> (,<span class="variable">$(<span class="built_in">findstring</span> armv,<span class="variable">$(uname_M)</span>)</span>)</span><br><span class="line">        CFLAGS+=-funwind-tables</span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Backwards compatibility for selecting an allocator</span></span><br><span class="line"><span class="comment">#编译的时候指定内存分配器</span></span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(USE_TCMALLOC)</span>,yes)</span><br><span class="line">	MALLOC=tcmalloc</span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(USE_TCMALLOC_MINIMAL)</span>,yes)</span><br><span class="line">	MALLOC=tcmalloc_minimal</span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(USE_JEMALLOC)</span>,yes)</span><br><span class="line">	MALLOC=jemalloc</span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(USE_JEMALLOC)</span>,no)</span><br><span class="line">	MALLOC=libc</span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Override default settings if possible</span></span><br><span class="line"><span class="keyword">-include</span> .make-settings</span><br><span class="line"><span class="comment"># 最终的编译选项CFLAGS是-c的选项，LDFLAGS是链接的选项</span></span><br><span class="line">FINAL_CFLAGS=<span class="variable">$(STD)</span> <span class="variable">$(WARN)</span> <span class="variable">$(OPT)</span> <span class="variable">$(DEBUG)</span> <span class="variable">$(CFLAGS)</span> <span class="variable">$(REDIS_CFLAGS)</span></span><br><span class="line">FINAL_LDFLAGS=<span class="variable">$(LDFLAGS)</span> <span class="variable">$(REDIS_LDFLAGS)</span> <span class="variable">$(DEBUG)</span></span><br><span class="line"><span class="comment"># m这个lib是libmath 也就是math的链接</span></span><br><span class="line">FINAL_LIBS=-lm</span><br><span class="line"><span class="comment"># 调试信息</span></span><br><span class="line">DEBUG=-g -ggdb</span><br><span class="line"><span class="comment">#根据操作系统继续指定编译选项</span></span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(uname_S)</span>,SunOS)</span><br><span class="line">	<span class="comment"># SunOS</span></span><br><span class="line">        <span class="keyword">ifneq</span> ($(@@),32bit)</span><br><span class="line">		CFLAGS+= -m64</span><br><span class="line">		LDFLAGS+= -m64</span><br><span class="line">	<span class="keyword">endif</span></span><br><span class="line">	DEBUG=-g</span><br><span class="line">	DEBUG_FLAGS=-g</span><br><span class="line">	<span class="keyword">export</span> CFLAGS LDFLAGS DEBUG DEBUG_FLAGS</span><br><span class="line">	INSTALL=cp -pf</span><br><span class="line">	FINAL_CFLAGS+= -D__EXTENSIONS__ -D_XPG6</span><br><span class="line">	FINAL_LIBS+= -ldl -lnsl -lsocket -lresolv -lpthread -lrt</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(uname_S)</span>,Darwin)</span><br><span class="line">	<span class="comment"># Darwin</span></span><br><span class="line">	FINAL_LIBS+= -ldl</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(uname_S)</span>,AIX)</span><br><span class="line">        <span class="comment"># AIX</span></span><br><span class="line">        FINAL_LDFLAGS+= -Wl,-bexpall</span><br><span class="line">        FINAL_LIBS+=-ldl -pthread -lcrypt -lbsd</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(uname_S)</span>,OpenBSD)</span><br><span class="line">	<span class="comment"># OpenBSD</span></span><br><span class="line">	FINAL_LIBS+= -lpthread</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(uname_S)</span>,FreeBSD)</span><br><span class="line">	<span class="comment"># FreeBSD</span></span><br><span class="line">	FINAL_LIBS+= -lpthread</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">	<span class="comment"># 特别是对Linux的指定</span></span><br><span class="line">	<span class="comment"># All the other OSes (notably Linux)</span></span><br><span class="line">	<span class="comment"># -rdynamic将链接器将所有符号添加到动态符号表</span></span><br><span class="line">	FINAL_LDFLAGS+= -rdynamic</span><br><span class="line">	<span class="comment">#pthread库 用于多线程， dl是libdl 动态链接库</span></span><br><span class="line">	FINAL_LIBS+=-ldl -pthread</span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"><span class="comment"># Include paths to dependencies</span></span><br><span class="line"><span class="comment"># -I 指定头文件的目录</span></span><br><span class="line">FINAL_CFLAGS+= -I../deps/hiredis -I../deps/linenoise -I../deps/lua/src</span><br><span class="line"></span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(MALLOC)</span>,tcmalloc)</span><br><span class="line">	FINAL_CFLAGS+= -DUSE_TCMALLOC</span><br><span class="line">	FINAL_LIBS+= -ltcmalloc</span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(MALLOC)</span>,tcmalloc_minimal)</span><br><span class="line">	FINAL_CFLAGS+= -DUSE_TCMALLOC</span><br><span class="line">	FINAL_LIBS+= -ltcmalloc_minimal</span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"><span class="comment">#使用jemalloc的话 链接 libjemalloc.a -I指定jemalloc的头文件目录</span></span><br><span class="line"><span class="keyword">ifeq</span> (<span class="variable">$(MALLOC)</span>,jemalloc)</span><br><span class="line">	DEPENDENCY_TARGETS+= jemalloc</span><br><span class="line">	FINAL_CFLAGS+= -DUSE_JEMALLOC -I../deps/jemalloc/<span class="keyword">include</span></span><br><span class="line">	FINAL_LIBS+= ../deps/jemalloc/lib/libjemalloc.a</span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"><span class="comment">#redis 的gcc -c 选项</span></span><br><span class="line">REDIS_CC=<span class="variable">$(QUIET_CC)</span><span class="variable">$(CC)</span> <span class="variable">$(FINAL_CFLAGS)</span></span><br><span class="line"><span class="comment">#redis的gcc 链接选项</span></span><br><span class="line">REDIS_LD=<span class="variable">$(QUIET_LINK)</span><span class="variable">$(CC)</span> <span class="variable">$(FINAL_LDFLAGS)</span></span><br><span class="line"><span class="comment">#redis的安装选项</span></span><br><span class="line">REDIS_INSTALL=<span class="variable">$(QUIET_INSTALL)</span><span class="variable">$(INSTALL)</span></span><br><span class="line"></span><br><span class="line">CCCOLOR=<span class="string">&quot;\033[34m&quot;</span></span><br><span class="line">LINKCOLOR=<span class="string">&quot;\033[34;1m&quot;</span></span><br><span class="line">SRCCOLOR=<span class="string">&quot;\033[33m&quot;</span></span><br><span class="line">BINCOLOR=<span class="string">&quot;\033[37;1m&quot;</span></span><br><span class="line">MAKECOLOR=<span class="string">&quot;\033[32;1m&quot;</span></span><br><span class="line">ENDCOLOR=<span class="string">&quot;\033[0m&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ifndef</span> V</span><br><span class="line">QUIET_CC = @printf &#x27;    %b %b\n&#x27; <span class="variable">$(CCCOLOR)</span>CC<span class="variable">$(ENDCOLOR)</span> <span class="variable">$(SRCCOLOR)</span><span class="variable">$@</span><span class="variable">$(ENDCOLOR)</span> 1&gt;&amp;2;</span><br><span class="line">QUIET_LINK = @printf &#x27;    %b %b\n&#x27; <span class="variable">$(LINKCOLOR)</span>LINK<span class="variable">$(ENDCOLOR)</span> <span class="variable">$(BINCOLOR)</span><span class="variable">$@</span><span class="variable">$(ENDCOLOR)</span> 1&gt;&amp;2;</span><br><span class="line">QUIET_INSTALL = @printf &#x27;    %b %b\n&#x27; <span class="variable">$(LINKCOLOR)</span>INSTALL<span class="variable">$(ENDCOLOR)</span> <span class="variable">$(BINCOLOR)</span><span class="variable">$@</span><span class="variable">$(ENDCOLOR)</span> 1&gt;&amp;2;</span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"></span><br><span class="line">REDIS_SERVER_NAME=redis-server</span><br><span class="line">REDIS_SENTINEL_NAME=redis-sentinel</span><br><span class="line"><span class="comment"># redis-server的需要使用的对象文件，也就是各个模块</span></span><br><span class="line">REDIS_SERVER_OBJ=adlist.o quicklist.o ae.o anet.o dict.o server.o sds.o zmalloc.o lzf_c.o lzf_d.o pqsort.o zipmap.o sha1.o ziplist.o release.o networking.o util.o object.o db.o replication.o rdb.o t_string.o t_list.o t_set.o t_zset.o t_hash.o config.o aof.o pubsub.o multi.o debug.o sort.o intset.o syncio.o cluster.o crc16.o endianconv.o slowlog.o scripting.o bio.o rio.o rand.o memtest.o crc64.o bitops.o sentinel.o notify.o setproctitle.o blocked.o hyperloglog.o latency.o sparkline.o redis-check-rdb.o redis-check-aof.o geo.o lazyfree.o module.o evict.o expire.o geohash.o geohash_helper.o childinfo.o defrag.o siphash.o rax.o</span><br><span class="line">REDIS_CLI_NAME=redis-cli</span><br><span class="line"><span class="comment">#redis-cli 需要使用的对象文件</span></span><br><span class="line">REDIS_CLI_OBJ=anet.o adlist.o redis-cli.o zmalloc.o release.o anet.o ae.o crc64.o</span><br><span class="line">REDIS_BENCHMARK_NAME=redis-benchmark</span><br><span class="line"><span class="comment">#redis-benchmark需要使用的对象文件</span></span><br><span class="line">REDIS_BENCHMARK_OBJ=ae.o anet.o redis-benchmark.o adlist.o zmalloc.o redis-benchmark.o</span><br><span class="line">REDIS_CHECK_RDB_NAME=redis-check-rdb</span><br><span class="line">REDIS_CHECK_AOF_NAME=redis-check-aof</span><br><span class="line"><span class="comment">#所有需要需要构建的对象，第一条规则也就是默认规则，不指定规则的话，从第一个规则执行</span></span><br><span class="line"><span class="section">all: <span class="variable">$(REDIS_SERVER_NAME)</span> <span class="variable">$(REDIS_SENTINEL_NAME)</span> <span class="variable">$(REDIS_CLI_NAME)</span> <span class="variable">$(REDIS_BENCHMARK_NAME)</span> <span class="variable">$(REDIS_CHECK_RDB_NAME)</span> <span class="variable">$(REDIS_CHECK_AOF_NAME)</span></span></span><br><span class="line">	@echo <span class="string">&quot;&quot;</span></span><br><span class="line">	@echo <span class="string">&quot;Hint: It&#x27;s a good idea to run &#x27;make test&#x27; ;)&quot;</span></span><br><span class="line">	@echo <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="comment">#Makefil.dep 的生成</span></span><br><span class="line"><span class="section">Makefile.dep:</span></span><br><span class="line">	-<span class="variable">$(REDIS_CC)</span> -MM *.c &gt; Makefile.dep 2&gt; /dev/null || true</span><br><span class="line"></span><br><span class="line"><span class="keyword">ifeq</span> (0, <span class="variable">$(words $(<span class="built_in">findstring</span> <span class="variable">$(MAKECMDGOALS)</span>, <span class="variable">$(NODEPS)</span>)</span>))</span><br><span class="line"><span class="keyword">-include</span> Makefile.dep</span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="meta-keyword">.PHONY</span>: all</span></span><br><span class="line"><span class="comment">#先清除所有编译的输出然后，  将所有设置持久化</span></span><br><span class="line"><span class="section">persist-settings: distclean</span></span><br><span class="line">	echo STD=<span class="variable">$(STD)</span> &gt;&gt; .make-settings</span><br><span class="line">	echo WARN=<span class="variable">$(WARN)</span> &gt;&gt; .make-settings</span><br><span class="line">	echo OPT=<span class="variable">$(OPT)</span> &gt;&gt; .make-settings</span><br><span class="line">	echo MALLOC=<span class="variable">$(MALLOC)</span> &gt;&gt; .make-settings</span><br><span class="line">	echo CFLAGS=<span class="variable">$(CFLAGS)</span> &gt;&gt; .make-settings</span><br><span class="line">	echo LDFLAGS=<span class="variable">$(LDFLAGS)</span> &gt;&gt; .make-settings</span><br><span class="line">	echo REDIS_CFLAGS=<span class="variable">$(REDIS_CFLAGS)</span> &gt;&gt; .make-settings</span><br><span class="line">	echo REDIS_LDFLAGS=<span class="variable">$(REDIS_LDFLAGS)</span> &gt;&gt; .make-settings</span><br><span class="line">	echo PREV_FINAL_CFLAGS=<span class="variable">$(FINAL_CFLAGS)</span> &gt;&gt; .make-settings</span><br><span class="line">	echo PREV_FINAL_LDFLAGS=<span class="variable">$(FINAL_LDFLAGS)</span> &gt;&gt; .make-settings</span><br><span class="line">	-(cd ../deps &amp;&amp; <span class="variable">$(MAKE)</span> <span class="variable">$(DEPENDENCY_TARGETS)</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="meta-keyword">.PHONY</span>: persist-settings</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Prerequisites target</span></span><br><span class="line"><span class="section">.make-prerequisites:</span></span><br><span class="line">	@touch <span class="variable">$@</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Clean everything, persist settings and build dependencies if anything changed</span></span><br><span class="line"><span class="comment">#当设置有变化的时候清除并重新持久化设置</span></span><br><span class="line"><span class="keyword">ifneq</span> (<span class="variable">$(<span class="built_in">strip</span> <span class="variable">$(PREV_FINAL_CFLAGS)</span>)</span>, <span class="variable">$(<span class="built_in">strip</span> <span class="variable">$(FINAL_CFLAGS)</span>)</span>)</span><br><span class="line"><span class="section">.make-prerequisites: persist-settings</span></span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ifneq</span> (<span class="variable">$(<span class="built_in">strip</span> <span class="variable">$(PREV_FINAL_LDFLAGS)</span>)</span>, <span class="variable">$(<span class="built_in">strip</span> <span class="variable">$(FINAL_LDFLAGS)</span>)</span>)</span><br><span class="line"><span class="section">.make-prerequisites: persist-settings</span></span><br><span class="line"><span class="keyword">endif</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># redis-server</span></span><br><span class="line"><span class="comment">#redis-server可执行程序的链接，需要链接的静态链接文件包括hiredi和lua,还有final_libs</span></span><br><span class="line"><span class="variable">$(REDIS_SERVER_NAME)</span>: <span class="variable">$(REDIS_SERVER_OBJ)</span></span><br><span class="line">	<span class="variable">$(REDIS_LD)</span> -o <span class="variable">$@</span> <span class="variable">$^</span> ../deps/hiredis/libhiredis.a ../deps/lua/src/liblua.a <span class="variable">$(FINAL_LIBS)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># redis-sentinel</span></span><br><span class="line"><span class="comment">#redis-sentienl构建</span></span><br><span class="line"><span class="variable">$(REDIS_SENTINEL_NAME)</span>: <span class="variable">$(REDIS_SERVER_NAME)</span></span><br><span class="line">	<span class="variable">$(REDIS_INSTALL)</span> <span class="variable">$(REDIS_SERVER_NAME)</span> <span class="variable">$(REDIS_SENTINEL_NAME)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># redis-check-rdb</span></span><br><span class="line"><span class="comment">#redis-check-rdb的构建</span></span><br><span class="line"><span class="variable">$(REDIS_CHECK_RDB_NAME)</span>: <span class="variable">$(REDIS_SERVER_NAME)</span></span><br><span class="line">	<span class="variable">$(REDIS_INSTALL)</span> <span class="variable">$(REDIS_SERVER_NAME)</span> <span class="variable">$(REDIS_CHECK_RDB_NAME)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># redis-check-aof</span></span><br><span class="line"><span class="comment">#redis-check-aof的构建</span></span><br><span class="line"><span class="variable">$(REDIS_CHECK_AOF_NAME)</span>: <span class="variable">$(REDIS_SERVER_NAME)</span></span><br><span class="line">	<span class="variable">$(REDIS_INSTALL)</span> <span class="variable">$(REDIS_SERVER_NAME)</span> <span class="variable">$(REDIS_CHECK_AOF_NAME)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># redis-cli</span></span><br><span class="line"><span class="comment">#redis-cli的链接</span></span><br><span class="line"><span class="variable">$(REDIS_CLI_NAME)</span>: <span class="variable">$(REDIS_CLI_OBJ)</span></span><br><span class="line">	<span class="variable">$(REDIS_LD)</span> -o <span class="variable">$@</span> <span class="variable">$^</span> ../deps/hiredis/libhiredis.a ../deps/linenoise/linenoise.o <span class="variable">$(FINAL_LIBS)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># redis-benchmark</span></span><br><span class="line"><span class="comment">#redis-benchmark的链接</span></span><br><span class="line"><span class="variable">$(REDIS_BENCHMARK_NAME)</span>: <span class="variable">$(REDIS_BENCHMARK_OBJ)</span></span><br><span class="line">	<span class="variable">$(REDIS_LD)</span> -o <span class="variable">$@</span> <span class="variable">$^</span> ../deps/hiredis/libhiredis.a <span class="variable">$(FINAL_LIBS)</span></span><br><span class="line"></span><br><span class="line"><span class="section">dict-benchmark: dict.c zmalloc.c sds.c siphash.c</span></span><br><span class="line">	<span class="variable">$(REDIS_CC)</span> <span class="variable">$(FINAL_CFLAGS)</span> <span class="variable">$^</span> -D DICT_BENCHMARK_MAIN -o <span class="variable">$@</span> <span class="variable">$(FINAL_LIBS)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Because the jemalloc.h header is generated as a part of the jemalloc build,</span></span><br><span class="line"><span class="comment"># building it should complete before building any other object. Instead of</span></span><br><span class="line"><span class="comment"># depending on a single artifact, build all dependencies first.</span></span><br><span class="line"><span class="comment">#将所有点c文件编译成.o文件 自动完成file.c 到file.o的对应</span></span><br><span class="line"><span class="section">%.o: %.c .make-prerequisites</span></span><br><span class="line">	<span class="variable">$(REDIS_CC)</span> -c <span class="variable">$&lt;</span></span><br><span class="line"></span><br><span class="line"><span class="section">clean:</span></span><br><span class="line">	rm -rf <span class="variable">$(REDIS_SERVER_NAME)</span> <span class="variable">$(REDIS_SENTINEL_NAME)</span> <span class="variable">$(REDIS_CLI_NAME)</span> <span class="variable">$(REDIS_BENCHMARK_NAME)</span> <span class="variable">$(REDIS_CHECK_RDB_NAME)</span> <span class="variable">$(REDIS_CHECK_AOF_NAME)</span> *.o *.gcda *.gcno *.gcov redis.info lcov-html Makefile.dep dict-benchmark</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="meta-keyword">.PHONY</span>: clean</span></span><br><span class="line"></span><br><span class="line"><span class="section">distclean: clean</span></span><br><span class="line">	-(cd ../deps &amp;&amp; <span class="variable">$(MAKE)</span> distclean)</span><br><span class="line">	-(rm -f .make-*)</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="meta-keyword">.PHONY</span>: distclean</span></span><br><span class="line"></span><br><span class="line"><span class="section">test: <span class="variable">$(REDIS_SERVER_NAME)</span> <span class="variable">$(REDIS_CHECK_AOF_NAME)</span></span></span><br><span class="line">	@(cd ..; ./runtest)</span><br><span class="line"></span><br><span class="line"><span class="section">test-sentinel: <span class="variable">$(REDIS_SENTINEL_NAME)</span></span></span><br><span class="line">	@(cd ..; ./runtest-sentinel)</span><br><span class="line"></span><br><span class="line"><span class="section">check: test</span></span><br><span class="line"></span><br><span class="line"><span class="section">lcov:</span></span><br><span class="line">	<span class="variable">$(MAKE)</span> gcov</span><br><span class="line">	@(set -e; cd ..; ./runtest --clients 1)</span><br><span class="line">	@geninfo -o redis.info .</span><br><span class="line">	@genhtml --legend -o lcov-html redis.info</span><br><span class="line"></span><br><span class="line"><span class="section">test-sds: sds.c sds.h</span></span><br><span class="line">	<span class="variable">$(REDIS_CC)</span> sds.c zmalloc.c -DSDS_TEST_MAIN <span class="variable">$(FINAL_LIBS)</span> -o /tmp/sds_test</span><br><span class="line">	/tmp/sds_test</span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="meta-keyword">.PHONY</span>: lcov</span></span><br><span class="line"></span><br><span class="line"><span class="section">bench: <span class="variable">$(REDIS_BENCHMARK_NAME)</span></span></span><br><span class="line">	./<span class="variable">$(REDIS_BENCHMARK_NAME)</span></span><br><span class="line"></span><br><span class="line"><span class="section">32bit:</span></span><br><span class="line">	@echo <span class="string">&quot;&quot;</span></span><br><span class="line">	@echo <span class="string">&quot;WARNING: if it fails under Linux you probably need to install libc6-dev-i386&quot;</span></span><br><span class="line">	@echo <span class="string">&quot;&quot;</span></span><br><span class="line">	<span class="variable">$(MAKE)</span> CFLAGS=<span class="string">&quot;-m32&quot;</span> LDFLAGS=<span class="string">&quot;-m32&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="section">gcov:</span></span><br><span class="line">	<span class="variable">$(MAKE)</span> REDIS_CFLAGS=<span class="string">&quot;-fprofile-arcs -ftest-coverage -DCOVERAGE_TEST&quot;</span> REDIS_LDFLAGS=<span class="string">&quot;-fprofile-arcs -ftest-coverage&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="section">noopt:</span></span><br><span class="line">	<span class="variable">$(MAKE)</span> OPTIMIZATION=<span class="string">&quot;-O0&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="section">valgrind:</span></span><br><span class="line">	<span class="variable">$(MAKE)</span> OPTIMIZATION=<span class="string">&quot;-O0&quot;</span> MALLOC=<span class="string">&quot;libc&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="section">helgrind:</span></span><br><span class="line">	<span class="variable">$(MAKE)</span> OPTIMIZATION=<span class="string">&quot;-O0&quot;</span> MALLOC=<span class="string">&quot;libc&quot;</span> CFLAGS=<span class="string">&quot;-D__ATOMIC_VAR_FORCE_SYNC_MACROS&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="section">src/help.h:</span></span><br><span class="line">	@../utils/generate-command-help.rb &gt; help.h</span><br><span class="line"><span class="comment">#将构建完成的可执行程序安装到指定的目录，-p选项自行创建多层目录</span></span><br><span class="line"><span class="section">install: all</span></span><br><span class="line">	@mkdir -p <span class="variable">$(INSTALL_BIN)</span></span><br><span class="line">	<span class="variable">$(REDIS_INSTALL)</span> <span class="variable">$(REDIS_SERVER_NAME)</span> <span class="variable">$(INSTALL_BIN)</span></span><br><span class="line">	<span class="variable">$(REDIS_INSTALL)</span> <span class="variable">$(REDIS_BENCHMARK_NAME)</span> <span class="variable">$(INSTALL_BIN)</span></span><br><span class="line">	<span class="variable">$(REDIS_INSTALL)</span> <span class="variable">$(REDIS_CLI_NAME)</span> <span class="variable">$(INSTALL_BIN)</span></span><br><span class="line">	<span class="variable">$(REDIS_INSTALL)</span> <span class="variable">$(REDIS_CHECK_RDB_NAME)</span> <span class="variable">$(INSTALL_BIN)</span></span><br><span class="line">	<span class="variable">$(REDIS_INSTALL)</span> <span class="variable">$(REDIS_CHECK_AOF_NAME)</span> <span class="variable">$(INSTALL_BIN)</span></span><br><span class="line">	@ln -sf <span class="variable">$(REDIS_SERVER_NAME)</span> <span class="variable">$(INSTALL_BIN)</span>/<span class="variable">$(REDIS_SENTINEL_NAME)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>uname_S := $(shell sh -c &#39;uname -s 2&gt;/dev/null || echo not&#39;)</code> 这一句-c让后面的字符串命令当成一个完成的命令来执行，从而避免向文件中写入东西的时候权限不够的问题。就算加上sudo也不行，因为里面的命令有&gt;,echo等很多个文件，所以只能用-c来当成一个整体来执行。<a href="https://blog.csdn.net/bobchill/article/details/84647575">参考</a></p>
<h2 id="Makefile思路"><a href="#Makefile思路" class="headerlink" title="Makefile思路"></a>Makefile思路</h2><p>总结一下Redis Makefile的思路：</p>
<ol>
<li>在默认规则也就是第一条规则之前，通过变量设置好编译的相关选项：LDFLAGS，相应的对应关系REDIS_SERVER_OBJ，将规则的target用变量表示好（方便all规则里面用作前置条件），比如REDIS_SERVER_NAME。</li>
<li>在第一条默认规则 all规则里面指定需要构建的东西</li>
<li>在第一规则后面先完成链接，再完成编译的规则</li>
<li>其他功能性规则如clean和distclean</li>
</ol>
<p>也就是从上到下的结构是总-分。显示整个项目 ，然后是各个模块如redis-server，redis-cli的链接，然后是从源文件到obj文件的编译。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/bobchill/article/details/84647575">https://blog.csdn.net/bobchill/article/details/84647575</a></p>
]]></content>
      <categories>
        <category>源码阅读</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>Makefile</tag>
      </tags>
  </entry>
  <entry>
    <title>GCC的编译选项</title>
    <url>/2019-04-25-GCC%E7%9A%84%E7%BC%96%E8%AF%91%E9%80%89%E9%A1%B9.html</url>
    <content><![CDATA[<h1 id="GCC的编译选项"><a href="#GCC的编译选项" class="headerlink" title="GCC的编译选项"></a>GCC的编译选项</h1><p>Makefile 中的gcc的编译选项有很多，因此学习记录下。</p>
<a id="more"></a>
<p>这些选项在Makefile文件中常用CFLAGS（gcc 用在c语言的编译），CXXFLAGS（g++用在c++的编译）来表示。</p>
<h2 id="程序编译的过程"><a href="#程序编译的过程" class="headerlink" title="程序编译的过程"></a>程序编译的过程</h2><p>gcc 与 g++ 分别是 gnu 的 c &amp; c++ 编译器 gcc/g++ 在执行编译工作的时候，总共需要4步：</p>
<ul>
<li>1、预处理,生成 .i 的文件[预处理器cpp]</li>
<li>2、将预处理后的文件转换成汇编语言, 生成文件 .s [编译器egcs]</li>
<li>3、将汇编代码变为目标代码(机器代码)生成 .o 的obj文件[汇编器as]</li>
<li>4、连接目标代码, 生成可执行程序 [链接器ld]</li>
</ul>
<h2 id="选项"><a href="#选项" class="headerlink" title="选项"></a>选项</h2><div class="table-container">
<table>
<thead>
<tr>
<th>选项</th>
<th>释义</th>
</tr>
</thead>
<tbody>
<tr>
<td>-c</td>
<td>只激活预处理,编译,和汇编。只把程序做成obj文件，不是可执行文件（因为没有链接,有的程序中也没有main入口）</td>
</tr>
<tr>
<td>-S</td>
<td>只激活预处理和编译，就是只把文件编译成为汇编代码。生成.s的汇编代码</td>
</tr>
<tr>
<td>-E</td>
<td>只激活预处理,不生成文件,你需要把它重定向到一个输出文件里面 例子：gcc -E hello.c &gt; pianoapan.txt</td>
</tr>
<tr>
<td>-o</td>
<td>指定输出，缺省的时候,gcc 编译出来的文件是a.out</td>
</tr>
<tr>
<td>-Wall</td>
<td>显示所有警告信息</td>
</tr>
<tr>
<td>-w</td>
<td>不生成任何警告信息。</td>
</tr>
<tr>
<td>-Wextra</td>
<td>打印出更多的警告信息，比开启 -Wall 打印的还多</td>
</tr>
<tr>
<td>-ansi</td>
<td>关闭gnu c中与ansi c不兼容的特性,激活ansi c的专有特性(包括禁止一些asm inline typeof关键字,以及UNIX,vax等预处理宏</td>
</tr>
<tr>
<td>-include file</td>
<td>包含某个代码,简单来说,就是便于某个文件需要另一个文件的时候,就可以用它设<br>定,功能就相当于在代码中使用#include<filename></filename></td>
</tr>
<tr>
<td>-Idir</td>
<td>添加dir目录为头文件搜索路径，如-I./ 在当前目录查找头文件</td>
</tr>
<tr>
<td>-I-</td>
<td>取消前一个参数的功能,所以一般在-Idir之后使用</td>
</tr>
<tr>
<td>-llib</td>
<td>指定编译的时候使用的库，gcc -lcurses hello.c 使用库curses进行编译</td>
</tr>
<tr>
<td>-std=</td>
<td>编译的标准,包括GNU99，c++11,c99,等等</td>
</tr>
<tr>
<td>-O2</td>
<td>编译器的优化选项的4个级别，-O0表示没有优化,-O1为缺省值，-O3优化级别最高</td>
</tr>
<tr>
<td>-Ldir</td>
<td>链接的时候，搜索库的路径 -L./ 在当前目录搜说</td>
</tr>
<tr>
<td>-g</td>
<td>产生调试信息，可以使用gdb调试可执行文件</td>
</tr>
<tr>
<td>-ggdb</td>
<td>此选项将尽可能的生成gdb的可以使用的调试信息.</td>
</tr>
<tr>
<td>-static</td>
<td>禁止使用动态库，所以，编译出来的东西，一般都很大，也不需要什么</td>
</tr>
<tr>
<td>-share</td>
<td>此选项将尽量使用动态库，所以生成文件比较小，但是需要系统由动态库.</td>
</tr>
<tr>
<td>-shared</td>
<td>创建一个动态链接库（不指定的话输出的是obj文件）gcc -fPIC -shared func.c -o libfunc.s</td>
</tr>
<tr>
<td>-rdynamic</td>
<td>动态连接符号信息，用于动态连接功能。所有符号添加到动态符号表中（目的是能够通过使用 dlopen 来实现向后跟踪）</td>
</tr>
<tr>
<td>-pedantic</td>
<td>用于保证代码规范满足ISO C和ISO C++标准, 不允许使用任何扩展以及不满足ISO C和C++的代码, 遵守 -std 选项指定的标准</td>
</tr>
<tr>
<td>-pthread</td>
<td>支持多线程, 使用pthread库</td>
</tr>
<tr>
<td>-fPIC</td>
<td>PIC 是 position-independent code的意思, 此选项去除独立位置代码, 适合于动态链接</td>
</tr>
</tbody>
</table>
</div>
<p>ar -r libhello.a hello.o  #这里的ar相当于tar的作用，将多个目标打包。 makefile中用于创建静态链接库（就是把多个目标文件打包成一个）</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/navyhu/article/details/46788559">https://blog.csdn.net/navyhu/article/details/46788559</a></p>
<p><a href="https://blog.csdn.net/woshinia/article/details/11060797&gt;">https://blog.csdn.net/woshinia/article/details/11060797&gt;</a></p>
<p><a href="https://gcc.gnu.org/onlinedocs/">https://gcc.gnu.org/onlinedocs/</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>gcc</tag>
        <tag>make</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux shell脚本计算运行时间</title>
    <url>/2019-04-10-Linux-shell%E8%84%9A%E6%9C%AC%E8%AE%A1%E7%AE%97%E8%BF%90%E8%A1%8C%E6%97%B6%E9%97%B4.html</url>
    <content><![CDATA[<h1 id="Linux-shell脚本计算运行时间"><a href="#Linux-shell脚本计算运行时间" class="headerlink" title="Linux shell脚本计算运行时间"></a>Linux shell脚本计算运行时间</h1><p>这个功能经常用但是，总是现用现查，很麻烦。</p>
<a id="more"></a>
<p>代码</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># filename: msec_diff.sh</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">timediff</span></span>() &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment"># time format:date +&quot;%s.%N&quot;, such as 1502758855.907197692</span></span><br><span class="line">    start_time=<span class="variable">$1</span></span><br><span class="line">    end_time=<span class="variable">$2</span></span><br><span class="line">    </span><br><span class="line">    start_s=<span class="variable">$&#123;start_time%.*&#125;</span></span><br><span class="line">    start_nanos=<span class="variable">$&#123;start_time#*.&#125;</span></span><br><span class="line">    end_s=<span class="variable">$&#123;end_time%.*&#125;</span></span><br><span class="line">    end_nanos=<span class="variable">$&#123;end_time#*.&#125;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># end_nanos &gt; start_nanos? </span></span><br><span class="line">    <span class="comment"># Another way, the time part may start with 0, which means</span></span><br><span class="line">    <span class="comment"># it will be regarded as oct format, use &quot;10#&quot; to ensure</span></span><br><span class="line">    <span class="comment"># calculateing with decimal</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$end_nanos</span>&quot;</span> -lt <span class="string">&quot;<span class="variable">$start_nanos</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">        end_s=$(( <span class="number">10</span>#<span class="variable">$end_s</span> - <span class="number">1</span> ))</span><br><span class="line">        end_nanos=$(( <span class="number">10</span>#<span class="variable">$end_nanos</span> + <span class="number">10</span>**<span class="number">9</span> ))</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># get timediff</span></span><br><span class="line">    time=$(( <span class="number">10</span>#<span class="variable">$end_s</span> - <span class="number">10</span>#<span class="variable">$start_s</span> )).`<span class="built_in">printf</span> <span class="string">&quot;%03d\n&quot;</span> $(( (<span class="number">10</span>#<span class="variable">$end_nanos</span> - <span class="number">10</span>#<span class="variable">$start_nanos</span>)/<span class="number">10</span>**<span class="number">6</span> ))`</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$time</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#start=$(date +&quot;%s.%N&quot;)</span></span><br><span class="line"><span class="comment"># Now exec some command</span></span><br><span class="line"><span class="comment">#end=$(date +&quot;%s.%N&quot;)</span></span><br><span class="line"><span class="comment"># here give the values</span></span><br><span class="line">start=1502758855.907197692</span><br><span class="line">end=1502758865.066894173</span><br><span class="line"></span><br><span class="line">timediff <span class="variable">$start</span> <span class="variable">$end</span></span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/f-ck-need-u/p/7426987.html">https://www.cnblogs.com/f-ck-need-u/p/7426987.html</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis源码阅读——字典</title>
    <url>/2019-04-10-Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94%E5%AD%97%E5%85%B8.html</url>
    <content><![CDATA[<h2 id="Redis源码阅读——字典"><a href="#Redis源码阅读——字典" class="headerlink" title="Redis源码阅读——字典"></a>Redis源码阅读——字典</h2><p>字典是整个Redis实现的基础,之前把《Redis设计与实现》书上的理论知识看完了，开始阅读源码。</p>
<a id="more"></a>
<h2 id="Rehash"><a href="#Rehash" class="headerlink" title="Rehash"></a>Rehash</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://tech.meituan.com/2018/07/27/redis-rehash-practice-optimization.html">https://tech.meituan.com/2018/07/27/redis-rehash-practice-optimization.html</a></p>
<p><a href="https://luoming1224.github.io/2018/11/12/[redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0]redis%E6%B8%90%E8%BF%9B%E5%BC%8Frehash%E6%9C%BA%E5%88%B6/">https://luoming1224.github.io/2018/11/12/[redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0]redis%E6%B8%90%E8%BF%9B%E5%BC%8Frehash%E6%9C%BA%E5%88%B6/</a></p>
]]></content>
      <categories>
        <category>源码阅读</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>gdb 调试</title>
    <url>/2019-04-03-gdb-%E8%B0%83%E8%AF%95.html</url>
    <content><![CDATA[<h1 id="gdb调试"><a href="#gdb调试" class="headerlink" title="gdb调试"></a>gdb调试</h1><p>以前学习过，用得少，又忘记了，现在刚好为了调试redis 的dict 模块，所以再次记录。</p>
<a id="more"></a>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>摘自参考<a href="&lt;https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/gdb.html">&lt;https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/gdb.html</a>&gt;。 主要是补充实例</p>
<p>对C/C++程序的调试，需要在编译前就加上-g选项:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$g++ -g hello.cpp -o hello</span><br></pre></td></tr></table></figure>
<p>自己的Makefile里面修改成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dict-benchmark: dict.c sds.c siphash.c</span><br><span class="line">        $(CC) -g -o $@ $^</span><br></pre></td></tr></table></figure>
<p>但是好像没加之前也是可以直接就使用gdb  了？ 不知道为什么（可以使用但是出错不会显示所在的行，相当于没有调试）</p>
<p>调试可执行文件:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$gdb &lt;program&gt;</span><br></pre></td></tr></table></figure>
<p>program也就是你的执行文件，一般在当前目录下。</p>
<p>调试core文件(core是程序非法执行后core dump后产生的文件):</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$gdb &lt;program&gt; &lt;core dump file&gt;</span><br><span class="line">$gdb program core.11127</span><br></pre></td></tr></table></figure>
<p>调试服务程序:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$gdb &lt;program&gt; &lt;PID&gt;</span><br><span class="line">$gdb hello 11127</span><br></pre></td></tr></table></figure>
<p>如果你的程序是一个服务程序，那么你可以指定这个服务程序运行时的进程ID。gdb会自动attach上去，并调试他。program应该在PATH环境变量中搜索得到。</p>
<h2 id="gdb交互命令"><a href="#gdb交互命令" class="headerlink" title="gdb交互命令"></a>gdb交互命令</h2><p>启动gdb后，进入到交互模式，通过以下命令完成对程序的调试；注意高频使用的命令一般都会有缩写，熟练使用这些缩写命令能提高调试的效率；</p>
<h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><ul>
<li>run：简记为 r ，其作用是运行程序，当遇到断点后，程序会在断点处停止运行，等待用户输入下一步的命令。</li>
<li>continue （简写c ）：继续执行，到下一个断点处（或运行结束）</li>
<li>next：（简写 n），单步跟踪程序，当遇到函数调用时，也不进入此函数体；此命令同 step 的主要区别是，step 遇到用户自定义的函数，将步进到函数中去运行，而 next 则直接调用函数，不会进入到函数体内。</li>
<li>step （简写s）：单步调试如果有函数调用，则进入函数；与命令n不同，n是不进入调用的函数的</li>
<li>until：当你厌倦了在一个循环体内单步跟踪时，这个命令可以运行程序直到退出循环体。</li>
<li>until+行号： 运行至某行，不仅仅用来跳出循环</li>
<li>finish： 运行程序，直到当前函数完成返回，并打印函数返回时的堆栈地址和返回值及参数值等信息。</li>
<li>call 函数(参数)：调用程序中可见的函数，并传递“参数”，如：call gdb_test(55)</li>
<li>quit：简记为 q ，退出gdb</li>
</ul>
<h3 id="设置断点"><a href="#设置断点" class="headerlink" title="设置断点"></a>设置断点</h3><ul>
<li><p>break n （简写b n）:在第n行处设置断点</p>
<p>（可以带上代码路径和代码名称： b OAGUPDATE.cpp:578）</p>
</li>
<li><p>b fn1 if a＞b：条件断点设置</p>
</li>
<li><p>break func（break缩写为b）：在函数func()的入口处设置断点，如：break cb_button</p>
</li>
<li><p>delete 断点号n：删除第n个断点</p>
</li>
<li><p>disable 断点号n：暂停第n个断点</p>
</li>
<li><p>enable 断点号n：开启第n个断点</p>
</li>
<li><p>clear 行号n：清除第n行的断点</p>
</li>
<li><p>info b （info breakpoints） ：显示当前程序的断点设置情况</p>
</li>
<li><p>delete breakpoints：清除所有断点：</p>
</li>
</ul>
<h3 id="查看源代码"><a href="#查看源代码" class="headerlink" title="查看源代码"></a>查看源代码</h3><ul>
<li>list ：简记为 l ，其作用就是列出程序的源代码，默认每次显示10行。</li>
<li>list 行号：将显示当前文件以“行号”为中心的前后10行代码，如：list 12</li>
<li>list 函数名：将显示“函数名”所在函数的源代码，如：list main</li>
<li>list ：不带参数，将接着上一次 list 命令的，输出下边的内容。</li>
</ul>
<h3 id="打印表达式"><a href="#打印表达式" class="headerlink" title="打印表达式"></a>打印表达式</h3><ul>
<li>print 表达式：简记为 p ，其中“表达式”可以是任何当前正在被测试程序的有效表达式，比如当前正在调试C语言的程序，那么“表达式”可以是任何C语言的有效表达式，包括数字，变量甚至是函数调用。</li>
<li>print a：将显示整数 a 的值</li>
<li>print ++a：将把 a 中的值加1,并显示出来</li>
<li>print name：将显示字符串 name 的值</li>
<li>print gdb_test(22)：将以整数22作为参数调用 gdb_test() 函数</li>
<li>print gdb_test(a)：将以变量 a 作为参数调用 gdb_test() 函数</li>
<li>display 表达式：在单步运行时将非常有用，使用display命令设置一个表达式后，它将在每次单步进行指令后，紧接着输出被设置的表达式及值。如： display a</li>
<li>watch 表达式：设置一个监视点，一旦被监视的“表达式”的值改变，gdb将强行终止正在被调试的程序。如： watch a</li>
<li>whatis ：查询变量或函数</li>
<li>info function： 查询函数</li>
<li>扩展info locals： 显示当前堆栈页的所有变量</li>
</ul>
<h3 id="查询运行信息"><a href="#查询运行信息" class="headerlink" title="查询运行信息"></a>查询运行信息</h3><ul>
<li>where/bt ：当前运行的堆栈列表；</li>
<li>bt backtrace 显示当前调用堆栈</li>
<li>up/down 改变堆栈显示的深度</li>
<li>set args 参数:指定运行时的参数</li>
<li>show args：查看设置好的参数</li>
<li>info program： 来查看程序的是否在运行，进程号，被暂停的原因。</li>
</ul>
<h3 id="分割窗口"><a href="#分割窗口" class="headerlink" title="分割窗口"></a>分割窗口</h3><ul>
<li>layout：用于分割窗口，可以一边查看代码，一边测试：</li>
<li>layout src：显示源代码窗口</li>
<li>layout asm：显示反汇编窗口</li>
<li>layout regs：显示源代码/反汇编和CPU寄存器窗口</li>
<li>layout split：显示源代码和反汇编窗口</li>
<li>Ctrl + L：刷新窗口</li>
</ul>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>调试程序dict-benchmark</p>
<p>开始调试：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@hw103:&#x2F;home&#x2F;yky&#x2F;test&#x2F;redis_dict&#x2F;dict-benchmark# gdb dict-benchmark </span><br><span class="line">GNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1</span><br><span class="line">Copyright (C) 2016 Free Software Foundation, Inc.</span><br><span class="line">License GPLv3+: GNU GPL version 3 or later &lt;http:&#x2F;&#x2F;gnu.org&#x2F;licenses&#x2F;gpl.html&gt;</span><br><span class="line">This is free software: you are free to change and redistribute it.</span><br><span class="line">There is NO WARRANTY, to the extent permitted by law.  Type &quot;show copying&quot;</span><br><span class="line">and &quot;show warranty&quot; for details.</span><br><span class="line">This GDB was configured as &quot;x86_64-linux-gnu&quot;.</span><br><span class="line">Type &quot;show configuration&quot; for configuration details.</span><br><span class="line">For bug reporting instructions, please see:</span><br><span class="line">&lt;http:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;gdb&#x2F;bugs&#x2F;&gt;.</span><br><span class="line">Find the GDB manual and other documentation resources online at:</span><br><span class="line">&lt;http:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;gdb&#x2F;documentation&#x2F;&gt;.</span><br><span class="line">For help, type &quot;help&quot;.</span><br><span class="line">Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;...</span><br><span class="line">Reading symbols from dict-benchmark...done.</span><br><span class="line">(gdb) </span><br></pre></td></tr></table></figure>
<p>使用 r 运行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) r</span><br><span class="line">Starting program: &#x2F;home&#x2F;yky&#x2F;test&#x2F;redis_dict&#x2F;dict-benchmark&#x2F;dict-benchmark </span><br><span class="line">Add elements to dict</span><br><span class="line">Add ret0 is :0, ht[0].used:1, ht[0].size:4,ht[1].used:0, ht[1].size:0</span><br><span class="line">Add ret1 is :0, ht[0].used:2, ht[0].size:4,ht[1].used:0, ht[1].size:0</span><br><span class="line">Add ret2 is :0, ht[0].used:3, ht[0].size:4,ht[1].used:0, ht[1].size:0</span><br><span class="line"></span><br><span class="line">Program received signal SIGSEGV, Segmentation fault.</span><br><span class="line">0x000000000040288b in _dictKeyIndex (d&#x3D;0x60a050, key&#x3D;0x60a011, hash&#x3D;3050426978, existing&#x3D;0x0) at dict.c:975</span><br><span class="line">975                 if (key&#x3D;&#x3D;he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) &#123;</span><br><span class="line">(gdb) </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在_dictKeyIndex ()函数也就是计算索引的时候出错。</p>
<p>l 列出代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) l</span><br><span class="line">970         for (table &#x3D; 0; table &lt;&#x3D; 1; table++) &#123;</span><br><span class="line">971             idx &#x3D; hash &amp; d-&gt;ht[table].sizemask;</span><br><span class="line">972             &#x2F;* Search if this slot does not already contain the given key *&#x2F;</span><br><span class="line">973             he &#x3D; d-&gt;ht[table].table[idx];</span><br><span class="line">974             while(he) &#123;</span><br><span class="line">975                 if (key&#x3D;&#x3D;he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) &#123;</span><br><span class="line">976                     if (existing) *existing &#x3D; he;</span><br><span class="line">977                     return -1;</span><br><span class="line">978                 &#125;</span><br><span class="line">979                 he &#x3D; he-&gt;next;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在975行加入断点  b n</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) b 975</span><br><span class="line">Breakpoint 2 at 0x402887: file dict.c, line 975.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出对应变量的值：print</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(gdb) print key </span><br><span class="line">$1 &#x3D; (const void *) 0x60a011</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/gdb.html">https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/gdb.html</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>gdb</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis源码阅读——dict</title>
    <url>/2019-04-03-Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94dict.html</url>
    <content><![CDATA[<h1 id="Redis源码阅读——dict"><a href="#Redis源码阅读——dict" class="headerlink" title="Redis源码阅读——dict"></a>Redis源码阅读——dict</h1><p>继续Redis 的源码阅读，进入dict这一章节。知识点讲解，见redis设计与实现的读书笔记dict这一章。</p>
<a id="more"></a>
<h2 id="dict的创建"><a href="#dict的创建" class="headerlink" title="dict的创建"></a>dict的创建</h2><p>还是和sds一样单独将dict模块给提取出来，参考博客是直接将server的main函数给修改了的。再阅读Makefile的时候发现了dict-benchmark 这个选项，<code>make dict-benchmark</code> 这个命令，可以编译出一个可执行文件dict-benchmark。 </p>
<p>所以想着应该可以单独再把dict-benchmark给提取出来。</p>
<p>需要拷贝的文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dict.c ditc.h fmacros.h redisassert.h sdsaclloc.h sds.c sds.h siphash.c </span><br></pre></td></tr></table></figure>
<p>需要做的修改</p>
<p>zmalloc zfree zcalloc 需要修改成使用malloc ，free, calloc 其中zcalloc在修改成calloc的时候需要在调用的时候多传入一个参数1,作为第一个参数，因为zcalloc 和calloc 的接口不一样。</p>
<p>还需要在redisassert.h 中对_serverAssert函数做出定义： 参考debug.c 里面的定义。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">void _serverAssert(char *estr, char *file, int line)&#123;</span><br><span class="line">    printf(&quot;%s:%d&#39;%s&#39; is not true&quot;,file,line,estr);</span><br><span class="line">    *((char*)-1) &#x3D; &#39;x&#39;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>修改成使用libc 的内存分配器后会造成性能下降，zmalloc 要好于malloc 所以要想真实还原的话，看下怎么把zmalloc给移植过来。zmalloc用的是jemalloc.</p>
<p>编写简单的Makefile为：</p>
<figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line"><span class="section">dict-benchmark: dict.c sds.c siphash.c</span></span><br><span class="line">        <span class="variable">$(CC)</span> -g -o <span class="variable">$@</span> <span class="variable">$^</span></span><br><span class="line">.PHONY :clean</span><br><span class="line">clean :</span><br><span class="line">        rm dict-benchmark </span><br></pre></td></tr></table></figure>
<h2 id="计算索引"><a href="#计算索引" class="headerlink" title="计算索引"></a>计算索引</h2><p>_dictKeyIndex 函数。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* Returns the index of a free slot that can be populated with</span></span><br><span class="line"><span class="comment"> * a hash entry for the given &#x27;key&#x27;.</span></span><br><span class="line"><span class="comment"> * If the key already exists, -1 is returned</span></span><br><span class="line"><span class="comment"> * and the optional output parameter may be filled.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Note that if we are in the process of rehashing the hash table, the</span></span><br><span class="line"><span class="comment"> * index is always returned in the context of the second (new) hash table. */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">int</span> _dictKeyIndex(dict *d, <span class="keyword">const</span> <span class="keyword">void</span> *key, <span class="keyword">unsigned</span> <span class="keyword">int</span> hash, dictEntry **existing)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> idx, table;</span><br><span class="line">    dictEntry *he;</span><br><span class="line">    <span class="keyword">if</span> (existing) *existing = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Expand the hash table if needed */</span></span><br><span class="line">    <span class="keyword">if</span> (_dictExpandIfNeeded(d) == DICT_ERR)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span> (table = <span class="number">0</span>; table &lt;= <span class="number">1</span>; table++) &#123;</span><br><span class="line">        idx = hash &amp; d-&gt;ht[table].sizemask;</span><br><span class="line">        <span class="comment">/* Search if this slot does not already contain the given key */</span></span><br><span class="line">        he = d-&gt;ht[table].table[idx];</span><br><span class="line">        <span class="keyword">while</span>(he) &#123;</span><br><span class="line">            <span class="keyword">if</span> (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) &#123;</span><br><span class="line">                <span class="keyword">if</span> (existing) *existing = he;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            he = he-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (!dictIsRehashing(d)) <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> idx;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/yangbodong22011/article/details/78467583">https://blog.csdn.net/yangbodong22011/article/details/78467583</a></p>
]]></content>
      <categories>
        <category>源码阅读</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>dict</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker 学习</title>
    <url>/2019-04-01-Docker-%E5%AD%A6%E4%B9%A0.html</url>
    <content><![CDATA[<h1 id="Docker-学习"><a href="#Docker-学习" class="headerlink" title="Docker 学习"></a>Docker 学习</h1><p>之前一直听说容器可以免去配置环境的麻烦，但是一直没有机会接触，现在刚好有机会可以使用。</p>
<a id="more"></a>
<h2 id="三大基础"><a href="#三大基础" class="headerlink" title="三大基础"></a>三大基础</h2><p>镜像、容器、仓库</p>
<p>类比的话：镜像是类（iso），容器是实例（操作系统），仓库类似于git的仓库。</p>
<p>镜像的构建是一层层的继承而来的，镜像和容器都是文件，容器退出的时候容器文件依然存在。除非手动使用-rm指定，才会删除。</p>
<p>从仓库下载镜像，加载镜像后进入容器，容器保存后构成镜像，镜像推送至仓库。</p>
<h2 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h2><p><strong>加载镜像</strong>，进入一个新的容器：<code>docker run --name mydocker image:tag command</code></p>
<p>实例 摘自：<a href="https://yeasy.gitbooks.io/docker_practice/image/pull.html">https://yeasy.gitbooks.io/docker_practice/image/pull.html</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -it --rm \</span><br><span class="line">    ubuntu:18.04 \</span><br><span class="line">    bash</span><br></pre></td></tr></table></figure>
<p><code>docker run</code> 就是运行容器的命令 从镜像中新建一个容器</p>
<ul>
<li><code>-it</code>：这是两个参数，一个是 <code>-i</code>：交互式操作，一个是 <code>-t</code> 终端。我们这里打算进入 <code>bash</code> 执行一些命令并查看返回结果，因此我们需要交互式终端。</li>
<li><code>--rm</code>：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 <code>docker rm</code>。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 <code>--rm</code> 可以避免浪费空间。</li>
<li><code>ubuntu:18.04</code>：这是指用 <code>ubuntu:18.04</code> 镜像为基础来启动容器。</li>
<li><code>bash</code>：放在镜像名后的是<strong>命令</strong>，这里我们希望有个交互式 Shell，因此用的是 <code>bash</code>。</li>
<li><code>--name</code> 后面跟创建的容器的名字。</li>
</ul>
<p><strong>退出:</strong> 在容器中<code>exit</code></p>
<p><strong>进入容器：</strong></p>
<p><code>docker exec</code>  进入一个已有的容器。 （容器需要已经运行）</p>
<p><strong>容器的复制</strong><code>dcoker cp</code>  </p>
<p><code>docker ps</code> <strong>列出启动容器</strong> <code>docker ps -a</code> 列出所有容器包括关闭的。</p>
<p><strong>启动之前关闭的容器</strong>： <code>docker start container-name</code> ， </p>
<p>需要先启动容器，才能执行<code>docker exec -it container-name bash</code></p>
<p><strong>列出所有镜像</strong> <code>docker images</code></p>
<p><strong>删除镜像</strong> <code>docker rmi img-name</code> </p>
<p><strong>数据挂载</strong> <code>docker run --mount type=bind,source=/src/webapp,target=/opt/webapp</code></p>
<p><strong>容器保存为镜像</strong> <code>docker commit hash_id img_name</code></p>
<p><strong>实例</strong></p>
<p><code>sudo docker run -it --rm --gpus all  -v /home/test/:/home/test aibench/pytorch:v1 bash</code></p>
<p>需要注意的是所有参数都应放在启动的镜像之前，不然报错。</p>
<h2 id="端口映射"><a href="#端口映射" class="headerlink" title="端口映射"></a>端口映射</h2><p><a href="https://blog.csdn.net/wanglei_storage/article/details/48471753">https://blog.csdn.net/wanglei_storage/article/details/48471753</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://juejin.im/entry/5b19e350e51d45069f5e1d66">https://juejin.im/entry/5b19e350e51d45069f5e1d66</a></p>
<p><a href="https://yeasy.gitbooks.io/docker_practice/basic_concept/container.html">https://yeasy.gitbooks.io/docker_practice/basic_concept/container.html</a></p>
<p><a href="https://blog.csdn.net/u010246789/article/details/53958662">https://blog.csdn.net/u010246789/article/details/53958662</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title>计算Linux系统的CPU利用率</title>
    <url>/2019-03-31-%E8%AE%A1%E7%AE%97Linux%E7%B3%BB%E7%BB%9F%E7%9A%84CPU%E5%88%A9%E7%94%A8%E7%8E%87.html</url>
    <content><![CDATA[<h1 id="计算Linux系统的CPU利用率"><a href="#计算Linux系统的CPU利用率" class="headerlink" title="计算Linux系统的CPU利用率"></a>计算Linux系统的CPU利用率</h1><p>通过读取系统的/proc/stat 信息来计算CPU的利用率</p>
<a id="more"></a>
<h2 id="cpu-信息的读取"><a href="#cpu-信息的读取" class="headerlink" title="cpu 信息的读取"></a>cpu 信息的读取</h2><p>摘自参考博客：<a href="https://blog.csdn.net/x_i_y_u_e/article/details/50684508">https://blog.csdn.net/x_i_y_u_e/article/details/50684508</a></p>
<p>在Linux/Unix下，CPU利用率分为用户态，系统态和空闲态，分别表示CPU处于用户态执行的时间，系统内核执行的时间，和空闲系统进程执行的时间。平时所说的CPU利用率是指：CPU执行非系统空闲进程的时间 / CPU总的执行时间。</p>
<p>在Linux的内核中，有一个全局变量：Jiffies。 Jiffies代表时间。它的单位随硬件平台的不同而不同。系统里定义了一个常数HZ，代表每秒种最小时间间隔的数目。这样jiffies的单位就是1/HZ。Intel平台jiffies的单位是1/100秒，这就是系统所能分辨的最小时间间隔了。每个CPU时间片，Jiffies都要加1。 CPU的利用率就是用执行用户态+系统态的Jiffies除以总的Jifffies来表示。</p>
<p>在Linux系统中，可以用/proc/stat文件来计算cpu的利用率(详细的解释可参考：<a href="http://www.linuxhowtos.org/System/procstat.htm">http://www.linuxhowtos.org/System/procstat.htm</a>)。这个文件包含了所有CPU活动的信息，该文件中的所有值都是从系统启动开始累计到当前时刻。</p>
<p>在本机上的信息如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> root@hw103:&#x2F;home&#x2F;yky# cat&#x2F;proc&#x2F;stat </span><br><span class="line">cpu  7283854 35392 293546 204419077 75835 0 8385 0 0 0</span><br><span class="line">cpu0 365010 48 18204 8453603 767 0 4306 0 0 0</span><br><span class="line">cpu1 292817 9 14990 8526563 1358 0 2114 0 0 0</span><br><span class="line">cpu2 286689 0 11880 8538662 1033 0 752 0 0 0</span><br><span class="line">cpu3 287106 14624 12945 8523209 641 0 320 0 0 0</span><br><span class="line">cpu4 293293 0 11784 8532282 604 0 147 0 0 0</span><br><span class="line">cpu5 371312 2824 13669 8408187 40794 0 406 0 0 0</span><br><span class="line">cpu6 358116 10934 14014 8453775 1230 0 68 0 0 0</span><br><span class="line">cpu7 313971 6281 12243 8504303 1575 0 28 0 0 0</span><br><span class="line">cpu8 318084 0 11598 8506770 2036 0 15 0 0 0</span><br><span class="line">cpu9 294503 0 11137 8530318 2185 0 10 0 0 0</span><br><span class="line">cpu10 307922 144 12434 8516570 1177 0 15 0 0 0</span><br><span class="line">cpu11 291752 0 11502 8533957 1128 0 4 0 0 0</span><br><span class="line">cpu12 315096 0 15927 8503001 3528 0 3 0 0 0</span><br><span class="line">cpu13 375976 0 17927 8442873 2041 0 1 0 0 0</span><br><span class="line">cpu14 299344 0 10140 8523716 2818 0 1 0 0 0</span><br><span class="line">cpu15 288470 3 10146 8538685 1240 0 0 0 0 0</span><br><span class="line">cpu16 301148 0 10681 8523612 2185 0 0 0 0 0</span><br><span class="line">cpu17 263183 4 9149 8565345 771 0 0 0 0 0</span><br><span class="line">cpu18 262518 370 10343 8562955 2105 0 11 0 0 0</span><br><span class="line">cpu19 280230 3 10399 8546414 1227 0 6 0 0 0</span><br><span class="line">cpu20 278962 0 10346 8547585 1221 0 9 0 0 0</span><br><span class="line">cpu21 277042 143 11502 8547940 1048 0 2 0 0 0</span><br><span class="line">cpu22 275560 0 9458 8549093 1271 0 153 0 0 0</span><br><span class="line">cpu23 285740 0 11118 8539648 1838 0 6 0 0 0</span><br><span class="line">intr 91288599 43 2 0 0 0 0 0 0 1 0 0 0 4 0 0 0 41 0 2 0 0 0 0 0 0 0 334158 0 1 2473208 45518 96917 44876 138077 45263 45258 54441 0 0 44198 44198 44198 44198 44198 44198 441</span><br><span class="line">98 44198 0 0 44198 44198 44198 44198 44198 44198 44198 44198 0 0 44199 44199 44199 44199 44199 44199 44199 44199 0 0 44199 44199 44199 44199 44199 44199 44199 44199 0 0 44198 44198 44198 44198 44198 44198 44198 44198 0 0 44198 44198 44198 44198 44198 44198 44198 44198 0 0 44199 44199 44199 44199 44199 44199 44199 44199 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0ctxt 42731429</span><br><span class="line">btime 1553933982</span><br><span class="line">processes 190449</span><br><span class="line">procs_running 5</span><br><span class="line">procs_blocked 0</span><br><span class="line">softirq 57291816 8 26401645 17536 5419485 334063 0 1044 10019160 0 15098875</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>第一行cpu是总的cpu信息，其他的cpu0-cpu23 是24个核的信息。</p>
<p>计算cpu利用率只用到前7个参数， 对应的参数解释为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">	 user	 nice  system idle      iowait   irq  softirq</span><br><span class="line">cpu  7283854 35392 293546 204419077 75835    0    8385     </span><br></pre></td></tr></table></figure>
<p>user (7283854) 从系统启动开始累计到当前时刻，用户态的CPU时间（单位：jiffies） ，不包含 nice值为负进程。1jiffies=0.01秒<br>nice (35392) 从系统启动开始累计到当前时刻，nice值为负的进程所占用的CPU时间（单位：jiffies）<br>system (293546) 从系统启动开始累计到当前时刻，核心时间（单位：jiffies）<br>idle (204419077) 从系统启动开始累计到当前时刻，除硬盘IO等待时间以外其它等待时间（单位：jiffies）<br>iowait (75835) 从系统启动开始累计到当前时刻，硬盘IO等待时间（单位：jiffies） ，<br>irq (0) 从系统启动开始累计到当前时刻，硬中断时间（单位：jiffies）<br>softirq (8385) 从系统启动开始累计到当前时刻，软中断时间（单位：jiffies）</p>
<p>CPU时间=user+system+nice+idle+iowait+irq+softirq</p>
<p>“intr”这行给出中断的信息，第一个为自系统启动以来，发生的所有的中断的次数；然后每个数对应一个特定的中断自系统启动以来所发生的次数。<br>“ctxt”给出了自系统启动以来CPU发生的上下文交换的次数。<br>“btime”给出了从系统启动到现在为止的时间，单位为秒。<br>“processes (total_forks) 自系统启动以来所创建的任务的个数目。<br>“procs_running”：当前运行队列的任务的数目。<br>“procs_blocked”：当前被阻塞的任务的数目。</p>
<p>计算cpu利用率的方法就是计算出在一段时间里面，cpu工作的时间/总得时间</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cpu usage&#x3D;(idle2-idle1)&#x2F;(cpu2-cpu1)*100</span><br><span class="line">cpu usage&#x3D;[(user_2 +sys_2+nice_2) - (user_1 + sys_1+nice_1)]&#x2F;(total_2 - total_1)*100</span><br></pre></td></tr></table></figure>
<p>第二中方法只把user+sys+nice 这三个时间看作cpu的工作时间，因为其他的几个比较小。</p>
<h2 id="shell脚本"><a href="#shell脚本" class="headerlink" title="shell脚本"></a>shell脚本</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#echo user nice system idle iowait irq softirq</span></span></span><br><span class="line"></span><br><span class="line">cpulog_start=$(cat /proc/stat | grep &#x27;cpu&#x27;| awk &#x27;NR==1&#123;print $2 &quot;\t&quot; $3 &quot;\t&quot; $4 &quot;\t&quot; $5 &quot;\t&quot; $6 &quot;\t&quot; $7 &quot;\t&quot; $8&#125;&#x27; )</span><br><span class="line">cpu_use_start=$(echo $cpulog_start | awk &#x27;&#123;print $1+$2+$3&#125;&#x27;)</span><br><span class="line">cpu_iowait_start=$(echo $cpulog_start | awk &#x27;&#123;print $5&#125;&#x27;)</span><br><span class="line">cpu_total_start=$(echo $cpulog_start | awk &#x27;&#123;print $1+$2+$3+$4+$5+$6+$7&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">sleep 10</span><br><span class="line"></span><br><span class="line">cpulog_end=$(cat /proc/stat | grep &#x27;cpu&#x27;| awk &#x27;NR==1&#123;print $2 &quot;\t&quot; $3 &quot;\t&quot; $4 &quot;\t&quot; $5 &quot;\t&quot; $6 &quot;\t&quot; $7 &quot;\t&quot; $8&#125;&#x27; )</span><br><span class="line">cpu_use_end=$(echo $cpulog_end | awk &#x27;&#123;print $1+$2+$3&#125;&#x27;)</span><br><span class="line">cpu_iowait_end=$(echo $cpulog_end | awk &#x27;&#123;print $5&#125;&#x27;)</span><br><span class="line">cpu_total_end=$(echo $cpulog_end | awk &#x27;&#123;print $1+$2+$3+$4+$5+$6+$7&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">cpu_use_diff=`expr $cpu_use_end - $cpu_use_start`</span><br><span class="line">cpu_iowait_diff=`expr $cpu_iowait_end - $cpu_iowait_start`</span><br><span class="line">cpu_total_diff=`expr $cpu_total_end - $cpu_total_start`</span><br><span class="line"></span><br><span class="line">cpu_use_rate=`expr $cpu_use_diff/$cpu_total_diff*100 | bc -l`</span><br><span class="line">cpu_iowait_rate=`expr $cpu_iowait_diff/$cpu_total_diff*100 | bc -l`</span><br><span class="line"></span><br><span class="line">echo &quot;---------------cpuinfo----------------------&quot;</span><br><span class="line">echo &quot;cpu_usage_rate (%) : $cpu_use_rate&quot;</span><br><span class="line">echo &quot;cpu_iowait_rate (%): $cpu_iowait_rate&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>主要的知识点讲解：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cpulog_start=$(cat /proc/stat | grep &#x27;cpu&#x27;| awk &#x27;NR==1&#123;print $2 &quot;\t&quot; $3 &quot;\t&quot; $4 &quot;\t&quot; $5 &quot;\t&quot; $6 &quot;\t&quot; $7 &quot;\t&quot; $8&#125;&#x27; )</span><br></pre></td></tr></table></figure>
<p>这一句先将/proc/stat文件的信息读取出来然后用管道| 传递给grep 命令 ,grep 将包含cpu的信息给提取出来</p>
<p><code>cat /proc/stat | grep &#39;cpu&#39;</code> 输出为：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@hw103:/home/yky<span class="comment"># cat /proc/stat | grep &#x27;cpu&#x27;</span></span><br><span class="line">cpu  7645369 35392 318219 206250476 80895 0 9870 0 0 0</span><br><span class="line">cpu0 365256 48 18792 8544685 767 0 5678 0 0 0</span><br><span class="line">cpu1 292983 9 15427 8618292 1389 0 2206 0 0 0</span><br><span class="line">cpu2 287020 0 12884 8629895 1033 0 760 0 0 0</span><br><span class="line">cpu3 288372 14624 13910 8613386 681 0 320 0 0 0</span><br><span class="line">cpu4 293707 0 13074 8623117 646 0 147 0 0 0</span><br><span class="line">cpu5 390012 2824 15130 8477387 44027 0 418 0 0 0</span><br><span class="line">cpu6 362140 10934 15217 8541127 1233 0 69 0 0 0</span><br><span class="line">cpu7 377620 6281 12915 8532734 1700 0 28 0 0 0</span><br><span class="line">cpu8 333520 0 12524 8582987 2100 0 15 0 0 0</span><br><span class="line">cpu9 348913 0 12619 8567005 2453 0 10 0 0 0</span><br><span class="line">cpu10 333584 144 14002 8581939 1297 0 15 0 0 0</span><br><span class="line">cpu11 292980 0 12966 8623861 1137 0 4 0 0 0</span><br><span class="line">cpu12 350188 0 16793 8559301 4017 0 3 0 0 0</span><br><span class="line">cpu13 430200 0 19278 8479849 2344 0 1 0 0 0</span><br><span class="line">cpu14 312421 0 11483 8601843 2906 0 1 0 0 0</span><br><span class="line">cpu15 343656 3 10768 8575706 1268 0 0 0 0 0</span><br><span class="line">cpu16 302673 0 12190 8613170 2190 0 0 0 0 0</span><br><span class="line">cpu17 263473 4 9906 8656857 771 0 0 0 0 0</span><br><span class="line">cpu18 263094 370 11278 8653940 2165 0 11 0 0 0</span><br><span class="line">cpu19 281216 3 11166 8637253 1280 0 6 0 0 0</span><br><span class="line">cpu20 280412 0 11645 8637433 1221 0 9 0 0 0</span><br><span class="line">cpu21 277219 143 11967 8639929 1048 0 2 0 0 0</span><br><span class="line">cpu22 275894 0 10291 8640508 1271 0 153 0 0 0</span><br><span class="line">cpu23 298806 0 11985 8618261 1939 0 6 0 0 0</span><br></pre></td></tr></table></figure>
<h3 id="awk-命令"><a href="#awk-命令" class="headerlink" title="awk 命令"></a>awk 命令</h3><p>之后使用awk命令再次进行操作。这个命令之前用的很少，参考《鸟哥的Linux私房菜》介绍：</p>
<p>sed常用于一整行的处理，awk则倾向于将一行分成数个“字段”来处理，awk适合处理小型的数据。</p>
<p>用法为：</p>
<p><code>awk &#39;条件类型1&#123;操作1&#125; 条件类型1&#123;操作2&#125;...&#39;  filename</code></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量名称</th>
<th>意义</th>
</tr>
</thead>
<tbody>
<tr>
<td>NF</td>
<td>每一行（$0） 拥有的字段总数</td>
</tr>
<tr>
<td>NR</td>
<td>awk当前处理的第几行数据</td>
</tr>
<tr>
<td>FS</td>
<td>目前的分割符，默认空格键</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cpulog_start=$(cat /proc/stat | grep &#x27;cpu&#x27;| awk &#x27;NR==1&#123;print $2 &quot;\t&quot; $3 &quot;\t&quot; $4 &quot;\t&quot; $5 &quot;\t&quot; $6 &quot;\t&quot; $7 &quot;\t&quot; $8&#125;&#x27; )</span><br></pre></td></tr></table></figure>
<p><code>NR==1</code> 限定条件为第一行，因为第一行的数据才是cpu的总信息，{}里面的操作是输出字段，$N 就是第N个字段。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> $1	$2		$3		$4	  $5		$6		 $7	  $8</span><br><span class="line">     user	 nice  system idle      iowait   irq  softirq</span><br><span class="line">cpu  7283854 35392 293546 204419077 75835    0    8385    </span><br></pre></td></tr></table></figure>
<h3 id="expr"><a href="#expr" class="headerlink" title="expr"></a>expr</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cpu_use_diff=`expr <span class="variable">$cpu_use_end</span> - <span class="variable">$cpu_use_start</span>`</span><br></pre></td></tr></table></figure>
<p>使用expr 执行变量计算，然后是`` 进行反引用将值赋值给cpu_use_diff</p>
<p>注意shell 脚本中= 不能用空格分开 需要直接相邻</p>
<h3 id="bc-命令"><a href="#bc-命令" class="headerlink" title="bc 命令"></a>bc 命令</h3><p>bc 命令是任意精度计算器语言，通常在linux下当计算器用。</p>
<p>简单的描述参考：<a href="http://www.runoob.com/linux/linux-comm-bc.html">http://www.runoob.com/linux/linux-comm-bc.html</a></p>
<p>而expr命令不支持小数运算，所以需要使用bc进行计算。</p>
<p>语法为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bc(选项)(参数)</span><br></pre></td></tr></table></figure>
<p><strong>选项值</strong></p>
<ul>
<li>-i：强制进入交互式模式；</li>
<li>-l：定义使用的标准数学库</li>
<li>； -w：对POSIX bc的扩展给出警告信息；</li>
<li>-q：不打印正常的GNU bc环境信息；</li>
<li>-v：显示指令版本信息；</li>
<li>-h：显示指令的帮助信息。</li>
</ul>
<p><strong>参数</strong></p>
<p>文件：指定包含计算任务的文件。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/x_i_y_u_e/article/details/50684508">https://blog.csdn.net/x_i_y_u_e/article/details/50684508</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis设计与实现读书笔记——第九章 数据库</title>
    <url>/2019-03-29-Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%AC%AC%E4%B9%9D%E7%AB%A0-%E6%95%B0%E6%8D%AE%E5%BA%93.html</url>
    <content><![CDATA[<h1 id="Redis设计与实现读书笔记——第九章-数据库"><a href="#Redis设计与实现读书笔记——第九章-数据库" class="headerlink" title="Redis设计与实现读书笔记——第九章 数据库"></a>Redis设计与实现读书笔记——第九章 数据库</h1><p>终于看到了服务器部分了，主要是想搞懂数据库的整个流程与架构。</p>
<h2 id="9-1-服务器中的数据库"><a href="#9-1-服务器中的数据库" class="headerlink" title="9.1 服务器中的数据库"></a>9.1 服务器中的数据库</h2><p>将所有数据库都保存在redis.h/redisServer结构的db数组中（哇，不可思议用数组来保存的），db数组每个项都是一个redis.h/redisDb结构指针，而一个redisDb代表了一个数据库。</p>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis设计与实现读书笔记——第8章 对象</title>
    <url>/2019-03-29-Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%AC%AC8%E7%AB%A0-%E5%AF%B9%E8%B1%A1.html</url>
    <content><![CDATA[<h1 id="Redis设计与实现读书笔记——第8章-对象"><a href="#Redis设计与实现读书笔记——第8章-对象" class="headerlink" title="Redis设计与实现读书笔记——第8章 对象"></a>Redis设计与实现读书笔记——第8章 对象</h1><p>Redis 并没有直接使用sds、dict等数据结构来实现键值对数据库， 而是基于这些数据结构创建了一个对象系统， 这个系统包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象， 每种对象都用到了至少一种前面所介绍的数据结构。</p>
<a id="more"></a>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>对象的好处：</p>
<ul>
<li>根据对象的类型来判断是否可以执行给定的命令。</li>
<li>针对不同的使用场景， 为对象设置多种不同的数据结构实现， 从而优化对象在不同场景下的使用效率。</li>
</ul>
<p>对象的特性：</p>
<ul>
<li>基于引用计数技术的内存回收机制（和java的是否原理相似）</li>
<li>通过引用计数技术实现对象共享机制，多个键共享同一个对象来节约内存</li>
<li>带有访问时间记录信息，可以用来删除空转时长较大的键</li>
</ul>
<h2 id="8-1-对象的类型与编码"><a href="#8-1-对象的类型与编码" class="headerlink" title="8.1 对象的类型与编码"></a>8.1 对象的类型与编码</h2><p>Redis 使用对象来表示数据库中的键和值，创建一个新的键值对的时候，至少包含两个对象：1. 键对象 2. 值对象</p>
<p>对象都由一个 <code>redisObject</code> 结构表示，保存数据相关的属性</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">redisObject</span> &#123;</span></span><br><span class="line">    <span class="comment">// 对象类型</span></span><br><span class="line">    <span class="keyword">unsigned</span> type:<span class="number">4</span>;</span><br><span class="line">    <span class="comment">// 编码</span></span><br><span class="line">    <span class="keyword">unsigned</span> encoding:<span class="number">4</span>;</span><br><span class="line">    <span class="comment">//lru 时钟 记录最后被访问的时间，也就是空转时长</span></span><br><span class="line">    <span class="keyword">unsigned</span> lru:LRU_BITS; <span class="comment">/* LRU time (relative to global lru_clock) or</span></span><br><span class="line"><span class="comment">                            * LFU data (least significant 8 bits frequency</span></span><br><span class="line"><span class="comment">                            * and most significant 16 bits decreas time). */</span></span><br><span class="line">    <span class="comment">// 引用计数 实现内存自动回收等。</span></span><br><span class="line">    <span class="keyword">int</span> refcount;</span><br><span class="line">    <span class="comment">// 指向底层实现数据结构的指针</span></span><br><span class="line">    <span class="keyword">void</span> *ptr;</span><br><span class="line">&#125; robj;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>ptr指向的就是之前的sds，dict这些数据结构的内存地址。</p>
<p>变量申明加冒号的用法： 是C语言的位域的用法。<code>:</code>后面的数字用来限定成员变量占用的位数。冒号后面的数字说明只会用到对应多少个bit位。type只会用到4个bit，encoding 也只会用的4个bit。变量占用的内存不再是有类型决定，而是位域。</p>
<h3 id="8-1-1-类型"><a href="#8-1-1-类型" class="headerlink" title="8.1.1 类型"></a>8.1.1 类型</h3><p>type属性的值包括5种：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">类型常量</th>
<th style="text-align:left">对象的名称</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>REDIS_STRING</code></td>
<td style="text-align:left">字符串对象</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_LIST</code></td>
<td style="text-align:left">列表对象</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_HASH</code></td>
<td style="text-align:left">哈希对象</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_SET</code></td>
<td style="text-align:left">集合对象</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_ZSET</code></td>
<td style="text-align:left">有序集合对象</td>
</tr>
</tbody>
</table>
</div>
<p>键对象只能是字符串对象，而值对象则5种之一。所以执行TYPE命令返回的是键值对中值对象的类型。</p>
<p>type命令的</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">对象</th>
<th style="text-align:left">对象 <code>type</code> 属性的值</th>
<th style="text-align:left">TYPE 命令的输出</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">字符串对象</td>
<td style="text-align:left"><code>REDIS_STRING</code></td>
<td style="text-align:left"><code>&quot;string&quot;</code></td>
</tr>
<tr>
<td style="text-align:left">列表对象</td>
<td style="text-align:left"><code>REDIS_LIST</code></td>
<td style="text-align:left"><code>&quot;list&quot;</code></td>
</tr>
<tr>
<td style="text-align:left">哈希对象</td>
<td style="text-align:left"><code>REDIS_HASH</code></td>
<td style="text-align:left"><code>&quot;hash&quot;</code></td>
</tr>
<tr>
<td style="text-align:left">集合对象</td>
<td style="text-align:left"><code>REDIS_SET</code></td>
<td style="text-align:left"><code>&quot;set&quot;</code></td>
</tr>
<tr>
<td style="text-align:left">有序集合对象</td>
<td style="text-align:left"><code>REDIS_ZSET</code></td>
<td style="text-align:left"><code>&quot;zset&quot;</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="8-2-编码和底层实现"><a href="#8-2-编码和底层实现" class="headerlink" title="8.2 编码和底层实现"></a>8.2 编码和底层实现</h3><p>ptr指向底层数据结构，而encoding则决定了具体应该指向什么样的数据结构实现。之所以不能用type来决定指向的具体数据结构，是因为同一种对象，但是底层实现会有不同的数据结构。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">编码常量</th>
<th style="text-align:left">编码所对应的底层数据结构</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>REDIS_ENCODING_INT</code></td>
<td style="text-align:left"><code>long</code> 类型的整数</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_ENCODING_EMBSTR</code></td>
<td style="text-align:left"><code>embstr</code> 编码的简单动态字符串</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_ENCODING_RAW</code></td>
<td style="text-align:left">简单动态字符串</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_ENCODING_HT</code></td>
<td style="text-align:left">字典</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_ENCODING_LINKEDLIST</code></td>
<td style="text-align:left">双端链表</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_ENCODING_ZIPLIST</code></td>
<td style="text-align:left">压缩列表</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_ENCODING_INTSET</code></td>
<td style="text-align:left">整数集合</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_ENCODING_SKIPLIST</code></td>
<td style="text-align:left">跳跃表和字典</td>
</tr>
</tbody>
</table>
</div>
<p>每种类型的对象都至少使用了两种不同的编码，对应关系为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">类型</th>
<th style="text-align:left">编码</th>
<th style="text-align:left">对象</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>REDIS_STRING</code></td>
<td style="text-align:left"><code>REDIS_ENCODING_INT</code></td>
<td style="text-align:left">使用整数值实现的字符串对象。</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_STRING</code></td>
<td style="text-align:left"><code>REDIS_ENCODING_EMBSTR</code></td>
<td style="text-align:left">使用 <code>embstr</code> 编码的简单动态字符串实现的字符串对象。</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_STRING</code></td>
<td style="text-align:left"><code>REDIS_ENCODING_RAW</code></td>
<td style="text-align:left">使用简单动态字符串实现的字符串对象。</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_LIST</code></td>
<td style="text-align:left"><code>REDIS_ENCODING_ZIPLIST</code></td>
<td style="text-align:left">使用压缩列表实现的列表对象。</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_LIST</code></td>
<td style="text-align:left"><code>REDIS_ENCODING_LINKEDLIST</code></td>
<td style="text-align:left">使用双端链表实现的列表对象。</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_HASH</code></td>
<td style="text-align:left"><code>REDIS_ENCODING_ZIPLIST</code></td>
<td style="text-align:left">使用压缩列表实现的哈希对象。</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_HASH</code></td>
<td style="text-align:left"><code>REDIS_ENCODING_HT</code></td>
<td style="text-align:left">使用字典实现的哈希对象。</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_SET</code></td>
<td style="text-align:left"><code>REDIS_ENCODING_INTSET</code></td>
<td style="text-align:left">使用整数集合实现的集合对象。</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_SET</code></td>
<td style="text-align:left"><code>REDIS_ENCODING_HT</code></td>
<td style="text-align:left">使用字典实现的集合对象。</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_ZSET</code></td>
<td style="text-align:left"><code>REDIS_ENCODING_ZIPLIST</code></td>
<td style="text-align:left">使用压缩列表实现的有序集合对象。</td>
</tr>
<tr>
<td style="text-align:left"><code>REDIS_ZSET</code></td>
<td style="text-align:left"><code>REDIS_ENCODING_SKIPLIST</code></td>
<td style="text-align:left">使用跳跃表和字典实现的有序集合对象。</td>
</tr>
</tbody>
</table>
</div>
<p>使用 OBJECT ENCODING 命令可以查看一个数据库键的值对象的编码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">redis&gt; SET msg &quot;hello wrold&quot;</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">redis&gt; OBJECT ENCODING msg</span><br><span class="line">&quot;embstr&quot;</span><br></pre></td></tr></table></figure>
<p>不同编码的对象所对应的OBJECT ENCODING输出：</p>
<p>表 8-5 OBJECT ENCODING 对不同编码的输出</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">对象所使用的底层数据结构</th>
<th style="text-align:left">编码常量</th>
<th style="text-align:left">OBJECT ENCODING 命令输出</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">整数</td>
<td style="text-align:left"><code>REDIS_ENCODING_INT</code></td>
<td style="text-align:left"><code>&quot;int&quot;</code></td>
</tr>
<tr>
<td style="text-align:left"><code>embstr</code> 编码的简单动态字符串（SDS）</td>
<td style="text-align:left"><code>REDIS_ENCODING_EMBSTR</code></td>
<td style="text-align:left"><code>&quot;embstr&quot;</code></td>
</tr>
<tr>
<td style="text-align:left">简单动态字符串</td>
<td style="text-align:left"><code>REDIS_ENCODING_RAW</code></td>
<td style="text-align:left"><code>&quot;raw&quot;</code></td>
</tr>
<tr>
<td style="text-align:left">字典</td>
<td style="text-align:left"><code>REDIS_ENCODING_HT</code></td>
<td style="text-align:left"><code>&quot;hashtable&quot;</code></td>
</tr>
<tr>
<td style="text-align:left">双端链表</td>
<td style="text-align:left"><code>REDIS_ENCODING_LINKEDLIST</code></td>
<td style="text-align:left"><code>&quot;linkedlist&quot;</code></td>
</tr>
<tr>
<td style="text-align:left">压缩列表</td>
<td style="text-align:left"><code>REDIS_ENCODING_ZIPLIST</code></td>
<td style="text-align:left"><code>&quot;ziplist&quot;</code></td>
</tr>
<tr>
<td style="text-align:left">整数集合</td>
<td style="text-align:left"><code>REDIS_ENCODING_INTSET</code></td>
<td style="text-align:left"><code>&quot;intset&quot;</code></td>
</tr>
<tr>
<td style="text-align:left">跳跃表和字典</td>
<td style="text-align:left"><code>REDIS_ENCODING_SKIPLIST</code></td>
<td style="text-align:left"><code>&quot;skiplist&quot;</code></td>
</tr>
</tbody>
</table>
</div>
<p>使用encoding属性来设定对象指向的具体数据结构实现的好处：灵活与效率。根据不同场景来给同一种对象设置不同的数据结构实现。</p>
<p>举个例子， 在列表对象包含的元素比较少时， Redis 使用压缩列表作为列表对象的底层实现：</p>
<ul>
<li>因为压缩列表比双端链表更节约内存， 并且在元素数量较少时， 在内存中以连续块方式保存的压缩列表比起双端链表可以更快被载入到缓存中；</li>
<li>随着列表对象包含的元素越来越多， 使用压缩列表来保存元素的优势逐渐消失时， 对象就会将底层实现从压缩列表转向功能更强、也更适合保存大量元素的双端链表上面；</li>
</ul>
<p>是不是也可以理解成多态呢？</p>
<h2 id="8-2-字符串对象"><a href="#8-2-字符串对象" class="headerlink" title="8.2 字符串对象"></a>8.2 字符串对象</h2><p>字符串对象的编码可以是 <code>int</code> 、 <code>raw</code> 或者 <code>embstr</code> 。</p>
<ol>
<li>保存的对象是整数，而且可以用long类型表示：ptr指针指向一个long类型的内存地址。（是不是不用指向sdshdr了），然后将encoding 设置为REDIS_ENCODING_INT (应该在源码中有个宏定义)</li>
<li>保存的对象是字符串值，长度小于44字节（书上的39字节，但是先版本已经更新）。这种短字符串对象，直接<strong>只调用一次内存分配</strong>函数来分配一块<strong>连续的空间</strong>  依次包含redisObject和sdshdr两个结构。</li>
<li>字符串长度大于等于44字节的时候使用raw编码。会调用两次内存分配函数来创建redisObject结构和sdshdr结构。</li>
</ol>
<p>embstr编码的好处：</p>
<ul>
<li>只用分配一次内存，raw编码需要两次</li>
<li>释放embstr的sds也只需要一次内存释放函数，raw两次</li>
<li>embstr的所有数据（对象结构，字符串结构）在连续的一块内存里面，从而可以更好地利用缓存带来的优势</li>
</ul>
<p>embstr和raw的示意图，摘自书上，但是注意sds的结构稍有变化。</p>
<p>raw编码：</p>
<p><img src="http://redisbook.com/_images/graphviz-8731210637d0567af28d3a9d4089d5f864d29950.png" alt=""></p>
<p>embstr编码：</p>
<p><img src="http://redisbook.com/_images/graphviz-900c13b23ce79372939259603be936c955ccaa62.png" alt=""></p>
<p>注意：</p>
<p>long double类型的浮点数是转换成字符串值来保存的。执行的时候取出字符串再转换成long double 类型。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">值</th>
<th style="text-align:left">编码</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">可以用 <code>long</code> 类型保存的整数。</td>
<td style="text-align:left"><code>int</code></td>
</tr>
<tr>
<td style="text-align:left">可以用 <code>long double</code> 类型保存的浮点数。</td>
<td style="text-align:left"><code>embstr</code> 或者 <code>raw</code></td>
</tr>
<tr>
<td style="text-align:left">字符串值， 或者因为长度太大而没办法用 <code>long</code> 类型表示的整数， 又或者因为长度太大而没办法用 <code>long double</code> 类型表示的浮点数。</td>
<td style="text-align:left"><code>embstr</code> 或者 <code>raw</code></td>
</tr>
</tbody>
</table>
</div>
<h3 id="8-2-1-编码的转换"><a href="#8-2-1-编码的转换" class="headerlink" title="8.2.1 编码的转换"></a>8.2.1 编码的转换</h3><p>当保存的值发生变化的时候会进行转换，比如保存的整数值变成了字符串，那么编码从int 变成raw或者embstr。</p>
<p><strong>注意</strong>： Redis 没有为embstr编写任何修改程序，只有int和raw编码的字符串对象有这些程序，所以embstr编码的对象是只读，因此发生修改的话会从embstr变成raw。</p>
<p>发生修改的源码中应该是将原来的ptr指向的数据结构内存释放掉后，重新指向新创建的对象。</p>
<h3 id="8-2-2-字符串命令的实现"><a href="#8-2-2-字符串命令的实现" class="headerlink" title="8.2.2 字符串命令的实现"></a>8.2.2 字符串命令的实现</h3><p>所有命令都是针对字符串对象的。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">命令</th>
<th style="text-align:left"><code>int</code> 编码的实现方法</th>
<th style="text-align:left"><code>embstr</code> 编码的实现方法</th>
<th style="text-align:left"><code>raw</code> 编码的实现方法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">SET</td>
<td style="text-align:left">使用 <code>int</code> 编码保存值。</td>
<td style="text-align:left">使用 <code>embstr</code> 编码保存值。</td>
<td style="text-align:left">使用 <code>raw</code> 编码保存值。</td>
</tr>
<tr>
<td style="text-align:left">GET</td>
<td style="text-align:left">拷贝对象所保存的整数值， 将这个拷贝转换成字符串值， 然后向客户端返回这个字符串值。</td>
<td style="text-align:left">直接向客户端返回字符串值。</td>
<td style="text-align:left">直接向客户端返回字符串值。</td>
</tr>
<tr>
<td style="text-align:left">APPEND</td>
<td style="text-align:left">将对象转换成 <code>raw</code> 编码， 然后按 <code>raw</code>编码的方式执行此操作。</td>
<td style="text-align:left">将对象转换成 <code>raw</code> 编码， 然后按 <code>raw</code>编码的方式执行此操作。</td>
<td style="text-align:left">调用 <code>sdscatlen</code> 函数， 将给定字符串追加到现有字符串的末尾。</td>
</tr>
<tr>
<td style="text-align:left">INCRBYFLOAT</td>
<td style="text-align:left">取出整数值并将其转换成 <code>longdouble</code> 类型的浮点数， 对这个浮点数进行加法计算， 然后将得出的浮点数结果保存起来。</td>
<td style="text-align:left">取出字符串值并尝试将其转换成<code>long double</code> 类型的浮点数， 对这个浮点数进行加法计算， 然后将得出的浮点数结果保存起来。 如果字符串值不能被转换成浮点数， 那么向客户端返回一个错误。</td>
<td style="text-align:left">取出字符串值并尝试将其转换成 <code>longdouble</code> 类型的浮点数， 对这个浮点数进行加法计算， 然后将得出的浮点数结果保存起来。 如果字符串值不能被转换成浮点数， 那么向客户端返回一个错误。</td>
</tr>
<tr>
<td style="text-align:left">INCRBY</td>
<td style="text-align:left">对整数值进行加法计算， 得出的计算结果会作为整数被保存起来。</td>
<td style="text-align:left"><code>embstr</code> 编码不能执行此命令， 向客户端返回一个错误。</td>
<td style="text-align:left"><code>raw</code> 编码不能执行此命令， 向客户端返回一个错误。</td>
</tr>
<tr>
<td style="text-align:left">DECRBY</td>
<td style="text-align:left">对整数值进行减法计算， 得出的计算结果会作为整数被保存起来。</td>
<td style="text-align:left"><code>embstr</code> 编码不能执行此命令， 向客户端返回一个错误。</td>
<td style="text-align:left"><code>raw</code> 编码不能执行此命令， 向客户端返回一个错误。</td>
</tr>
<tr>
<td style="text-align:left">STRLEN</td>
<td style="text-align:left">拷贝对象所保存的整数值， 将这个拷贝转换成字符串值， 计算并返回这个字符串值的长度。</td>
<td style="text-align:left">调用 <code>sdslen</code> 函数， 返回字符串的长度。</td>
<td style="text-align:left">调用 <code>sdslen</code> 函数， 返回字符串的长度。</td>
</tr>
<tr>
<td style="text-align:left">SETRANGE</td>
<td style="text-align:left">将对象转换成 <code>raw</code> 编码， 然后按 <code>raw</code>编码的方式执行此命令。</td>
<td style="text-align:left">将对象转换成 <code>raw</code> 编码， 然后按 <code>raw</code>编码的方式执行此命令。</td>
<td style="text-align:left">将字符串特定索引上的值设置为给定的字符。</td>
</tr>
<tr>
<td style="text-align:left">GETRANGE</td>
<td style="text-align:left">拷贝对象所保存的整数值， 将这个拷贝转换成字符串值， 然后取出并返回字符串指定索引上的字符。</td>
<td style="text-align:left">直接取出并返回字符串指定索引上的字符。</td>
<td style="text-align:left">直接取出并返回字符串指定索引上的字符。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="8-8-内存回收"><a href="#8-8-内存回收" class="headerlink" title="8.8 内存回收"></a>8.8 内存回收</h2><p>Redis通过跟踪对象的引用计数信息，在适当的时候自动释放对象并进行内存回收。</p>
<p>计数信息由 <code>redisObject</code> 结构的 <code>refcount</code> 属性记录</p>
<ul>
<li>在创建一个新对象时， 引用计数的值会被初始化为 <code>1</code> ；</li>
<li>当对象被一个新程序使用时， 它的引用计数值会被增一；</li>
<li>当对象不再被一个程序使用时， 它的引用计数值会被减一；</li>
<li>当对象的引用计数值变为 <code>0</code> 时， 对象所占用的内存会被释放。</li>
</ul>
<p>对应的API：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">函数</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>incrRefCount</code></td>
<td style="text-align:left">将对象的引用计数值增一。</td>
</tr>
<tr>
<td style="text-align:left"><code>decrRefCount</code></td>
<td style="text-align:left">将对象的引用计数值减一， 当对象的引用计数值等于 <code>0</code> 时， 释放对象。</td>
</tr>
<tr>
<td style="text-align:left"><code>resetRefCount</code></td>
<td style="text-align:left">将对象的引用计数值设置为 <code>0</code> ， 但并不释放对象， 这个函数通常在需要重新设置对象的引用计数值时使用。</td>
</tr>
</tbody>
</table>
</div>
<p>对象生命周期可以划分为创建对象、操作对象、释放对象三个阶段</p>
<p>一个字符串对象从创建到释放的整个过程：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建一个字符串对象 s ，对象的引用计数为 1</span></span><br><span class="line">robj *s = createStringObject(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对象 s 执行各种操作 ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将对象 s 的引用计数减一，使得对象的引用计数变为 0</span></span><br><span class="line"><span class="comment">// 导致对象 s 被释放</span></span><br><span class="line">decrRefCount(s)</span><br></pre></td></tr></table></figure>
<p>是不是通过计算指向对象这块内存的指针数量来实现的？</p>
<h3 id="8-9-对象共享"><a href="#8-9-对象共享" class="headerlink" title="8.9 对象共享"></a>8.9 对象共享</h3><p>对于整数值的字符串对象（int编码），在新创建一个字符串对象的时候如果有这个值的字符串对象存在，就不再为新键创建字符串对象，而是将这个新键指向之前已经存在了的字符串对象。然后将这个字符串对象的引用+1。主要目的是节约内存。</p>
<p>Redis初始化服务器的时候创建一万个字符串对象， 这些对象包含了从 <code>0</code> 到 <code>9999</code> 的所有整数值， 当服务器需要用到值为 <code>0</code>到 <code>9999</code> 的字符串对象时， 服务器就会使用这些共享对象， 而不是新创建对象。</p>
<p>创建共享字符串对象的数量可以通过修改 <code>redis.h/REDIS_SHARED_INTEGERS</code> 常量来修改。</p>
<p><strong>为什么 Redis 不共享包含字符串的对象？</strong></p>
<p>当服务器考虑将一个共享对象设置为键的值对象时， 程序需要先检查给定的共享对象和键想创建的目标对象是否完全相同， 只有在共享对象和目标对象完全相同的情况下， 程序才会将共享对象用作键的值对象， 而一个共享对象保存的值越复杂， 验证共享对象和目标对象是否相同所需的复杂度就会越高， 消耗的 CPU 时间也会越多：</p>
<ul>
<li>如果共享对象是保存整数值的字符串对象， 那么验证操作的复杂度为 O(1) ；</li>
<li>如果共享对象是保存字符串值的字符串对象， 那么验证操作的复杂度为 O(N) ；</li>
<li>如果共享对象是包含了多个值（或者对象的）对象， 比如列表对象或者哈希对象， 那么验证操作的复杂度将会是 O(N^2) 。</li>
</ul>
<p>因此， 尽管共享更复杂的对象可以节约更多的内存， 但受到 CPU 时间的限制， Redis 只对包含整数值的字符串对象进行共享。 （时刻在空间与时间上做权衡）</p>
<h2 id="8-9-对象的空转时长"><a href="#8-9-对象的空转时长" class="headerlink" title="8.9 对象的空转时长"></a>8.9 对象的空转时长</h2><p><code>lru</code> 属性， 该属性记录了对象最后一次被<strong>命令程序访问</strong>的时间。</p>
<p>OBJECT IDLETIME 命令可以打印出给定键的空转时长， 这一空转时长就是通过将当前时间减去键的值对象的 <code>lru</code> 时间计算得出的。</p>
<p>OBJECT IDLETIME 命令的实现是特殊的， 这个命令在访问键的值对象时， 不会修改值对象的 <code>lru</code> 属性。</p>
<p>lru的主要作用就是配合maxmemory选项，淘汰不常用的键值对。</p>
<p>如果服务器打开了 <code>maxmemory</code> 选项， 并且服务器用于回收内存的算法为 <code>volatile-lru</code> 或者 <code>allkeys-lru</code> ， 那么当服务器占用的内存数超过了 <code>maxmemory</code> 选项所设置的上限值时， 空转时长较高的那部分键会优先被服务器释放， 从而回收内存。</p>
<h2 id="疑惑"><a href="#疑惑" class="headerlink" title="疑惑"></a>疑惑</h2><ol>
<li>为什么在一块连续的内存里面可以更好地利用缓存带来的优势？</li>
<li>引用计数的程序怎么定义？是指线程吗？</li>
<li>引用计数的具体实现？</li>
<li>对象共享的时候怎么去判断存不存在已经创建了这个整数值的对象？对整个数据库遍历吗？</li>
</ol>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>object</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis 设计与实现读书笔记——第四章 字典</title>
    <url>/2019-03-27-Redis-%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%AD%97%E5%85%B8.html</url>
    <content><![CDATA[<h1 id="Redis-设计与实现读书笔记——第四章-字典"><a href="#Redis-设计与实现读书笔记——第四章-字典" class="headerlink" title="Redis 设计与实现读书笔记——第四章 字典"></a>Redis 设计与实现读书笔记——第四章 字典</h1><p>字典在Redis中应用很广泛，Redis的数据库就是用字典作为底层实现的，对数据库的增删改查操作也是构建在对字典的操作之上的。</p>
<a id="more"></a>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>作用：</p>
<ul>
<li>数据库底层实现</li>
<li>哈希键底层实现<ul>
<li>哈希键包含的键值对比较多，或者键值对中的元素都是比较长的字符串时，使用字典来实现。</li>
</ul>
</li>
<li>其他功能</li>
</ul>
<h2 id="4-1-字典的实现"><a href="#4-1-字典的实现" class="headerlink" title="4.1 字典的实现"></a>4.1 字典的实现</h2><p>字典使用哈希表实现，一个哈希表里面可以有多个哈希表节点，一个哈希表节点就保存了字典中的一个键值对。（python的dict也是使用哈希表实现的）。</p>
<p>哈希的本质就是预留内存空间，将需要存储的元素计算索引值(通过哈希函数)来确定对应的存储位置。当需要访问的时候可以通过哈希函数直接获得对应的地址。（编译器将变量名与地址做了映射，变量名是地址的别名，哈希则是将键与地址做了映射（通过哈希函数），大大提高了访问的效率。访问任何元素都是O（1），感觉是两个层面的映射，有点相似的感觉）。</p>
<h3 id="4-1-1-哈希表"><a href="#4-1-1-哈希表" class="headerlink" title="4.1.1 哈希表"></a>4.1.1 哈希表</h3><p>Redis使用的哈希表由dict.h/dictht 结构定义。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">dictht</span> &#123;</span></span><br><span class="line">    <span class="comment">//哈希表数组</span></span><br><span class="line">    dictEntry **table;</span><br><span class="line">    <span class="comment">// 哈希表大小</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> size;</span><br><span class="line">    <span class="comment">//哈希表大小掩码，用于计算索引值</span></span><br><span class="line">    <span class="comment">//总是等于size-1</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> sizemask;</span><br><span class="line">    <span class="comment">//该哈希表已有节点的数量</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> used;</span><br><span class="line">&#125; dictht;</span><br></pre></td></tr></table></figure>
<p><code>typedef Oldname newname</code> 所以这里是<code>typedef struct dictht dictht</code> 将struct dictht 取了个别名dictht。</p>
<p><code>dictEntry **table;</code> 书上解释的是table是数组，但是最直接的说法是table是一个指向（指向dictEntry类型的指针）的指针，是指向指针的指针。   </p>
<p>数组获得内存是连续的，而指针不是，所以书中说是table数组，应该是分配内存的时候给指针分配了连续的内存，但是代码没找到。</p>
<p>关于指针和数组的异同《C专家编程》一书有讲解（第四章、第九章、第十章）。</p>
<ul>
<li>主要的不同：指针存放的是地址，所以需要经过两次取地址的内容。（取指针的地址中的数据（变量的地址），取变量地址的数据）。而数组是直接存储数组的首元素的数据，所以只用一次取地址中的数据</li>
<li>数组与指针相同<ul>
<li>表达式中的数组名就是指针</li>
<li>C语言把数组下标作为指针的偏移量。a是数组，a[6]就是首地址偏移6。b是指针，b[6]也是指针存储的地址向后偏移6.</li>
<li>作为函数参数的数组名等同于指针。只是把首地址传入给了参数，并没有把数组所有的内存区域都传入。所以传入数组，就是传入指针。</li>
</ul>
</li>
</ul>
<p>sizemask和哈希值一起决定一个键应该被放到table数组的那个索引上面。</p>
<p><img src="http://redisbook.com/_images/graphviz-bd3eecd927a4d8fc33b4a1c7f5957c52d67c5021.png" alt="一个空的哈希表"></p>
<h3 id="4-1-2-哈希表节点"><a href="#4-1-2-哈希表节点" class="headerlink" title="4.1.2 哈希表节点"></a>4.1.2 哈希表节点</h3><p>使用dictEntry结构体表示节点，每一个dictEntry结构都保存着一个键值对。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">dictEntry</span> &#123;</span></span><br><span class="line">    <span class="comment">// 键</span></span><br><span class="line">    <span class="keyword">void</span> *key;	</span><br><span class="line">    <span class="comment">// 值</span></span><br><span class="line">    <span class="class"><span class="keyword">union</span> &#123;</span></span><br><span class="line">        <span class="keyword">void</span> *val;</span><br><span class="line">        <span class="keyword">uint64_t</span> u64;</span><br><span class="line">        <span class="keyword">int64_t</span> s64;</span><br><span class="line">        <span class="keyword">double</span> d;</span><br><span class="line">    &#125; v;</span><br><span class="line">    <span class="comment">//指向下一个哈希表节点   </span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">dictEntry</span> *<span class="title">next</span>;</span></span><br><span class="line">&#125; dictEntry;</span><br></pre></td></tr></table></figure>
<p>key指向键值对中的键，而v则保存键值对中的值，值可以是一个指针，或者uint64_t整数，或者有符号的int64_t整数,或者是double类型（double也占8byte无论32还是64位）。</p>
<p><code>union</code> 是c中的共用体（联合体），和结构体非常类似。和结构体的区别是</p>
<ul>
<li>结构体的各个成员会占用不同的内存，互相之间没有影响；而共用体的所有成员占用同一段内存，修改一个成员会影响其余所有成员。</li>
<li>结构体占用的内存大于等于所有成员占用的内存的总和（成员之间可能会存在缝隙），共用体占用的内存等于最长的成员占用的内存。共用体使用了内存覆盖技术，同一时刻只能保存一个成员的值，如果对新的成员赋值，就会把原来成员的值覆盖掉。 所以这里面v占用8byte内存，但是属性却可以有4中。</li>
</ul>
<p>next指针可以将多个哈希值相同的键值对链接在一起，用来解决哈希键冲突(索引相同)的问题。链式的方法。</p>
<p><img src="http://redisbook.com/_images/graphviz-d2641d962325fd58bf15d9fffb4208f70251a999.png" alt=""></p>
<h3 id="4-1-3-字典"><a href="#4-1-3-字典" class="headerlink" title="4.1.3 字典"></a>4.1.3 字典</h3><p>Redis中字典由dict.h/dict 结构体表示。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">dict</span> &#123;</span></span><br><span class="line">    <span class="comment">//类型特定函数</span></span><br><span class="line">    dictType *type;</span><br><span class="line">    <span class="comment">// 私有数据</span></span><br><span class="line">    <span class="keyword">void</span> *privdata;</span><br><span class="line">    <span class="comment">// 哈希表</span></span><br><span class="line">    dictht ht[<span class="number">2</span>];</span><br><span class="line">    <span class="comment">//rehash 索引</span></span><br><span class="line">    <span class="comment">//当rehash不在进行是值为-1。用来记录是否在rehash</span></span><br><span class="line">    <span class="keyword">long</span> rehashidx; <span class="comment">/* rehashing not in progress if rehashidx == -1 */</span></span><br><span class="line">    <span class="comment">//当前迭代的个数</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> iterators; <span class="comment">/* number of iterators currently running */</span></span><br><span class="line">&#125; dict;</span><br></pre></td></tr></table></figure>
<p>type 和private属性是针对不同类型的键值对（redis支持5中数据类型），为创建多态字典而设置的。</p>
<ul>
<li>type指向dictType结构，dictType结构保存了一簇用于操作特定类型键值对的函数，Redis为不同用途的字典设置了不同的类型的特定函数。（多态的表现。）</li>
<li>privadata属性保存了需要传给哪些类型特点函数的可选参数。</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">dictType</span> &#123;</span></span><br><span class="line">    <span class="comment">// 计算哈希值的函数</span></span><br><span class="line">    <span class="keyword">uint64_t</span> (*hashFunction)(<span class="keyword">const</span> <span class="keyword">void</span> *key);</span><br><span class="line">    <span class="comment">// 复制键的函数</span></span><br><span class="line">    <span class="keyword">void</span> *(*keyDup)(<span class="keyword">void</span> *privdata, <span class="keyword">const</span> <span class="keyword">void</span> *key);</span><br><span class="line">    <span class="comment">// 复制值的函数</span></span><br><span class="line">    <span class="keyword">void</span> *(*valDup)(<span class="keyword">void</span> *privdata, <span class="keyword">const</span> <span class="keyword">void</span> *obj);</span><br><span class="line">    <span class="comment">// 对比键的函数</span></span><br><span class="line">    <span class="keyword">int</span> (*keyCompare)(<span class="keyword">void</span> *privdata, <span class="keyword">const</span> <span class="keyword">void</span> *key1, <span class="keyword">const</span> <span class="keyword">void</span> *key2);</span><br><span class="line">    <span class="comment">// 销毁键的函数</span></span><br><span class="line">    <span class="keyword">void</span> (*keyDestructor)(<span class="keyword">void</span> *privdata, <span class="keyword">void</span> *key);</span><br><span class="line">    <span class="comment">// 销毁值的函数</span></span><br><span class="line">    <span class="keyword">void</span> (*valDestructor)(<span class="keyword">void</span> *privdata, <span class="keyword">void</span> *obj);</span><br><span class="line">&#125; dictType;</span><br></pre></td></tr></table></figure>
<p>ht是长度为2的数组，而数组的元素就是哈希表dictht。一般情况只会使用ht[0],ht[1]只会在ht[0]空间不够的时候进行rehash的时候使用。</p>
<p>rehashidx 用来标识是否在 rehash，没有的话值为-1,有的话rehasidx用来记录rehash的进度。</p>
<p><img src="http://redisbook.com/_images/graphviz-e73003b166b90094c8c4b7abbc8d59f691f91e27.png" alt=""></p>
<h2 id="4-2-哈希算法"><a href="#4-2-哈希算法" class="headerlink" title="4.2 哈希算法"></a>4.2 哈希算法</h2><p>添加一个新的键值对的时候，</p>
<pre><code> 1. 需要先根据键值使用hash函数计算哈希值
    2. 哈希值和sizemask并运算求得索引值
    3. 再根据索引值将包含键值对的哈希表节点放到哈希表数组table上的对应索引值上。
</code></pre><p>Redis 计算哈希值和索引值的方法为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 使用字典设置的哈希函数，计算键 key 的哈希值</span><br><span class="line">hash &#x3D; dict-&gt;type-&gt;hashFunction(key);</span><br><span class="line"></span><br><span class="line"># 使用哈希表的 sizemask 属性和哈希值，计算出索引值</span><br><span class="line"># 根据情况不同， ht[x] 可以是 ht[0] 或者 ht[1]</span><br><span class="line">index &#x3D; hash &amp; dict-&gt;ht[x].sizemask;</span><br></pre></td></tr></table></figure>
<p>哈希值用hashFunction计算，index用hash值和sizemask进行并操作。</p>
<p>对应在源码中为：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//dict.h 中：</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> dictHashKey(d, key) (d)-&gt;type-&gt;hashFunction(key) <span class="comment">//计算哈希值</span></span></span><br><span class="line"><span class="comment">//dict.c 中</span></span><br><span class="line">idx = hash &amp; d-&gt;ht[table].sizemask;</span><br><span class="line">h = dictHashKey(d, de-&gt;key) &amp; d-&gt;ht[<span class="number">1</span>].sizemask;</span><br></pre></td></tr></table></figure>
<p>插入一个键值对<k0,v0>到字典中的过程为：</k0,v0></p>
<p><img src="http://redisbook.com/_images/graphviz-ca9c88ba1713cb595c4e0e4a17ef2042b0008621.png" alt=""></p>
<p>先使用语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hash &#x3D; dict-&gt;type-&gt;hashFunction(k0);</span><br></pre></td></tr></table></figure>
<p>计算键 <code>k0</code> 的哈希值。</p>
<p>假设计算得出的哈希值为 <code>8</code> ， 那么程序会继续使用语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">index &#x3D; hash &amp; dict-&gt;ht[0].sizemask &#x3D; 8 &amp; 3 &#x3D; 0;</span><br></pre></td></tr></table></figure>
<p>计算出键 <code>k0</code> 的索引值 <code>0</code> ， 这表示包含键值对 <code>k0</code> 和 <code>v0</code> 的节点应该被放置到哈希表数组的索引 <code>0</code> 位置上。</p>
<p><img src="http://redisbook.com/_images/graphviz-2369cb08288f5e59548355042c4c40cc4339e339.png" alt=""></p>
<p>当字典被用作数据库的底层实现， 或者哈希键的底层实现时， Redis 使用 MurmurHash2 算法来计算键的哈希值。算法的优点</p>
<ul>
<li>即使输入的键是有规律的， 算法仍能给出一个很好的随机分布性， 并且算法的计算速度也非常快。</li>
</ul>
<p>MurmurHash 算法目前的最新版本为 MurmurHash3 ， 而 Redis 使用的是 MurmurHash2 ， 关于 MurmurHash 算法的更多信息可以参考该算法的主页： <a href="http://code.google.com/p/smhasher/">http://code.google.com/p/smhasher/</a> 。</p>
<p>暂时没有找到hashFunction的源码。</p>
<h3 id="4-3-键冲突"><a href="#4-3-键冲突" class="headerlink" title="4.3 键冲突"></a>4.3 键冲突</h3><p>两个以上的键分配到了同一个索引就产生了冲突。</p>
<p>使用链地址法解决，相同索引上面的节点可以用next指针来链接，这样同一个索引就可以存放多个哈希表节点了。</p>
<p>哈希表节点没有指向链表链尾的节点，所以不能迅速的知道哪个节点是尾节点，（只能通过遍历才能找到指向null的尾节点O(N)）,所以为了速度考虑总是将新节点插入到最前面（dictht-&gt;table指针指向的第一个节点就是最前面的节点。）</p>
<p>直接使用书上的图：</p>
<p>还未发生键冲突</p>
<p><img src="http://redisbook.com/_images/graphviz-db40aa6d0f1265d74f1c1d12b8fa074c0bcda12a.png" alt=""></p>
<p>键冲突发生后：新键值对<k2,v2> 插入到<k1,v1>前面。</k1,v1></k2,v2></p>
<p><img src="http://redisbook.com/_images/graphviz-4b52dcf6eb0768750e1c15480be3326ca37e05b3.png" alt=""></p>
<h2 id="4-4-rehash"><a href="#4-4-rehash" class="headerlink" title="4.4  rehash"></a>4.4  rehash</h2><p>哈希表保存的键值对会增多或者减少，需要让负载因子（负载因子(load factor)，它用来衡量哈希表的 <strong>空/满</strong> 程度，一定程度上也可以体现查询的效率）维持在合理的范围，所以哈希节点太多的时候需要扩容（不然冲突太多，降低效率），这些哈希表的空间动态扩容或者缩容通过rehash操作来实现。</p>
<p>步骤为：</p>
<ol>
<li>为ht[1]分配空间<ol>
<li>扩容时ht[1].size = 大于等于ht[0].used*2的第一个2^n。</li>
<li>收缩时ht[1].size = 大于等于ht[0].used的第一个2^n。</li>
</ol>
</li>
<li>迁移：将ht[0]上面的哈希节点重新计算哈希值与索引值并放到ht[1]哈希表上面。</li>
<li>迁移完成后释放ht[0],把ht[1]设置成ht[0],并在ht[1]新创建一个空白哈希表，等待下一次rehash。</li>
</ol>
<p>注意：不管是扩容还是收缩，新分配的空间都会比原来用到的(used)大。设置ht[1]为ht[0] 的时候只需要简单的将原来ht[0]上面的指针指向ht[1]就好，但是需要把原来指向的哈希表的内存给释放掉。（指针是真的强，每次指针变换指向的地址的时候，都需要考虑下之前指向的地址是否还需要，不需要就要释放，不然会造成内存泄漏）。ht[1]则重新新建个哈希表结构体，然后把ht[1]的指针指过来就好了。</p>
<p>rehash的过程图解参考书中的图讲的很详细。</p>
<p><img src="http://redisbook.com/_images/graphviz-93608325578e8e45848938ef420115bf2227639e.png" alt=""></p>
<p>假设程序要对图 4-8 所示字典的 <code>ht[0]</code> 进行扩展操作。</p>
<ol>
<li><code>ht[0].used</code> 当前的值为 <code>4</code> ， <code>4 * 2 = 8</code> ， 而 <code>8</code> （2^3）恰好是第一个大于等于 <code>4</code> 的 <code>2</code> 的 <code>n</code> 次方， 所以程序会将 <code>ht[1]</code> 哈希表的大小设置为 <code>8</code> 。图 4-9 展示了 <code>ht[1]</code> 在分配空间之后， 字典的样子：</li>
</ol>
<p><img src="http://redisbook.com/_images/graphviz-b68acb4d868ec7d79a44935ce08a159746ca58da.png" alt=""></p>
<ol>
<li>将 <code>ht[0]</code> 包含的四个键值对都 rehash 到 <code>ht[1]</code> ， 如图 4-10 所示。</li>
</ol>
<p><img src="http://redisbook.com/_images/graphviz-92dc47e4329eabae941cddfd727b736ef738e8cf.png" alt=""></p>
<ol>
<li>释放 <code>ht[0]</code> ，并将 <code>ht[1]</code> 设置为 <code>ht[0]</code> ，然后为 <code>ht[1]</code> 分配一个空白哈希表，如图 4-11 所示。</li>
</ol>
<p><img src="http://redisbook.com/_images/graphviz-fa28d986a72f1f48b83c7f959ea217b1f9527d3c.png" alt=""></p>
<p>哈希表的扩展与收缩（源码还没找到）</p>
<p>当以下条件中的任意一个被满足时， 程序会自动开始对哈希表执行扩展操作：</p>
<ol>
<li>服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 <code>1</code> ；</li>
<li>服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 <code>5</code> ；</li>
</ol>
<p>其中哈希表的负载因子可以通过公式得到。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 负载因子 &#x3D; 哈希表已保存节点数量 &#x2F; 哈希表大小</span><br><span class="line">load_factor &#x3D; ht[0].used &#x2F; ht[0].size</span><br></pre></td></tr></table></figure>
<h2 id="4-5-渐进式rehash"><a href="#4-5-渐进式rehash" class="headerlink" title="4.5 渐进式rehash"></a>4.5 渐进式rehash</h2><p>考虑到redis数据库中存储的键值对很多的情况，如果一次性就rehash完，庞大的计算量可能会导致服务器性能急剧下降，甚至一段时间的停止服务，（经济学中的休克时疗法？）所以rehash这个过程需要渐进式的（软着陆）。</p>
<p>步骤：</p>
<ol>
<li>为 <code>ht[1]</code> 分配空间， 让字典同时持有 <code>ht[0]</code> 和 <code>ht[1]</code> 两个哈希表。</li>
<li>在dict中维持一个索引计数器变量 <code>rehashidx</code> ， 并将它的值设置为 <code>0</code> ， 表示 rehash 工作正式开始。</li>
<li>在 rehash 进行期间， 每次对字典执行添加、删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 <code>ht[0]</code> 哈希表在 <code>rehashidx</code> 索引上的所有键值对 rehash 到 <code>ht[1]</code> ， 当 rehash 工作完成之后， 程序将 <code>rehashidx</code> 属性的值增一。</li>
<li>随着字典操作的不断执行， 最终在某个时间点上， <code>ht[0]</code> 的所有键值对都会被 rehash 至 <code>ht[1]</code> ， 这时程序将 <code>rehashidx</code> 属性的值设为 <code>-1</code> ， 表示 rehash 操作已完成。</li>
</ol>
<p>注意：</p>
<ul>
<li>rehashidx的范围是从-1到ht[0].size-1 ，也就是对应的哈希表的索引，不是之前理解的记录哈希节点的个数。</li>
<li>迁移的过程不是拷贝，而是哈希表存储的哈希指针重新指向哈希节点的过程（指针对地址的操作）。</li>
</ul>
<p>rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量。</p>
<p>过程图解依然copy自redis设计与实现的电子书。</p>
<p><img src="http://redisbook.com/_images/graphviz-4c43eaf38cbca10d8d368a5144db6f3c69ab3d84.png" alt=""></p>
<p><img src="http://redisbook.com/_images/graphviz-b91705b0d7a6c7fd5e37332a930534e0e136ae73.png" alt=""></p>
<p><img src="http://redisbook.com/_images/graphviz-9e2996e6ca9665776062470cdac346e8fc255374.png" alt=""></p>
<p><img src="http://redisbook.com/_images/graphviz-c871b5de1a7910aea237ca9dc86508b48da94769.png" alt=""></p>
<p><img src="http://redisbook.com/_images/graphviz-3b31e4e08cc3e212f986039eb08ae77224cdeec9.png" alt=""></p>
<p><img src="http://redisbook.com/_images/graphviz-86f810ac65c4e6ee58b17105dfeaa06973d8dd16.png" alt=""></p>
<h3 id="渐进式rehash期间的哈希表操作"><a href="#渐进式rehash期间的哈希表操作" class="headerlink" title="渐进式rehash期间的哈希表操作"></a>渐进式rehash期间的哈希表操作</h3><p>增删改查在两个表上操作，但是添加的新键只添加到ht[1]上，确保ht[0]最终变成空表。</p>
<h2 id="4-6-字典API"><a href="#4-6-字典API" class="headerlink" title="4.6 字典API"></a>4.6 字典API</h2><h2 id="疑惑"><a href="#疑惑" class="headerlink" title="疑惑"></a>疑惑</h2><ul>
<li><p>为什么不直接用<code>dictEntry *table[]</code> 来表示，却要用指针？应该是指针更加灵活些？</p>
<ul>
<li>因为dict有两个哈希表，而其中有个哈希表ht[1] 是只有在rehash的时候才会使用的，因此使用指针的的话，在不需要的时候将table直接指向null就可以简单的完成释放空间的操作，如果用数组的话,在分配内存的时候，就必须要为两个dictht都分配size个byte的内存存储空间，当rehash的时候size会变化，结构体中的数组就需要重新分配内存空间，而结构体的内存是相邻连续的，所以这时候要变化空间，只能重新再给这个结构体分配空间，这样效率肯定就很低了。所以使用指针的原因就是rehash的时候数组的大小会变化，如果用数组来记录就很麻烦和效率低了。</li>
<li>所以在dict中可以使用数组来存储dictht,因为ht这个数组是不会变化的。</li>
</ul>
</li>
<li><p>既然在rehash过程中会有增删改查的操作，那么这些操作是在哪个哈希表上进行的呢？</p>
<ul>
<li>两个哈希表上面都会进行，但是增加的键值对会在ht[1] 上，不然ht[0]在减少的时候又新增键值对，不是在做无用之功吗。</li>
</ul>
</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://redisbook.com/preview/dict/hash_algorithm.html">http://redisbook.com/preview/dict/hash_algorithm.html</a></p>
<p><a href="https://blog.csdn.net/yangbodong22011/article/details/78467583">https://blog.csdn.net/yangbodong22011/article/details/78467583</a></p>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>dict</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Travis-CI自动部署博客</title>
    <url>/2019-03-26-%E4%BD%BF%E7%94%A8Travis-CI%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2.html</url>
    <content><![CDATA[<h1 id="使用Travis-CI自动部署博客"><a href="#使用Travis-CI自动部署博客" class="headerlink" title="使用Travis-CI自动部署博客"></a>使用Travis-CI自动部署博客</h1><p>因为在github上面存储的是编译好的html代码，不是hexo博客中的博客源文件，所以如果没有备份源文件的话，丢失了源文件（包括md,一堆配置文件）后就不能继续更新博客了，而且也不能多端部署Hexo。网上看到可以使用Travis-CI来自动部署，同时解决了博客源文件的备份问题。</p>
<a id="more"></a>
<h2 id="新建hexo分支来保存源文件"><a href="#新建hexo分支来保存源文件" class="headerlink" title="新建hexo分支来保存源文件"></a>新建hexo分支来保存源文件</h2><h3 id="新建分支"><a href="#新建分支" class="headerlink" title="新建分支"></a>新建分支</h3><p>直接在对应的github.io项目上的branch 按钮处点击新建分支 hexo。</p>
<p>官方文档<a href="https://help.github.com/en/articles/setting-the-default-branch">https://help.github.com/en/articles/setting-the-default-branch</a> </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 克隆项目到本地</span><br><span class="line">&gt; git clone https:&#x2F;&#x2F;github.com&#x2F;BraveY&#x2F;BraveY.github.io.git</span><br></pre></td></tr></table></figure>
<h3 id="设置默认分支"><a href="#设置默认分支" class="headerlink" title="设置默认分支"></a>设置默认分支</h3><p>参照官方文档设置<a href="https://help.github.com/en/articles/setting-the-default-branch">https://help.github.com/en/articles/setting-the-default-branch</a></p>
<h3 id="修改推送hexo分支"><a href="#修改推送hexo分支" class="headerlink" title="修改推送hexo分支"></a>修改推送hexo分支</h3><p>现将原来的文件BraveY.github.io 修改为BraveY.github.io.bak 避免拷贝的时候出现文件夹重名，同时将原来的hexo 目录blog文件也修改为blog.bak</p>
<p>直接克隆到 本地</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 克隆项目到本地</span><br><span class="line">&gt; git clone https:&#x2F;&#x2F;github.com&#x2F;BraveY&#x2F;BraveY.github.io.git</span><br></pre></td></tr></table></figure>
<p>这时拷贝的项目已经是hexo分支下面的，因为之前是设置了hexo分支作为默认分支的。</p>
<p>将克隆的BraveY.github.io文件夹重命名为blog</p>
<p>在克隆的文件夹里面删除除了.git的所有文件</p>
<p><strong>注意</strong> ：因为主题的next也是个git仓库，所以需要先把next目录下的.git删除</p>
<p>然后先把theme/next 目录下的.git 给删除掉，不然的话next这个主题本来就是个git仓库，里面的个性化配置是无法上传到自己的项目中的，而且以后也无法克隆。</p>
<p>从blog.bak文件中把所有hexo的源码文件拷过来。</p>
<p>在博客目录下初始化git仓库并将所有文件提交。</p>
<p>添加到远程仓库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add . </span><br><span class="line">git commit -m&quot;description&quot;</span><br><span class="line">git push origin </span><br></pre></td></tr></table></figure>
<p>现在hexo分支已经是存放了hexo的源文件了。</p>
<h2 id="设置Travis-CI"><a href="#设置Travis-CI" class="headerlink" title="设置Travis CI"></a>设置Travis CI</h2><p>主要参考下面的三个博客，</p>
<h3 id="Travis-CI-官网登陆"><a href="#Travis-CI-官网登陆" class="headerlink" title="Travis CI 官网登陆"></a>Travis CI 官网登陆</h3><p>使用github账号登陆<a href="https://travis-ci.org/">Travis CI官网</a>  ，选择博客项目启动。然后进行设置。</p>
<p>没有文章中说的<code>Build only if .travis.yml is present</code> 选项，但好像没有影响，采用默认的选项。</p>
<h3 id="github生成Access-Token"><a href="#github生成Access-Token" class="headerlink" title="github生成Access Token"></a>github生成Access Token</h3><p>参考博客，不赘述。</p>
<h2 id="Travis-CI"><a href="#Travis-CI" class="headerlink" title="Travis CI"></a>Travis CI</h2><h3 id="配置travis-yml"><a href="#配置travis-yml" class="headerlink" title="配置travis.yml"></a>配置travis.yml</h3><p>在blog根目录下设置travis配置文件。</p>
<p>.travis.yml 文件的配置为：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">language:</span> <span class="string">node_js</span> <span class="comment"># 设置语言</span></span><br><span class="line"><span class="attr">node_js:</span> <span class="string">stable</span> <span class="comment"># 设置相应版本</span></span><br><span class="line"><span class="attr">cache:</span></span><br><span class="line">  <span class="attr">apt:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">directories:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">node_modules</span>  <span class="comment"># 缓存不经常更改的内容</span></span><br><span class="line"><span class="attr">before_install:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">export</span> <span class="string">TZ=&#x27;Asia/Shanghai&#x27;</span> <span class="comment"># 更改时区</span></span><br><span class="line"><span class="attr">install:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">npm</span> <span class="string">install</span></span><br><span class="line"><span class="attr">script:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">hexo</span> <span class="string">clean</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">hexo</span> <span class="string">g</span> </span><br><span class="line"><span class="attr">after_script:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">git</span> <span class="string">clone</span> <span class="string">https://$&#123;GH_REF&#125;</span> <span class="string">.deploy_git</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">cd</span> <span class="string">.deploy_git</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">git</span> <span class="string">checkout</span> <span class="string">master</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">cd</span> <span class="string">../</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">mv</span> <span class="string">.deploy_git/.git/</span> <span class="string">./public/</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">cd</span> <span class="string">./public</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">git</span> <span class="string">config</span> <span class="string">user.name</span> <span class="string">&quot;BraveY&quot;</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">git</span> <span class="string">config</span> <span class="string">user.email</span> <span class="string">&quot;lsz_yky@163.com&quot;</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">git</span> <span class="string">add</span> <span class="string">.</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">git</span> <span class="string">commit</span> <span class="string">-m</span> <span class="string">&quot;Travis CI Auto Builder at `date +&quot;</span><span class="string">%Y-%m-%d</span> <span class="string">%H:%M&quot;`&quot;</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">git</span> <span class="string">push</span> <span class="string">--force</span> <span class="string">--quiet</span> <span class="string">&quot;https://$&#123;github_blog@$&#123;GH_REF&#125;&quot;</span> <span class="string">master:master</span></span><br><span class="line"><span class="attr">branches:</span></span><br><span class="line">  <span class="attr">only:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">hexo</span></span><br><span class="line"><span class="attr">env:</span></span><br><span class="line">  <span class="attr">global:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">GH_REF:</span> <span class="string">github.com/BraveY/BraveY.github.io.git</span></span><br><span class="line"><span class="attr">notifications:</span></span><br><span class="line">  <span class="attr">email:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">lsz_yky@163.com</span></span><br><span class="line">  <span class="attr">on_success:</span> <span class="string">change</span></span><br><span class="line">  <span class="attr">on_failure:</span> <span class="string">always</span></span><br></pre></td></tr></table></figure>
<h3 id="新增文章到-posts"><a href="#新增文章到-posts" class="headerlink" title="新增文章到_posts"></a>新增文章到_posts</h3><p>增减文章后，使用git push 命令即可，自动部署。</p>
<p>可以看到 Travis CI 上面构建的一系列输出，但是博客并没有更新，而且github上的master分支也没有更改过。查看Travis CI 上的输出信息，发现在执行</p>
<p><code>git push --force --quiet &quot;https://$&#123;github_blog@$&#123;GH_REF&#125;&quot; master:master</code> 这句后报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;home&#x2F;travis&#x2F;.travis&#x2F;functions: eval: line 104: unexpected EOF while looking for matching &#96;&quot;&#39;</span><br><span class="line">&#x2F;home&#x2F;travis&#x2F;.travis&#x2F;functions: eval: line 105: syntax error: unexpected end of file</span><br></pre></td></tr></table></figure>
<p>然后发现<code>$&#123;github_blog@$&#123;GH_REF&#125;</code> 这句花括号没有对齐匹配，无语。加上漏掉的右边花括号} 重新执行操作。一切正常。</p>
<h2 id="博客更新操作"><a href="#博客更新操作" class="headerlink" title="博客更新操作"></a>博客更新操作</h2><p>后面就不用自己<code>hexo g -d</code> 了</p>
<p>在_posts目录下增加文章后</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m&quot;&quot;</span><br><span class="line">git push origin</span><br></pre></td></tr></table></figure>
<p>感觉可以写个脚本</p>
<p>命名为git_script.ps1  以后直接用powershell运行这个脚本就可以了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add .</span><br><span class="line">$date &#x3D; get-date -uformat &quot;%Y-%m-%d %H:%M:%S&quot;</span><br><span class="line">git commit -m&quot;new post $date&quot;</span><br><span class="line">git push origin</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.itfanr.cc/2017/08/09/using-travis-ci-automatic-deploy-hexo-blogs/">https://www.itfanr.cc/2017/08/09/using-travis-ci-automatic-deploy-hexo-blogs/</a></p>
<p><a href="http://www.dxjia.cn/2016/01/27/hexo-write-everywhere/">http://www.dxjia.cn/2016/01/27/hexo-write-everywhere/</a></p>
<p><a href="http://www.yanglangjing.com/2018/08/28/travis_ci_auto_deploy_hexo_to_vps/">http://www.yanglangjing.com/2018/08/28/travis_ci_auto_deploy_hexo_to_vps/</a></p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Travis-CI</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis源码阅读——SDS</title>
    <url>/Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94SDS/2019-03-22-Redis%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94SDS.html</url>
    <content><![CDATA[<h1 id="Redis源码阅读——SDS"><a href="#Redis源码阅读——SDS" class="headerlink" title="Redis源码阅读——SDS"></a>Redis源码阅读——SDS</h1><p>参考Redis设计与实现 以及网上博客阅读Redis源码。</p>
<a id="more"></a>
<p>SDS相关知识点见读书笔记。</p>
<h2 id="创建和销毁"><a href="#创建和销毁" class="headerlink" title="创建和销毁"></a>创建和销毁</h2><p>为了能够对sds进行相关API的测试，因此把sds模块单独提出来。阅读Redis的Makefile发现，编译sds模块需要的源文件包括<code>sds.c</code>, <code>sds.h</code> <code>zmalloc.c</code> </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">test-sds: sds.c sds.h</span><br><span class="line">        $(REDIS_CC) sds.c zmalloc.c -DSDS_TEST_MAIN $(FINAL_LIBS) -o &#x2F;tmp&#x2F;sds_test</span><br><span class="line">        &#x2F;tmp&#x2F;sds_test</span><br></pre></td></tr></table></figure>
<p>但是实际编译后会发现会报很多函数未定义的错。原因是redis源码里面sds的内存分配、释放、重分配这些函数是封装成zmalloc,zfee这些函数的，只单纯的把zmalloc.c提取出来是远远不够的。后面发现redis的作者已经把sds给单独提出来了。包括三个源文件<code>sds.c</code>,<code>sds.h</code>,<code>sdsalloc.h</code> 因此执行如下操作即可单独把redis的sds模块提取出来。</p>
<h3 id="提取sds模块"><a href="#提取sds模块" class="headerlink" title="提取sds模块"></a>提取sds模块</h3><ol>
<li><p>新建redis_sds测试目录</p>
<p>选择合适的目录下新建</p>
<p><code>mkdir redis_sds</code></p>
</li>
<li><p>复制源文件至redis_sds目录下</p>
<p>在redis源码的src目录下执行：</p>
<p><code>cp sds.c ~/redis_sds/</code></p>
<p><code>cp sds.h ~/redis_sds/</code></p>
<p><code>cp sdsalloc.h ~/redis_sds/</code></p>
</li>
<li><p>修改sdsalloc.h </p>
<p>复制过来的sdsalloc.h 将sds模块的内存函数封装为使用zmalloc函数。为了简化处理直接使用libc的malloc函数来进行内存管理，同时将zmalloc.h给注释掉。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//#include &quot;zmalloc.h&quot;</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> s_malloc malloc</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> s_realloc realloc</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> s_free free</span></span><br></pre></td></tr></table></figure></li>
<li><p>新建主函数</p>
<p>新建主函数sds_test.c</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;sds.c&quot;</span></span></span><br><span class="line"><span class="comment">//#include &quot;sds.h&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span> </span>&#123;</span><br><span class="line">    sds s = sdsnew(<span class="string">&quot;Hello World!&quot;</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Length:%d, Type:%d\n&quot;</span>, sdslen(s), sdsReqType(sdslen(s)));</span><br><span class="line"></span><br><span class="line">    s = sdscat(s, <span class="string">&quot;The length of this sentence is greater than 32 bytes&quot;</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Length:%d, Type:%d\n&quot;</span>, sdslen(s), sdsReqType(sdslen(s)));</span><br><span class="line"></span><br><span class="line">    sdsfree(s);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>直接include sds.c 即可，因为如果#include “sds.h” 的话，sdsReqType这个函数并没有在sds.h里面声明，而且因为sdsReqType的申明是：</p>
<p><code>static inline char sdsReqType(size_t string_size) &#123;</code>  有static限制所以不能在sds.h中先声明，所以为了简单就直接#include 了sds.c了</p>
</li>
<li><p>编译  </p>
<p>为了方便重复编译，所以写了个简单的Makefile。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">test : sds_test.c sds.h sds.c sdsalloc.h</span><br><span class="line">        gcc -o sdstest sds_test.c</span><br></pre></td></tr></table></figure>
<p>只需要编译sds_test.c 即可。因为sds_test.c 里面是直接#include sds.c 了所以再</p>
<p><code>gcc -o sdstest sds_test.c sds.c</code> 会将sds.c 里面的函数重复编译两次，造成Multiple definition 问题。</p>
<p>之后只需要执行make命令就可以生成可执行文件sdstest。</p>
<p>执行后输出为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;sdstest </span><br><span class="line">Length:12, Type:0</span><br><span class="line">Length:64, Type:1</span><br></pre></td></tr></table></figure>
<h3 id="sds的创建"><a href="#sds的创建" class="headerlink" title="sds的创建"></a>sds的创建</h3><p>通过<code>sdsnew</code> 来创建了一个sds。sdsnew源码为：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* Create a new sds string starting from a null terminated C string. */</span></span><br><span class="line"><span class="function">sds <span class="title">sdsnew</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *init)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//使用？条件判断符来简化if语句对NULL的判断，直接使用strlen来返回字符指针的长度。</span></span><br><span class="line">    <span class="keyword">size_t</span> initlen = (init == <span class="literal">NULL</span>) ? <span class="number">0</span> : <span class="built_in">strlen</span>(init); </span><br><span class="line">    <span class="keyword">return</span> sdsnewlen(init, initlen);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要注意的是字符数组和字符指针是有区别的：字符指针的数据是存放在进程的虚拟地址空间的程序代码和数据段，是只读的不能修改。字符数组存放的字符串数据是存放在用户栈的，是可以更改的。且字符指针的数据没有”\0”这个结束符。</p>
<p>参考博客讲的很好：<a href="https://blog.csdn.net/on_1y/article/details/13030439">https://blog.csdn.net/on_1y/article/details/13030439</a></p>
<p><code>sdsnew</code> 通过把字符串长度和字符串传递给<code>sdsnewlen</code>，来完成创建。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/* Create a new sds string with the content specified by the &#x27;init&#x27; pointer</span></span><br><span class="line"><span class="comment"> * and &#x27;initlen&#x27;.</span></span><br><span class="line"><span class="comment"> * If NULL is used for &#x27;init&#x27; the string is initialized with zero bytes.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The string is always null-termined (all the sds strings are, always) so</span></span><br><span class="line"><span class="comment"> * even if you create an sds string with:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * mystring = n(&quot;abc&quot;,3);</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * You can print the string with printf() as there is an implicit \0 at the</span></span><br><span class="line"><span class="comment"> * end of the string. However the string is binary safe and can contain</span></span><br><span class="line"><span class="comment"> * \0 characters in the middle, as the length is stored in the sds header. */</span></span><br><span class="line"><span class="function">sds <span class="title">sdsnewlen</span><span class="params">(<span class="keyword">const</span> <span class="keyword">void</span> *init, <span class="keyword">size_t</span> initlen)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">void</span> *sh;</span><br><span class="line">    sds s;</span><br><span class="line">    <span class="keyword">char</span> type = sdsReqType(initlen);  <span class="comment">//返回字符串对应的type</span></span><br><span class="line">    <span class="comment">/* Empty strings are usually created in order to append. Use type 8</span></span><br><span class="line"><span class="comment">     * since type 5 is not good at this. */</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">    空字符串使用sdshdr8来存储，而不是sdshdr5,（虽然长度小于32），因为sdshdr5不适合扩容。</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">if</span> (type == SDS_TYPE_5 &amp;&amp; initlen == <span class="number">0</span>) type = SDS_TYPE_8;</span><br><span class="line">    <span class="keyword">int</span> hdrlen = sdsHdrSize(type); <span class="comment">// 返回对应类型的sdsheader长度。</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span> *fp; <span class="comment">/* flags pointer. */</span></span><br><span class="line"></span><br><span class="line">    sh = s_malloc(hdrlen+initlen+<span class="number">1</span>); <span class="comment">// 申请头部+字符串+NULL的大小。(单位为byte)</span></span><br><span class="line">    <span class="keyword">if</span> (!init)</span><br><span class="line">        <span class="built_in">memset</span>(sh, <span class="number">0</span>, hdrlen+initlen+<span class="number">1</span>); <span class="comment">// 将sh后面对应大小的字节全部置为0；</span></span><br><span class="line">    <span class="keyword">if</span> (sh == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    s = (<span class="keyword">char</span>*)sh+hdrlen; <span class="comment">//s指针指向字符串的首字节。</span></span><br><span class="line">    fp = ((<span class="keyword">unsigned</span> <span class="keyword">char</span>*)s)<span class="number">-1</span>; <span class="comment">// fp指针指向flag</span></span><br><span class="line">    <span class="keyword">switch</span>(type) &#123;  <span class="comment">// 初始化sdshdr</span></span><br><span class="line">        <span class="keyword">case</span> SDS_TYPE_5: &#123;</span><br><span class="line">            *fp = type | (initlen &lt;&lt; SDS_TYPE_BITS);<span class="comment">// 设置flag这个字节的具体值</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">case</span> SDS_TYPE_8: &#123;</span><br><span class="line">            SDS_HDR_VAR(<span class="number">8</span>,s); <span class="comment">// 获取header指针sh</span></span><br><span class="line">            sh-&gt;len = initlen; <span class="comment">//header中len的初始</span></span><br><span class="line">            sh-&gt;alloc = initlen; <span class="comment">//header 中alloc的初试</span></span><br><span class="line">            *fp = type;  <span class="comment">//flag 的初始。</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">case</span> SDS_TYPE_16: &#123;</span><br><span class="line">            SDS_HDR_VAR(<span class="number">16</span>,s);</span><br><span class="line">            sh-&gt;len = initlen;</span><br><span class="line">            sh-&gt;alloc = initlen;</span><br><span class="line">            *fp = type; </span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">case</span> SDS_TYPE_32: &#123;</span><br><span class="line">            SDS_HDR_VAR(<span class="number">32</span>,s);</span><br><span class="line">            sh-&gt;len = initlen;</span><br><span class="line">            sh-&gt;alloc = initlen;</span><br><span class="line">            *fp = type;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">case</span> SDS_TYPE_64: &#123;</span><br><span class="line">            SDS_HDR_VAR(<span class="number">64</span>,s);</span><br><span class="line">            sh-&gt;len = initlen;</span><br><span class="line">            sh-&gt;alloc = initlen;</span><br><span class="line">            *fp = type;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (initlen &amp;&amp; init)</span><br><span class="line">        <span class="built_in">memcpy</span>(s, init, initlen); <span class="comment">// 将字符串拷贝到s(也就是buf数组)</span></span><br><span class="line">    s[initlen] = <span class="string">&#x27;\0&#x27;</span>; <span class="comment">//在字符串后面添加终止符</span></span><br><span class="line">    <span class="keyword">return</span> s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>char type = sdsReqType(initlen);</code> 获取sds类型，源码分析在读书笔记里面有记录。源码为</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">char</span> <span class="title">sdsReqType</span><span class="params">(<span class="keyword">size_t</span> string_size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (string_size &lt; <span class="number">1</span>&lt;&lt;<span class="number">5</span>) <span class="comment">// string_size &lt; 2^5</span></span><br><span class="line">        <span class="keyword">return</span> SDS_TYPE_5;</span><br><span class="line">    <span class="keyword">if</span> (string_size &lt; <span class="number">1</span>&lt;&lt;<span class="number">8</span>)  <span class="comment">//string_size &lt; 2^8</span></span><br><span class="line">        <span class="keyword">return</span> SDS_TYPE_8;</span><br><span class="line">    <span class="keyword">if</span> (string_size &lt; <span class="number">1</span>&lt;&lt;<span class="number">16</span>)	<span class="comment">//string_size &lt; 2^16</span></span><br><span class="line">        <span class="keyword">return</span> SDS_TYPE_16;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> (LONG_MAX == LLONG_MAX)</span></span><br><span class="line">    <span class="keyword">if</span> (string_size &lt; <span class="number">1ll</span>&lt;&lt;<span class="number">32</span>)  <span class="comment">//string_size &lt; 2^32</span></span><br><span class="line">        <span class="keyword">return</span> SDS_TYPE_32;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    <span class="keyword">return</span> SDS_TYPE_64; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>采用左移来计算对应多少位的范围，而不是用2^5 这样的乘法。<strong>直接移位比使用幂来计算快很多</strong>。</p>
<p><code>1&lt;&lt;5</code>   计算出来就是2^5 次方。1是int型，4byte32位。最低8bit位的二进制为：00000001 左移5位后变成了：00100000 对应的十进制既是32。</p>
<p>计算n个bit位的最大值：(1&lt;&lt;n) -1</p>
<p>但是需要注意位数不够的情况。因为1是int型，只有32个bit。所以在左移32个bit时，需要使用long long int型。用1ll来表示，此时1ll为64个bit。</p>
<p>还得考虑机器是否为64位机器，在32位机器上LONG_MAX = 2147483647L，64位机器上LONG_MAX = 9223372036854775807L 。不论32位机器还是64位机器上 LLONG_MAX 都是9223372036854775807L 。所以当LONG_MAX == LLONG_MAX 说明字长为64bit。加上条件编译，说明在32位机器上不使用sdshdr32而直接跳到了sdshdr64，仅仅在64位机器上使用sdshdr32。原因是什么？还没想通</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">sdsHdrSize</span><span class="params">(<span class="keyword">char</span> type)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span>(type&amp;SDS_TYPE_MASK) &#123;</span><br><span class="line">        <span class="keyword">case</span> SDS_TYPE_5:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">sizeof</span>(struct sdshdr5);</span><br><span class="line">        <span class="keyword">case</span> SDS_TYPE_8:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">sizeof</span>(struct sdshdr8);</span><br><span class="line">        <span class="keyword">case</span> SDS_TYPE_16:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">sizeof</span>(struct sdshdr16);</span><br><span class="line">        <span class="keyword">case</span> SDS_TYPE_32:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">sizeof</span>(struct sdshdr32);</span><br><span class="line">        <span class="keyword">case</span> SDS_TYPE_64:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">sizeof</span>(struct sdshdr64);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因为struct里面的buf数组是柔性数组，计算结构体的大小的时候不会计算在内。</p>
<p><code>memset(sh, 0, hdrlen+initlen+1);</code>  memset函数会将sh中当前位置后面的<code>hdrlen+initlen+1</code>个字节全部置于0。 注意sh指向的是<code>hdrlen+initlen+1</code> 个字节的首个字节。（sh指针存储的地址就是首个字节的地址。） </p>
<p>memset源码为：<a href="https://github.com/gcc-mirror/gcc/blob/master/libgcc/memset.c">https://github.com/gcc-mirror/gcc/blob/master/libgcc/memset.c</a></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* Public domain.  */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stddef.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">void</span> *</span><br><span class="line"><span class="built_in">memset</span> (<span class="keyword">void</span> *dest, <span class="keyword">int</span> val, <span class="keyword">size_t</span> len)</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">char</span> *ptr = dest; <span class="comment">// 用char来限定每次指针+1只移动一个字节。</span></span><br><span class="line">  <span class="keyword">while</span> (len-- &gt; <span class="number">0</span>)</span><br><span class="line">    *ptr++ = val;</span><br><span class="line">  <span class="keyword">return</span> dest;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>假设<code>hdrlen+initlen+1</code> 为8 ，经过memset后，从sh首字节开始共有8个字节都被置为0。</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1553525055/blog/memset.png" alt=""></p>
<p>指针的类型时用来确定指针需要从首地址寻址（偏移）多少个字节。比如int * 指针说明指针存储的地址朝后面偏移3个字节才是这个int类型的所有数据。即指针存储的地址时起点，而终点是由类型来确定的。此外，类型也是指针加减的步长，比如char类型的步长就是1byte，而uint_16类型的指针步长就是2byte。</p>
<p>随后用switch语句对不同类型的sdshdr设置初始值。</p>
<p>首先是sdshdr5</p>
<p> <code>*fp = type | (initlen &lt;&lt; SDS_TYPE_BITS)</code> 使用移位和或操作的方式来对8个bit位赋值。(不得不感慨这些操作真的是太巧妙了)</p>
<p>假设initlen为3。则initlen的二进制为0000 0011（应该是8byte（64位机器）或者4byte（32位机器），为了简单用1byte的二进制表示）而SDS_TYPE_BITS 为3。所以先将initlen 左移3个bit 变成0000 0001 1000(共有8byte或者4byte)。再与type进行或运算。type为0000 0000 进行或运算后，得到的内容是8bit的，因为type是char类型，即0001 1000 。</p>
<p>其他sdshdr类型的设置都差不多，详解下sdshdr8.</p>
<p><code>SDS_HDR_VAR(8,s)</code>  <code>SDS_HDR_VAR</code> 是个宏定义的函数</p>
<p><code>#define SDS_HDR_VAR(T,s) struct sdshdr##T *sh = (void*)((s)-(sizeof(struct sdshdr##T)));</code></p>
<p>采用宏定义函数的好处是</p>
<ol>
<li>能够减少额外的开销 因为如果写成普通函数的话，函数的调用会在用户栈开辟空间，形参压栈，返回时还需要释放栈，可想而知的开销。使用宏定义函数则在代码规模和速度方面都比函数更胜一筹。宏定义的本质就是替换，所以在使用宏定义函数的地方，执行的时候相当于是在直接执行<code>struct sdshdr##T *sh = (void*)((s)-(sizeof(struct sdshdr##T)))</code> 这句代码</li>
<li>函数的参数必须被声明为一种特定的类型，所以它只能在类型合适的表达式上使用。而宏定义则可以用于整形、长整形、单浮点型、双浮点型以及其他任何可以用“&gt;”操作符比较值大小的类型，也就是说，宏是与类型无关的。（有点C++模版类的感觉）</li>
</ol>
<p>宏定义函数中的## 是（token-pasting）<strong>符号连接操作符</strong> 直接将形参T链接到sdshdr上面。也就是sdshdrT。</p>
<p>所以这句代码也就很简单了，将字符串指针s向后移动header的大小，也就得到了header的指针。（不过有个疑问是为什么还要重新获取headr的地址，最开始不就是指向了header吗？，难道memset是直接对sh进行操作的？测试过了，memset不会修改sh的地址，所以应该是为了再次确保sh一定指向header）</p>
<p>解释一下：SDS_HDR_VAR 的作用是将sh的类型修改为结构体指针，因为之前sh 一直都是空指针，(虽然指针的指向地址是headr，但是没有限定它类型)不然后面没法用sh-&gt;len, sh-&gt;alloc 来访问对应的结构体成员。</p>
<p>最开始创建的时候alloc 和len是一样大的，没有分配多余空间）</p>
<p><code>memcpy(s, init, initlen);</code> 函数将init的前initlen个字符拷贝给s。</p>
<p>memcpy源码为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;* Public domain.  *&#x2F;</span><br><span class="line">#include &lt;stddef.h&gt;</span><br><span class="line"></span><br><span class="line">void *</span><br><span class="line">memcpy (void *dest, const void *src, size_t len)</span><br><span class="line">&#123;</span><br><span class="line">  char *d &#x3D; dest;</span><br><span class="line">  const char *s &#x3D; src;</span><br><span class="line">  while (len--)</span><br><span class="line">    *d++ &#x3D; *s++;</span><br><span class="line">  return dest;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>整个过程中的三个指针sh,s,fp对应关系如下图</p>
<p><img src="https://res.cloudinary.com/bravey/image/upload/v1553525055/blog/sds.png" alt=""></p>
</li>
</ol>
<h2 id="销毁"><a href="#销毁" class="headerlink" title="销毁"></a>销毁</h2><p>销毁使用sdsfree来实现</p>
<p>源码为：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* Free an sds string. No operation is performed if &#x27;s&#x27; is NULL. */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sdsfree</span><span class="params">(sds s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (s == <span class="literal">NULL</span>) <span class="keyword">return</span>;</span><br><span class="line">    s_free((<span class="keyword">char</span>*)s-sdsHdrSize(s[<span class="number">-1</span>]));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>s[-1],就是指针s向后移动移位，也就是flag的位置。将s移动到sh的位置，释放sh指针也就释放了整个sds内存。</p>
<p>疑惑：sh指针在sdsnewlen函数中是个局部变量，在sdsnewlen函数中是自动释放的，这里并没有传递sh指针为什么也可以释放对应的空间？</p>
<p>自己想了下：malloc 函数传递的参数是需要分配的内存大小(len)，返回的是指针也就是地址。free()函数只用将malloc函数返回的指针(地址)作为参数传入，就可以释放之前该地址分配到的内存空间。而地址只是首地址，总共的偏移量（大小），应该是由操作系统在内存分配的时候就记录了的。</p>
<p>博客中记录：申请的时候实际上占用的内存要比申请的大。因为超出的空间是用来记录对这块内存的管理信息。额外的空间用来记录管理信息——分配块的长度，指向下一个分配块的指针等等。果然malloc的时候用来一个struct来记录分配的信息。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">mem_control_block</span> &#123;</span> </span><br><span class="line">  <span class="keyword">int</span> is_available;    <span class="comment">//一般来说应该是一个可用空间的首地址，但这里英文单词却显示出空间是否可用的一个标记</span></span><br><span class="line">  <span class="keyword">int</span> size;            <span class="comment">//这是实际空间的大小 </span></span><br><span class="line">  &#125;;</span><br></pre></td></tr></table></figure>
<p><a href="http://www.cnblogs.com/hanyonglu/archive/2011/04/28/2031271.html">http://www.cnblogs.com/hanyonglu/archive/2011/04/28/2031271.html</a></p>
<p>free()就是根据这个结构体的信息来释放malloc()申请的空间</p>
<p>另外的疑惑：释放完空间后，s 指针不用把它指向null吗？  </p>
<p>暂时就只是创建和销毁的源码把，看了两天，阅读源码真的是酣畅淋漓，收获良多。学到了很多奇妙的C技巧，还对操作系统的知识有了更具象的理解。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>阅读sdsfromlonglong部分的源码：</p>
<p>sdsfromlonglong 函数用于将一个long long 类型的整形数字转换为字符数组。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* Create an sds string from a long long value. It is much faster than:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * sdscatprintf(sdsempty(),&quot;%lld\n&quot;, value);</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">sds <span class="title">sdsfromlonglong</span><span class="params">(<span class="keyword">long</span> <span class="keyword">long</span> value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">char</span> buf[SDS_LLSTR_SIZE];  <span class="comment">// 给buf数组分配最小的空间，21的长度</span></span><br><span class="line">    <span class="keyword">int</span> len = sdsll2str(buf,value); <span class="comment">//将long long 转为字符数组存储在buf中，并返回字符串的长度</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sdsnewlen(buf,len);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到主要的转换操作在sdsll2str这个函数中：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* Helper for sdscatlonglong() doing the actual number -&gt; string</span></span><br><span class="line"><span class="comment"> * conversion. &#x27;s&#x27; must point to a string with room for at least</span></span><br><span class="line"><span class="comment"> * SDS_LLSTR_SIZE bytes.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The function returns the length of the null-terminated string</span></span><br><span class="line"><span class="comment"> * representation stored at &#x27;s&#x27;. */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SDS_LLSTR_SIZE 21</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sdsll2str</span><span class="params">(<span class="keyword">char</span> *s, <span class="keyword">long</span> <span class="keyword">long</span> value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">char</span> *p, aux;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> v; </span><br><span class="line">    <span class="keyword">size_t</span> l;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Generate the string representation, this method produces</span></span><br><span class="line"><span class="comment">     * an reversed string. */</span></span><br><span class="line">    v = (value &lt; <span class="number">0</span>) ? -value : value;  <span class="comment">//判断是否为负数</span></span><br><span class="line">    p = s;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        *p++ = <span class="string">&#x27;0&#x27;</span>+(v%<span class="number">10</span>); <span class="comment">// 除以10取余数</span></span><br><span class="line">        v /= <span class="number">10</span>;     <span class="comment">//去除个位</span></span><br><span class="line">    &#125; <span class="keyword">while</span>(v);</span><br><span class="line">    <span class="keyword">if</span> (value &lt; <span class="number">0</span>) *p++ = <span class="string">&#x27;-&#x27;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Compute length and add null term. */</span></span><br><span class="line">    l = p-s;   <span class="comment">// 计算出字符串的长度 不含终止符</span></span><br><span class="line">    *p = <span class="string">&#x27;\0&#x27;</span>; <span class="comment">// 首地址填终止符。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Reverse the string. */</span></span><br><span class="line">    p--;</span><br><span class="line">    <span class="keyword">while</span>(s &lt; p) &#123;</span><br><span class="line">        aux = *s;</span><br><span class="line">        *s = *p;</span><br><span class="line">        *p = aux;</span><br><span class="line">        s++;</span><br><span class="line">        p--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">do</span> &#123;</span><br><span class="line">    *p++ = <span class="string">&#x27;0&#x27;</span>+(v%<span class="number">10</span>); <span class="comment">// 除以10取余数</span></span><br><span class="line">    v /= <span class="number">10</span>;     <span class="comment">//去除个位</span></span><br><span class="line">&#125; <span class="keyword">while</span>(v);</span><br></pre></td></tr></table></figure>
<p>假设v是352，变成字符串是将每一个对应的10进制上面的3,5,2这三个个位、十位、百位的数字给单独变成字符。</p>
<p><code>*p++ = &#39;0&#39;+(v%10);</code> p 指针是字符数组buf的首地址，而将整型变成字符型的操作就是与字符’0’ 相加，这样就可以对应的数字变成字符类型。同时p相应的加1来指向下一个byte用来存储下一个被转换的char。</p>
<p>char类型存储的是对应字符的ascii值，ASCII表为：<a href="https://baike.baidu.com/item/ASCII/309296">https://baike.baidu.com/item/ASCII/309296</a> ，所以字符的运算实际上是对应的ASCII的值的运算。<br>v%10是除以10取余数，352%10 =2； 35%10=5，所以也就是取得v值的10进制上面的个位数。<br>所以在while循环里面每次对v除以10并取余，就可以得到对应long long 型的字符串。<br>但是因为每次得到的字符都是最后面的个位数，所以352，所输出的字符串数组为：‘2’， ‘3’， ‘5’ 是一个倒序的，因此还需要再反转一次。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* Reverse the string. */</span> </span><br><span class="line">p--;</span><br><span class="line"><span class="keyword">while</span>(s &lt; p) &#123;</span><br><span class="line">    aux = *s;</span><br><span class="line">    *s = *p;</span><br><span class="line">    *p = aux;</span><br><span class="line">    s++;</span><br><span class="line">    p--;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>字符串反转，首尾各有一个指针，当首指针小于尾指针的时候，交换数字，并同时向中间移动</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://blog.csdn.net/yangbodong22011/article/details/78419966">https://blog.csdn.net/yangbodong22011/article/details/78419966</a></p>
]]></content>
      <categories>
        <category>源码阅读</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>内存分配的字节对齐</title>
    <url>/2019-03-22-%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%9A%84%E5%AD%97%E8%8A%82%E5%AF%B9%E9%BD%90.html</url>
    <content><![CDATA[<h1 id="C语言内存分配的字节对齐"><a href="#C语言内存分配的字节对齐" class="headerlink" title="C语言内存分配的字节对齐"></a>C语言内存分配的字节对齐</h1><p>阅读Redis源码时出现了<code>__attribute__ ((__packed__))</code> 语句作用是取消字节对齐，而使分配的内存连续。忘记了相关知识。记录下。</p>
<a id="more"></a>
<p>简单记录：？？？</p>
<p>对齐参数。参数满足两个特性： </p>
<p> 1.必须是2的幂</p>
<p> 2.必须是(void <em>)的整数倍  (void </em> 的大小就是字长，是CPU可以一次读取的字节数)</p>
<p>根据这个原理，在32位和64位的对齐单位分别为8字节和16字节</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.cnblogs.com/Creator/archive/2012/04/05/2433386.html">https://www.cnblogs.com/Creator/archive/2012/04/05/2433386.html</a></p>
]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>内存</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis设计与实现读书笔记——第二章SDS</title>
    <url>/2019-03-20-Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0.html</url>
    <content><![CDATA[<h1 id="Redis设计与实现读书笔记——第二章"><a href="#Redis设计与实现读书笔记——第二章" class="headerlink" title="Redis设计与实现读书笔记——第二章"></a>Redis设计与实现读书笔记——第二章</h1><p>为了做Redis相关实验，在网上粗略看了Redis设计与实现的电子版，感觉收获很多，但是因为是旧版，所以买了第二版，重读第二次。</p>
<a id="more"></a>
<h2 id="第二章-简单动态字符串"><a href="#第二章-简单动态字符串" class="headerlink" title="第二章 简单动态字符串"></a>第二章 简单动态字符串</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><ol>
<li>字符串值的键值对在底层都是由SDS实现的。</li>
<li>sds的功能：<ol>
<li>存储字符串值</li>
<li>用作缓冲区<ol>
<li>AOF模块缓冲区</li>
<li>客户端状态的输入缓冲区</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="2-1-SDS的定义"><a href="#2-1-SDS的定义" class="headerlink" title="2.1 SDS的定义"></a>2.1 SDS的定义</h3><p>文件：sds.h/sdshdr 结构体 </p>
<p>书中的为3.0版本，4.0版本有较大改动。</p>
<p>version: redis-4.02 </p>
<p>参考：<a href="https://www.cnblogs.com/chenpingzhao/p/7292182.html">https://www.cnblogs.com/chenpingzhao/p/7292182.html</a></p>
<p><a href="https://www.codesheep.cn/2018/08/09/Redis%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E5%86%85%E9%83%A8%E7%BC%96%E7%A0%81%E5%89%96%E6%9E%90/">https://www.codesheep.cn/2018/08/09/Redis%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B%E5%86%85%E9%83%A8%E7%BC%96%E7%A0%81%E5%89%96%E6%9E%90/</a></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">char</span> *sds;  <span class="comment">//注意，sds其实不是一个结构体类型，而是被typedef的char*</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Note: sdshdr5 is never used, we just access the flags byte directly.</span></span><br><span class="line"><span class="comment"> * However is here to document the layout of type 5 SDS strings. */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> __<span class="title">attribute__</span> ((__<span class="title">packed__</span>)) <span class="title">sdshdr5</span> &#123;</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span> flags; <span class="comment">/* 3 lsb of type, and 5 msb of string length */</span></span><br><span class="line">    <span class="keyword">char</span> buf[];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> __<span class="title">attribute__</span> ((__<span class="title">packed__</span>)) <span class="title">sdshdr8</span> &#123;</span></span><br><span class="line">    <span class="keyword">uint8_t</span> len; <span class="comment">/* used */</span></span><br><span class="line">    <span class="keyword">uint8_t</span> alloc; <span class="comment">/* excluding the header and null terminator */</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span> flags; <span class="comment">/* 3 lsb of type, 5 unused bits */</span></span><br><span class="line">    <span class="keyword">char</span> buf[];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> __<span class="title">attribute__</span> ((__<span class="title">packed__</span>)) <span class="title">sdshdr16</span> &#123;</span></span><br><span class="line">    <span class="keyword">uint16_t</span> len; <span class="comment">/* used */</span></span><br><span class="line">    <span class="keyword">uint16_t</span> alloc; <span class="comment">/* excluding the header and null terminator */</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span> flags; <span class="comment">/* 3 lsb of type, 5 unused bits */</span></span><br><span class="line">    <span class="keyword">char</span> buf[];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> __<span class="title">attribute__</span> ((__<span class="title">packed__</span>)) <span class="title">sdshdr32</span> &#123;</span></span><br><span class="line">    <span class="keyword">uint32_t</span> len; <span class="comment">/* used */</span></span><br><span class="line">    <span class="keyword">uint32_t</span> alloc; <span class="comment">/* excluding the header and null terminator */</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span> flags; <span class="comment">/* 3 lsb of type, 5 unused bits */</span></span><br><span class="line">    <span class="keyword">char</span> buf[];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> __<span class="title">attribute__</span> ((__<span class="title">packed__</span>)) <span class="title">sdshdr64</span> &#123;</span></span><br><span class="line">    <span class="keyword">uint64_t</span> len; <span class="comment">/* used */</span></span><br><span class="line">    <span class="keyword">uint64_t</span> alloc; <span class="comment">/* excluding the header and null terminator */</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">char</span> flags; <span class="comment">/* 3 lsb of type, 5 unused bits */</span></span><br><span class="line">    <span class="keyword">char</span> buf[];</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>除了结构体字段对len和alloc的数据类型的不同(<code>unit8, unit16， unit32, unit64</code>)，  其字段含义相差无几。其中header记录<code>len, alloc, flags</code> 信息。不同的header的目的是节省内存。<strong>header与buf数组在内存地址上前后相邻。</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+--------+-------------------------------+-----------+</span><br><span class="line">| Header | Binary safe C alike string... | Null term |</span><br><span class="line">+--------+-------------------------------+-----------+</span><br><span class="line">         |</span><br><span class="line">         &#96;-&gt; Pointer returned to the user.</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">len: 记录buf数组中已使用的字节数量 等于保存的字符串的长度 （不算结尾的\0 标识符）</span><br><span class="line">alloc: 字符串最大的容量。（除开header和最后的null终止符） </span><br><span class="line">flags: 总是会占用一个字节 8bit，加上unsigned是因为flags都是非负数 ，其中的最低3个bit用来表示header的类型还有 5个bit没有使用。</span><br><span class="line">buf: 字符数组，用于保存字符串。  柔性数组</span><br></pre></td></tr></table></figure>
<p>buf的大小=alloc+1；</p>
<p>header类型定义中，注意的地方：</p>
<ul>
<li><p>在各个header的定义中使用了<strong>attribute</strong> ((packed))，是为了让<strong>编译器以紧凑模式来分配内存</strong>，取消字节对齐。</p>
<ul>
<li>结构体的成员内存是’”连续”的，但是这个连续是以对齐的单位而言的。比如说A成员的内存是3个字节，假设对齐单位是4个字节，会给A成员多分配一个字节。A成员后面才又紧接B成员的内存。</li>
<li>如果没有这个属性，编译器可能会为struct的字段做优化对齐，在其中填充空字节。那样的话，就不能保证header和sds的数据部分紧紧前后相邻，也不能按照固定向低地址方向偏移1个字节的方式来获取flags字段了。</li>
</ul>
</li>
<li><p>在各个header的定义中最后有一个char buf[]。我们注意到这是一个没有指明长度的字符数组，这是C语言中定义字符数组的一种特殊写法，称为<strong>柔性数组</strong>（<a href="https://en.wikipedia.org/wiki/Flexible_array_member">flexible array member</a>），只能定义在一个结构体的最后一个字段上。它在这里只是起到一个标记的作用，表示在flags字段后面就是一个字符数组，或者说，它指明了紧跟在flags字段后面的这个字符数组在结构体中的偏移位置。而程序在为header分配的内存的时候，它并不占用内存空间。如果计算sizeof(struct sdshdr16)的值，那么结果是5个字节，其中没有buf字段。</p>
</li>
<li><p>sdshdr5与其它几个header结构不同，它不包含alloc字段，而长度使用flags的高5位来存储。因此，它不能为字符串分配空余空间。如果字符串需要动态增长，那么它就必然要重新分配内存才行。所以说，这种类型的sds字符串更适合存储静态的短字符串（长度小于32）。 因为长度的范围是5个bit来存储的</p>
<script type="math/tex; mode=display">
2^5-1 = 31</script></li>
</ul>
<p>sds字符串的header，其实隐藏在真正的字符串数据的前面（低地址方向）。这样的一个定义，有如下几个好处</p>
<ul>
<li>header和数据相邻，而不用分成两块内存空间来单独分配。这有利于减少内存碎片，提高存储效率（memory efficiency）。</li>
<li>虽然header有多个类型，但sds可以用统一的char *来表达。且它与传统的C语言字符串保持类型兼容。如果一个sds里面存储的是可打印字符串，那么我们可以直接把它传给C函数，比如使用strcmp比较字符串大小，或者使用printf进行打印。<h3 id="2-2-SDS与C字符串的区别"><a href="#2-2-SDS与C字符串的区别" class="headerlink" title="2.2 SDS与C字符串的区别"></a>2.2 SDS与C字符串的区别</h3></li>
</ul>
<p>c语言使用N+1长度的字符数组来表示长度为N的字符串，因为需要增加一个<code>\0</code> 字符终止</p>
<h4 id="2-2-1-常数复杂度获取字符串长度"><a href="#2-2-1-常数复杂度获取字符串长度" class="headerlink" title="2.2.1  常数复杂度获取字符串长度"></a>2.2.1  常数复杂度获取字符串长度</h4><p>因为c语言要知道字符串的长度只能遍历数组，所以复杂度为O(N)。 </p>
<p>而获取sds的字符串长度，只需要返回len的值就可以了复杂度为O(1)。这样对一个非常长的字符串键反复执行STRLEN命令，也不会对系统性能造成任何影响。</p>
<h4 id="2-2-2-杜绝缓冲区溢出"><a href="#2-2-2-杜绝缓冲区溢出" class="headerlink" title="2.2.2 杜绝缓冲区溢出"></a>2.2.2 杜绝缓冲区溢出</h4><p>C字符串不记录自身长度会带来易造成缓冲区溢出的问题。 比如使用strcat函数拼接两个字符串，被拼接的字符串要是没有提前分配空间，就会造成缓冲区溢出。（溢出的字节会导致这个字符串内存紧邻的其他字符串的内容被修改）</p>
<p>而SDS的空间分配策略完全杜绝了发生缓冲区溢出的可能，SDS的API需要修改SDS时，会先检查空间alloc是否满足修改所需的要求。不满足的话会先将空间扩展至修改所需的大小，再执行修改。</p>
<h4 id="2-2-3-减少修改字符串时带来的内存重分配次数"><a href="#2-2-3-减少修改字符串时带来的内存重分配次数" class="headerlink" title="2.2.3 减少修改字符串时带来的内存重分配次数"></a>2.2.3 减少修改字符串时带来的内存重分配次数</h4><p>C语言字符串用N+1个字节长的数组来保存N个字节的字符串，因为这个关联性所以每次每次增长或者缩短一个C字符串，都要对这个字符串进行一次内存重分配操作。</p>
<ul>
<li>执行增长操作 比如append，需要首先通过内存重分配来扩展底层数组的空间大小，否则产生缓冲区溢出</li>
<li>执行所动操作比如截断操作trime，需要首先通过内存重分配来释放字符串不再使用的空间，否则造成内存泄漏。</li>
</ul>
<p>内存重分配涉及复杂的算法，并且可能需要执行系统调用，所以通常是一个比较耗时的操作。这对Redis经常用于速度要求严苛，数据被频繁修改的场合来说，是不可接受的。</p>
<p>因此SDS通过未使用空间解除了字符串长度和底层数组长度之间的关联：buf的长度可以大于len的长度。 <strong>（4.0版本的源码还未找到对应的函数，所以可能和书上说的有变化了）</strong></p>
<ol>
<li><p>空间预分配 ——减少连续执行字符串增长操作所需的内存重分配次数。</p>
<ul>
<li><p>用于优化字符串<strong>增长</strong>操作。</p>
<p>当需要对SDS的空间进行空间扩展时，不仅会对SDS分配修改所必需的空间，还会额外分配未使用空间。</p>
<p>当len &lt; 1Mb时 alloc = 2*len; 当len &gt;= 1 mb时 alloc= len +1Mb。</p>
</li>
</ul>
<p>源码分析</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* Enlarge the free space at the end of the sds string so that the caller</span></span><br><span class="line"><span class="comment"> * is sure that after calling this function can overwrite up to addlen</span></span><br><span class="line"><span class="comment"> * bytes after the end of the string, plus one more byte for nul term.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Note: this does not change the *length* of the sds string as returned</span></span><br><span class="line"><span class="comment"> * by sdslen(), but only the free buffer space we have. */</span></span><br><span class="line"><span class="function">sds <span class="title">sdsMakeRoomFor</span><span class="params">(sds s, <span class="keyword">size_t</span> addlen)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">void</span> *sh, *newsh;</span><br><span class="line">    <span class="keyword">size_t</span> avail = sdsavail(s);</span><br><span class="line">    <span class="keyword">size_t</span> len, newlen;</span><br><span class="line">    <span class="keyword">char</span> type, oldtype = s[<span class="number">-1</span>] &amp; SDS_TYPE_MASK;</span><br><span class="line">    <span class="keyword">int</span> hdrlen;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Return ASAP if there is enough space left. */</span></span><br><span class="line">    <span class="keyword">if</span> (avail &gt;= addlen) <span class="keyword">return</span> s;</span><br><span class="line"></span><br><span class="line">    len = sdslen(s);</span><br><span class="line">    sh = (<span class="keyword">char</span>*)s-sdsHdrSize(oldtype);</span><br><span class="line">    newlen = (len+addlen);  <span class="comment">// 预分配</span></span><br><span class="line">    <span class="keyword">if</span> (newlen &lt; SDS_MAX_PREALLOC)</span><br><span class="line">        newlen *= <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        newlen += SDS_MAX_PREALLOC;</span><br><span class="line"></span><br><span class="line">    type = sdsReqType(newlen);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Don&#x27;t use type 5: the user is appending to the string and type 5 is</span></span><br><span class="line"><span class="comment">     * not able to remember empty space, so sdsMakeRoomFor() must be called</span></span><br><span class="line"><span class="comment">     * at every appending operation. */</span></span><br><span class="line">    <span class="keyword">if</span> (type == SDS_TYPE_5) type = SDS_TYPE_8;</span><br><span class="line"></span><br><span class="line">    hdrlen = sdsHdrSize(type);</span><br><span class="line">    <span class="keyword">if</span> (oldtype==type) &#123;</span><br><span class="line">        newsh = s_realloc(sh, hdrlen+newlen+<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span> (newsh == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">        s = (<span class="keyword">char</span>*)newsh+hdrlen;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">/* Since the header size changes, need to move the string forward,</span></span><br><span class="line"><span class="comment">         * and can&#x27;t use realloc */</span></span><br><span class="line">        newsh = s_malloc(hdrlen+newlen+<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span> (newsh == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">        <span class="built_in">memcpy</span>((<span class="keyword">char</span>*)newsh+hdrlen, s, len+<span class="number">1</span>);</span><br><span class="line">        s_free(sh);</span><br><span class="line">        s = (<span class="keyword">char</span>*)newsh+hdrlen;</span><br><span class="line">        s[<span class="number">-1</span>] = type;</span><br><span class="line">        sdssetlen(s, len);</span><br><span class="line">    &#125;</span><br><span class="line">    sdssetalloc(s, newlen);</span><br><span class="line">    <span class="keyword">return</span> s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p>惰性空间释放</p>
<ul>
<li><p>用于优化SDS的字符串<strong>缩短</strong>操作</p>
<p>缩短SDS保存的字符串时，并不立即使用内存重分配来回收缩短后多出来的字节，而是使用free属性，将这些字节的数量记录起来，并等待将来使用。</p>
</li>
</ul>
</li>
</ol>
<h4 id="2-2-4-二进制安全"><a href="#2-2-4-二进制安全" class="headerlink" title="2.2.4 二进制安全"></a>2.2.4 二进制安全</h4><p>C字符串中的字符必须符合某种编码（如ASCII），除了末尾字符串中间不能有<code>\0</code> 这个空字符，否则最先被程序读取的空字符将被认为是结尾，导致C字符串只能保存文本数据，而不能保存图片、音频、视频、压缩文件这样的二进制数据。</p>
<p>所谓二进制安全：以处理二进制的方式来处理SDS存放在buf数组里的数据，程序不会对其中的数据做任何限制、过滤、或者假设，数据在写入时是什么样的，被读取是就是什么样的。因为SDS使用len来判断字符串是否结束。</p>
<p>所以buf是字节数组，而不是字符数组。</p>
<h4 id="2-2-5-兼容部分C字符串函数"><a href="#2-2-5-兼容部分C字符串函数" class="headerlink" title="2.2.5 兼容部分C字符串函数"></a>2.2.5 兼容部分C字符串函数</h4><p>因为遵循C字符串以<code>\0</code>结尾的惯例，所以可以兼容<code>&lt;string.h&gt;/strcasecmp</code> ,<code>&lt;stdio.h&gt;/printf</code> 这些函数。但是是否是书上的使用结构体指针还是博客说的可以直接使用sds来调用？还需验证。</p>
<p>书：<code>printf(&quot;%s&quot;, sds-&gt;buf)</code>  sds是指向结构体的指针。</p>
<p>博客：<a href="https://blog.csdn.net/yangbodong22011/article/details/78419966">https://blog.csdn.net/yangbodong22011/article/details/78419966</a>   :<code>printf(%s, sds)</code></p>
<p>源码中是直接使用sds</p>
<h4 id="2-2-6-总结"><a href="#2-2-6-总结" class="headerlink" title="2.2.6 总结"></a>2.2.6 总结</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">C字符串</th>
<th style="text-align:center">SDS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">获取字符串长度复杂度为O(N)</td>
<td style="text-align:center">获取字符串长度复杂度为O(1)</td>
</tr>
<tr>
<td style="text-align:center">API不安全，可能造成缓冲区溢出</td>
<td style="text-align:center">API安全，不会造成缓冲区溢出</td>
</tr>
<tr>
<td style="text-align:center">修改字符串长度N次必然执行N次内存重分配</td>
<td style="text-align:center">最多执行N次内存重分配</td>
</tr>
<tr>
<td style="text-align:center">只能保存文本数据</td>
<td style="text-align:center">二进制安全文本与二进制数据皆可</td>
</tr>
<tr>
<td style="text-align:center">可使用<code>&lt;string.h&gt;</code>库中所有函数</td>
<td style="text-align:center">部分使用<code>&lt;string.h&gt;</code>库中函数</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-3-SDSAPI"><a href="#2-3-SDSAPI" class="headerlink" title="2.3 SDSAPI"></a>2.3 SDSAPI</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">sdslen(const sds s)</td>
<td style="text-align:center">获取sds字符串长度 O（1）</td>
</tr>
<tr>
<td style="text-align:center">sdssetlen(sds s, size_t newlen)</td>
<td style="text-align:center">设置sds字符串长度</td>
</tr>
<tr>
<td style="text-align:center">sdsinclen(sds s, size_t inc)</td>
<td style="text-align:center">增加sds字符串长度</td>
</tr>
<tr>
<td style="text-align:center">sdsalloc(const sds s)</td>
<td style="text-align:center">获取sds字符串容量</td>
</tr>
<tr>
<td style="text-align:center">sdssetalloc(sds s, size_t newlen)</td>
<td style="text-align:center">设置sds字符串容量。</td>
</tr>
<tr>
<td style="text-align:center">sdsavail(const sds s)</td>
<td style="text-align:center">获取sds字符串空余空间（即alloc - len）</td>
</tr>
<tr>
<td style="text-align:center">sdsHdrSize(char type)</td>
<td style="text-align:center">根据header类型得到header大小</td>
</tr>
<tr>
<td style="text-align:center">sdsReqType(size_t string_size)</td>
<td style="text-align:center">根据字符串数据长度计算所需要的header类型。</td>
</tr>
</tbody>
</table>
</div>
<h4 id="sdsReqType函数源码分析"><a href="#sdsReqType函数源码分析" class="headerlink" title="sdsReqType函数源码分析"></a>sdsReqType函数源码分析</h4><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">char</span> <span class="title">sdsReqType</span><span class="params">(<span class="keyword">size_t</span> string_size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (string_size &lt; <span class="number">1</span>&lt;&lt;<span class="number">5</span>) <span class="comment">// string_size &lt; 2^5</span></span><br><span class="line">        <span class="keyword">return</span> SDS_TYPE_5;</span><br><span class="line">    <span class="keyword">if</span> (string_size &lt; <span class="number">1</span>&lt;&lt;<span class="number">8</span>)  <span class="comment">//string_size &lt; 2^8</span></span><br><span class="line">        <span class="keyword">return</span> SDS_TYPE_8;</span><br><span class="line">    <span class="keyword">if</span> (string_size &lt; <span class="number">1</span>&lt;&lt;<span class="number">16</span>)	<span class="comment">//string_size &lt; 2^16</span></span><br><span class="line">        <span class="keyword">return</span> SDS_TYPE_16;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> (LONG_MAX == LLONG_MAX)</span></span><br><span class="line">    <span class="keyword">if</span> (string_size &lt; <span class="number">1ll</span>&lt;&lt;<span class="number">32</span>)  <span class="comment">//string_size &lt; 2^32</span></span><br><span class="line">        <span class="keyword">return</span> SDS_TYPE_32;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    <span class="keyword">return</span> SDS_TYPE_64; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>采用左移来计算对应多少位的范围，而不是用2^5 这样的乘法。<strong>直接移位比使用幂来计算快很多</strong>。</p>
<p><code>1&lt;&lt;5</code>   计算出来就是2^5 次方。1是int型，4byte32位。最低8bit位的二进制为：00000001 左移5位后变成了：00100000 对应的十进制既是32。</p>
<p>计算n个bit位的最大值：(1&lt;&lt;n) -1</p>
<p>但是需要注意位数不够的情况。因为1是int型，只有32个bit。所以在左移32个bit时，需要使用long long int型。用1ll来表示，此时1ll为64个bit。</p>
<p>还得考虑机器是否为64位机器，在32位机器上LONG_MAX = 2147483647L，64位机器上LONG_MAX = 9223372036854775807L 。不论32位机器还是64位机器上 LLONG_MAX 都是9223372036854775807L 。所以当LONG_MAX == LLONG_MAX 说明字长为64bit。加上条件编译，说明在32位机器上不使用sdshdr32而直接跳到了sdshdr64，仅仅在64位机器上使用sdshdr32。原因是什么？还没想通</p>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ol>
<li>为什么Redis需要自己实现字符串功能，而不直接使用c语言的传统字符串？<ul>
<li>见第二节。</li>
</ul>
</li>
<li>执行SET 与GET命令的过程。</li>
<li><code>char buf[]</code> 为什么没有指定大小？一个数组占用的内存大小<ul>
<li>在各个header的定义中最后有一个char buf[]。我们注意到这是一个没有指明长度的字符数组，这是C语言中定义字符数组的一种特殊写法，称为柔性数组（<a href="https://en.wikipedia.org/wiki/Flexible_array_member">flexible array member</a>），只能定义在一个结构体的最后一个字段上。它在这里只是起到一个标记的作用，表示在flags字段后面就是一个字符数组，或者说，它指明了紧跟在flags字段后面的这个字符数组在结构体中的偏移位置。而程序在为header分配的内存的时候，它并不占用内存空间。如果计算sizeof(struct sdshdr16)的值，那么结果是5个字节，其中没有buf字段。</li>
<li>数组内存大小为分配的的长度*数组类型的内存大小</li>
</ul>
</li>
<li>为什么redis 在32位机器上不使用sdshdr32？ </li>
</ol>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>ping 无法连接外网</title>
    <url>/ping%20%E9%97%AE%E9%A2%98.html</url>
    <content><![CDATA[<h2 id="ping-无法连接外网"><a href="#ping-无法连接外网" class="headerlink" title="ping 无法连接外网"></a>ping 无法连接外网</h2><a id="more"></a>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>ping外网ping不通</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yky@hw076:~&#x2F;tmux&gt; ping www.baidu.com</span><br><span class="line">ping: unknown host www.baidu.com</span><br><span class="line">yky@hw076:~&#x2F;tmux&gt; ping 8.8.8.8</span><br><span class="line">connect: Network is unreachable</span><br></pre></td></tr></table></figure>
<p>ping内网可以ping通</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hw076:~ # ping 172.18.11.114</span><br><span class="line">PING 172.18.11.114 (172.18.11.114) 56(84) bytes of data.</span><br><span class="line">64 bytes from 172.18.11.114: icmp_seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.193 ms</span><br><span class="line">64 bytes from 172.18.11.114: icmp_seq&#x3D;2 ttl&#x3D;64 time&#x3D;0.216 ms</span><br><span class="line">64 bytes from 172.18.11.114: icmp_seq&#x3D;3 ttl&#x3D;64 time&#x3D;0.207 ms</span><br><span class="line">64 bytes from 172.18.11.114: icmp_seq&#x3D;4 ttl&#x3D;64 time&#x3D;0.200 ms</span><br><span class="line">^C</span><br><span class="line">--- 172.18.11.114 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 2999ms</span><br><span class="line">rtt min&#x2F;avg&#x2F;max&#x2F;mdev &#x3D; 0.193&#x2F;0.204&#x2F;0.216&#x2F;0.008 ms</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>ifconfig信息为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hw076:~ # ifconfig </span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 90:E2:BA:15:C9:C4  </span><br><span class="line">          inet addr:172.18.11.76  Bcast:192.168.1.255  Mask:255.255.0.0</span><br><span class="line">          inet6 addr: fe80::92e2:baff:fe15:c9c4&#x2F;64 Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:9725797 errors:0 dropped:506 overruns:0 frame:0</span><br><span class="line">          TX packets:21023 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:1000 </span><br><span class="line">          RX bytes:598731249 (570.9 Mb)  TX bytes:2767270 (2.6 Mb)</span><br><span class="line">          Memory:fb480000-fb500000 </span><br><span class="line"></span><br><span class="line">lo        Link encap:Local Loopback  </span><br><span class="line">          inet addr:127.0.0.1  Mask:255.0.0.0</span><br><span class="line">          inet6 addr: ::1&#x2F;128 Scope:Host</span><br><span class="line">          UP LOOPBACK RUNNING  MTU:65536  Metric:1</span><br><span class="line">          RX packets:276 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:276 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0 </span><br><span class="line">          RX bytes:25088 (24.5 Kb)  TX bytes:25088 (24.5 Kb)</span><br></pre></td></tr></table></figure>
<p>route显示路由信息如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hw076:&#x2F;etc&#x2F;netconfig.d # route</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">default         *               0.0.0.0         UG    0      0        0 eth0</span><br><span class="line">loopback        *               255.0.0.0       U     0      0        0 lo</span><br><span class="line">link-local      *               255.255.0.0     U     0      0        0 eth0</span><br><span class="line">172.18.0.0      *               255.255.0.0     U     0      0        0 eth0</span><br></pre></td></tr></table></figure>
<p>原因是route没有配置网关，gateway是空着的。</p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>通过查看其他可以正常访问的节点的路由信息，得知网关节点为：172.18.0.254。因此增加默认网关节点配置。</p>
<p>执行命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">route add default  gw 172.18.0.254</span><br></pre></td></tr></table></figure>
<p>再次查看路由信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hw076:~ # route</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">default         172.18.0.254    0.0.0.0         UG    0      0        0 eth0</span><br><span class="line">loopback        *               255.0.0.0       U     0      0        0 lo</span><br><span class="line">link-local      *               255.255.0.0     U     0      0        0 eth0</span><br><span class="line">172.18.0.0      *               255.255.0.0     U     0      0        0 eth0</span><br></pre></td></tr></table></figure>
<p>再次ping8.8.8.8显示正常，问题解决。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>运维</tag>
        <tag>网络问题</tag>
      </tags>
  </entry>
  <entry>
    <title>shell 脚本遍历redis数据库</title>
    <url>/%E4%BD%BF%E7%94%A8shell%E8%84%9A%E6%9C%AC%E9%81%8D%E5%8E%86redis%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84%E6%89%80%E6%9C%89kv%E5%AF%B9.html</url>
    <content><![CDATA[<h2 id="使用shell脚本遍历redis数据库中的所有kv对"><a href="#使用shell脚本遍历redis数据库中的所有kv对" class="headerlink" title="使用shell脚本遍历redis数据库中的所有kv对"></a>使用shell脚本遍历redis数据库中的所有kv对</h2><p>记录下如何使用shell通过redis-cli 命令来操作redis数据库，因为直接在命令行中输入</p>
<p><code>redis-cli command</code> 的话command必须是单个单词，不能像是<code>KEYS *</code> 这种.</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">filename&#x3D;&#39;redis&#39;&#96;date +%Y-%m-%d_%H:%M&#96;</span><br><span class="line">work_path&#x3D;$(dirname &quot;$0&quot;) </span><br><span class="line">echo &quot;实例化redis数据文件为:$work_path&#x2F;$filename&quot;</span><br><span class="line">echo &quot;keys *&quot; | redis-cli &gt; key_db.txt</span><br><span class="line">echo &quot;将所有key保存到:$work_path&#x2F;key_db.txt&quot;</span><br><span class="line">for line in &#96;cat key_db.txt&#96;</span><br><span class="line">do</span><br><span class="line">        echo &quot;key:$line &quot; &gt;&gt;$work_path&#x2F;$filename.txt</span><br><span class="line">        echo &quot;key-value:&quot; &gt;&gt;$work_path&#x2F;$filename.txt</span><br><span class="line">        echo &quot;hgetall $line&quot; | redis-cli &gt;&gt;$work_path&#x2F;$filename.txt</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<p>使用echo 来把命令输出到管道然后再传递给redis-cli。在循环里面也是使用echo来把字符串输入到文件中。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>使用YCSB 评测redis性能</title>
    <url>/%E4%BD%BF%E7%94%A8YCSB%20%E8%AF%84%E6%B5%8Bredis%E6%80%A7%E8%83%BD.html</url>
    <content><![CDATA[<h1 id="使用YCSB-评测redis性能"><a href="#使用YCSB-评测redis性能" class="headerlink" title="使用YCSB 评测redis性能"></a>使用YCSB 评测redis性能</h1><p>YCSB是雅虎推出的可以评测许多主流数据库性能的基准测试，其中包括Redis。</p>
<a id="more"></a>
<h2 id="安装YCSB"><a href="#安装YCSB" class="headerlink" title="安装YCSB"></a>安装YCSB</h2><ol>
<li><p>安装java和maven</p>
<ol>
<li><p>机子已经有了java，所以只用安装maven Ubuntu安装命令为：</p>
<p><code>sudo apt-get install maven</code></p>
</li>
</ol>
</li>
<li><p>安装YCSB </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone http:&#x2F;&#x2F;github.com&#x2F;brianfrankcooper&#x2F;YCSB.git</span><br><span class="line">cd YCSB</span><br><span class="line">mvn -pl com.yahoo.ycsb:redis-binding -am clean package</span><br></pre></td></tr></table></figure>
<p>必须是gitclone的源码包才能执行mvn 命令。wget或者curl下来包是已经编译好了的无需执行mvn命令。</p>
</li>
<li><p><code>mvn -pl com.yahoo.ycsb:redis-binding -am clean package</code> 报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[INFO] Scanning for projects...</span><br><span class="line">[ERROR] [ERROR] Could not find the selected project in the reactor: com.yahoo.ycsb:redis-binding @ </span><br><span class="line">[ERROR] Could not find the selected project in the reactor: com.yahoo.ycsb:redis-binding -&gt; [Help 1]</span><br><span class="line">[ERROR] </span><br><span class="line">[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.</span><br><span class="line">[ERROR] Re-run Maven using the -X switch to enable full debug logging.</span><br><span class="line">[ERROR] </span><br><span class="line">[ERROR] For more information about the errors and possible solutions, please read the following articles:</span><br><span class="line">[ERROR] [Help 1] http:&#x2F;&#x2F;cwiki.apache.org&#x2F;confluence&#x2F;display&#x2F;MAVEN&#x2F;MavenExecutionException</span><br></pre></td></tr></table></figure>
<p>原因：此命令是在gitclone后未编译的时候使用的。而我之前是下载的编译好的tar.gz包，解压后是已经编译好了的。所以再次执行编译的命令时会报错。</p>
</li>
</ol>
<h2 id="使用YCSB"><a href="#使用YCSB" class="headerlink" title="使用YCSB"></a>使用YCSB</h2><p>将redis-server启动后开始使用YCSB</p>
<h3 id="设置数据库"><a href="#设置数据库" class="headerlink" title="设置数据库"></a>设置数据库</h3><p>需要先创建<code>usertable</code>的表，因为YCSB客户端默认是对<code>usertable</code> 进行操作。Redis将数据存储在内存中，不需要相关操作。</p>
<h3 id="选择合适的DB-interface"><a href="#选择合适的DB-interface" class="headerlink" title="选择合适的DB interface"></a>选择合适的DB interface</h3><p>YCSB的操作是通过DB interface来实现的。最基本的DB interface是<code>com.yahoo.ycsb.BasicDB</code>，会将输出输出到<code>System.out</code>里。可以通过继承DB interface来自定义DB interface，也可以使用原有的DB interface。Redis不需要此步操作。</p>
<h3 id="选择合适的负载"><a href="#选择合适的负载" class="headerlink" title="选择合适的负载"></a>选择合适的负载</h3><p>YCSB提供了6种负载，负载在worloads目录下。详情见<a href="https://github.com/brianfrankcooper/YCSB/wiki/Core-Workloads">https://github.com/brianfrankcooper/YCSB/wiki/Core-Workloads</a></p>
<ol>
<li><strong>Workload A: Update heavy workload</strong> 读写比例为： 50/50 混合负载 </li>
<li><strong>Workload A: Update heavy workload</strong> 读写比例为：95/5  读为主的负载</li>
<li><strong>Workload C: Read only</strong>  100% 的读  只读负载</li>
<li><strong>Workload D: Read latest workload</strong>  读取最近的数据负载</li>
<li><strong>Workload E: Short ranges</strong>  小范围的查询负载</li>
<li><strong>Workload F: Read-modify-write</strong> 读修改写负载</li>
</ol>
<p>自定义负载：参考<a href="https://github.com/brianfrankcooper/YCSB/wiki/Implementing-New-Workloads">https://github.com/brianfrankcooper/YCSB/wiki/Implementing-New-Workloads</a></p>
<p>可以通过修改参数文件或者新建java类来实现</p>
<p>需要注意的是YCSB的读写负载是针对哈希类型的数据而不是简单的字符串</p>
<h3 id="指定需要的运行参数"><a href="#指定需要的运行参数" class="headerlink" title="指定需要的运行参数"></a>指定需要的运行参数</h3><p>主要是指定redis的ip ，端口，密码等。</p>
<p>命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;ycsb load redis -s -P workloads&#x2F;workloada -p &quot;redis.host&#x3D;127.0.0.1&quot; -p &quot;redis.port&#x3D;6379&quot; &gt; outputLoad.txt</span><br></pre></td></tr></table></figure>
<p><code>-s</code> : <strong>status</strong>.十秒打印一次状态</p>
<h3 id="加载负载"><a href="#加载负载" class="headerlink" title="加载负载"></a>加载负载</h3><p>命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;ycsb load redis -s -P workloads&#x2F;workloada &gt; outputLoad.txt</span><br></pre></td></tr></table></figure>
<h3 id="运行负载"><a href="#运行负载" class="headerlink" title="运行负载"></a>运行负载</h3><p>命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;ycsb run redis -s -P workloads&#x2F;workloada &gt; outputRun.txt</span><br></pre></td></tr></table></figure>
<p>可以使用basic数据库来打印YCSB向数据库中写入的具体数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;ycsb.sh load basic -P workloads&#x2F;workloada</span><br><span class="line">bin&#x2F;ycsb.sh run basic -P workloads&#x2F;workloada</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://datawine.github.io/2018/12/11/YCSB%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/">https://datawine.github.io/2018/12/11/YCSB%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/</a></p>
<p><a href="https://github.com/brianfrankcooper/YCSB/tree/master/redis">https://github.com/brianfrankcooper/YCSB/tree/master/redis</a>  </p>
]]></content>
      <tags>
        <tag>redis</tag>
        <tag>benchmark</tag>
      </tags>
  </entry>
  <entry>
    <title>apt-get install失败</title>
    <url>/apt-get%20install%20%E5%A4%B1%E8%B4%A5.html</url>
    <content><![CDATA[<h2 id="apt-get-install失败"><a href="#apt-get-install失败" class="headerlink" title="apt-get install失败"></a>apt-get install失败</h2><a id="more"></a>
<h3 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h3><ol>
<li><p>使用perf 报错 内核无法找到perf</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@hw103:&#x2F;home&#x2F;yky&#x2F;redis-5.0.3# perf </span><br><span class="line">WARNING: perf not found for kernel 4.15.0-45</span><br><span class="line"></span><br><span class="line">  You may need to install the following packages for this specific kernel:</span><br><span class="line">    linux-tools-4.15.0-45-generic</span><br><span class="line">    linux-cloud-tools-4.15.0-45-generic</span><br><span class="line"></span><br><span class="line">  You may also want to install one of the following packages to keep up to date:</span><br><span class="line">    linux-tools-generic</span><br><span class="line">    linux-cloud-tools-generic</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p>安装此内核的通用工具时错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@hw103:&#x2F;home&#x2F;yky&#x2F;redis-5.0.3# apt-get install  linux-tools-4.15.0-45-generic</span><br><span class="line">Reading package lists... Done</span><br><span class="line">Building dependency tree       </span><br><span class="line">Reading state information... Done</span><br><span class="line">You might want to run &#39;apt-get -f install&#39; to correct these:</span><br><span class="line">The following packages have unmet dependencies:</span><br><span class="line"> console-setup : Depends: keyboard-configuration (&#x3D; 1.178ubuntu2.7) but 1.108ubuntu15.3 is to be installed</span><br><span class="line"> console-setup-linux : Depends: keyboard-configuration (&#x3D; 1.178ubuntu2.7) but 1.108ubuntu15.3 is to be installed</span><br><span class="line">                       Breaks: keyboard-configuration (&lt; 1.138) but 1.108ubuntu15.3 is to be installed</span><br><span class="line"> linux-tools-4.15.0-45-generic : Depends: linux-tools-4.15.0-45 but it is not going to be installed</span><br><span class="line">E: Unmet dependencies. Try &#39;apt-get -f install&#39; with no packages (or specify a solution).</span><br></pre></td></tr></table></figure></li>
<li><p>使用apt-get -f install 时报错</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">update-rc.d: error: insserv rejected the script header</span><br><span class="line">dpkg: error processing archive &#x2F;var&#x2F;cache&#x2F;apt&#x2F;archives&#x2F;keyboard-configuration_1.178ubuntu2.7_all.deb (--unpack):</span><br><span class="line"> subprocess new pre-installation script returned error exit status 1</span><br><span class="line">dpkg-query: warning: files list file for package &#39;keyboard-configuration&#39; missing; assuming package has no files currently installed</span><br><span class="line">dpkg-query: warning: files list file for package &#39;keyboard-configuration&#39; missing; assuming package has no files currently installed</span><br><span class="line">dpkg-query: warning: files list file for package &#39;keyboard-configuration&#39; missing; assuming package has no files currently installed</span><br><span class="line">Errors were encountered while processing:</span><br><span class="line"> &#x2F;var&#x2F;cache&#x2F;apt&#x2F;archives&#x2F;keyboard-configuration_1.178ubuntu2.7_all.deb</span><br><span class="line">E: Sub-process &#x2F;usr&#x2F;bin&#x2F;dpkg returned an error code (1)</span><br></pre></td></tr></table></figure>
<p>问题综述：</p>
<ol>
<li><code>apt-get install lib</code>时报错 Unmet dependencies</li>
<li><code>apt-get install -f</code> 时报错Sub-process /usr/bin/dpkg returned an error code (1)</li>
</ol>
</li>
</ol>
<ol>
<li><p>第一阶段解决办法</p>
<p>在/var/lib/dpkg/目录下有个info文件 ，然后文件中没有keyboard-configuration的相关文件但是有info的备份info_backup  ，这里面有相关的文件，于是将keyboard-configuration的所有相关文件都拷贝到了/var/lib/dpkg/info 中。</p>
<p>在info_backup目录下执行如下命令拷贝</p>
<p><code>cp keyboard-configuration.* ../info</code></p>
<p>随后再次执行安装内核通用工具 报错为第二阶段</p>
</li>
</ol>
<h3 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h3><ol>
<li><p>安装此内核的通用工具时时报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">insserv: Starting redis depends on plymouth and therefore on system facility &#96;$all&#39; which can not be true!</span><br><span class="line">insserv: exiting now without changing boot order!</span><br><span class="line">update-rc.d: error: insserv rejected the script header</span><br><span class="line">dpkg: error processing package avahi-daemon (--configure):</span><br><span class="line"> subprocess installed post-installation script returned error exit status 1</span><br><span class="line">No apport report written because MaxReports is reached already</span><br><span class="line">                                                              No apport report written because MaxReports is reached already</span><br><span class="line">                                                                                                                            dpkg: dependency problems prevent configuration o</span><br><span class="line">f avahi-utils: avahi-utils depends on avahi-daemon; however:</span><br><span class="line">  Package avahi-daemon is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package avahi-utils (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">Setting up unattended-upgrades (1.1ubuntu1.18.04.9) ...</span><br><span class="line">dpkg: error processing package unattended-upgrades (--configure):</span><br><span class="line"> subprocess installed post-installation script returned error exit status 10</span><br><span class="line">No apport report written because MaxReports is reached already</span><br><span class="line">                                                              Setting up linux-tools-4.15.0-45 (4.15.0-45.48) ...</span><br><span class="line">Setting up linux-tools-4.15.0-45-generic (4.15.0-45.48) ...</span><br><span class="line">Processing triggers for initramfs-tools (0.122ubuntu8.14) ...</span><br><span class="line">Errors were encountered while processing:</span><br><span class="line"> udev</span><br><span class="line"> snapd</span><br><span class="line"> ubuntu-core-launcher</span><br><span class="line"> kmod</span><br><span class="line"> ubuntu-drivers-common</span><br><span class="line"> whoopsie</span><br><span class="line"> openssh-server</span><br><span class="line"> ssh</span><br><span class="line"> avahi-daemon</span><br><span class="line"> avahi-utils</span><br><span class="line"> unattended-upgrades</span><br><span class="line">E: Sub-process &#x2F;usr&#x2F;bin&#x2F;dpkg returned an error code (1)</span><br></pre></td></tr></table></figure></li>
<li><p>解决办法：/var/lib/dpkg/info 目录下将上述出现问题的模块的postinst文件重命名。</p>
<p>在/var/lib/dpkg/info 下写了个脚本</p>
<p>solution.sh</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">for pack in $(cat module.txt)</span><br><span class="line">do </span><br><span class="line">    mv &quot;$pack&quot;.postinst &quot;$pack&quot;.postinst.bak</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<p>其中module.txt的内容为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">udev</span><br><span class="line">snapd</span><br><span class="line">ubuntu-core-launcher</span><br><span class="line">kmod</span><br><span class="line">ubuntu-drivers-common</span><br><span class="line">whoopsie</span><br><span class="line">openssh-server</span><br><span class="line">ssh</span><br><span class="line">avahi-daemon</span><br><span class="line">avahi-utils</span><br><span class="line">unattended-upgrades</span><br></pre></td></tr></table></figure></li>
<li><p>执行脚本后 使用<code>sudo apt-get upgrade</code> 进行更新</p>
</li>
<li><p>参考：</p>
<ol>
<li><a href="https://www.codelast.com/%E5%8E%9F%E5%88%9B-%E8%A7%A3%E5%86%B3ubuntu-%E6%97%A0%E6%B3%95%E7%94%A8-apt-get-install-%E5%AE%89%E8%A3%85%E4%BB%BB%E4%BD%95%E8%BD%AF%E4%BB%B6dpkg-error-processing-package-xxx%E7%9A%84%E9%97%AE/">https://www.codelast.com/%E5%8E%9F%E5%88%9B-%E8%A7%A3%E5%86%B3ubuntu-%E6%97%A0%E6%B3%95%E7%94%A8-apt-get-install-%E5%AE%89%E8%A3%85%E4%BB%BB%E4%BD%95%E8%BD%AF%E4%BB%B6dpkg-error-processing-package-xxx%E7%9A%84%E9%97%AE/</a></li>
<li><a href="https://askubuntu.com/questions/949760/dpkg-warning-files-list-file-for-package-missing">https://askubuntu.com/questions/949760/dpkg-warning-files-list-file-for-package-missing</a></li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title>Make学习</title>
    <url>/make%E5%AD%A6%E4%B9%A0.html</url>
    <content><![CDATA[<h1 id="make学习"><a href="#make学习" class="headerlink" title="make学习"></a>make学习</h1><p>开始阅读redis源码，都说redis很简单，源码不多。但是源码包下载下来后却发现不知道从何处入手，有那么多文件和源码。后面查找资料才发现阅读源码的第一步就是阅读Makefile，项目如何构建和源码间的关联都写在了Makefile文件中。之前没有接触过Makefile，记录下Make的学习。</p>
<a id="more"></a>
<h2 id="makefile的格式"><a href="#makefile的格式" class="headerlink" title="makefile的格式"></a>makefile的格式</h2><ol>
<li><p>概述</p>
<p>makefile 文件由一系列rules组成 rules的格式为：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;target&gt; : &lt;prerequisites&gt; </span><br><span class="line">[tab]  &lt;commands&gt;</span><br></pre></td></tr></table></figure>
<p>​    “目标”是必需的，不可省略；”前置条件”和”命令”都是可选的，但是两者之中必须至少存在一个。 </p>
<p>​    每条规则就明确两件事：构建目标的前置条件是什么，以及如何构建。 </p>
<ol>
<li><p>target</p>
<p>一个目标（target）就构成一条规则。目标通常是文件名，指明Make命令所要构建的对象，比如上文的 a.txt 目标可以是一个文件名，也可以是多个文件名，之间用空格分隔。（make的时候指定文件名从而对该文件进行构建build）</p>
<p>除了文件名，目标还可以是某个操作的名字，这称为”伪目标”（phony target）。伪目标不生成文件，只执行命令。</p>
<p>比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">clean:</span><br><span class="line">      rm *.o</span><br></pre></td></tr></table></figure>
<p>此时执行<code>make clean</code> 命令则会进行<code>rm *.o</code> 的操作。</p>
<p>但是当存在clean这个文件时，那么这个命令不会执行。因为Make发现clean文件已经存在，就认为没有必要重新构建了，就不会执行指定的rm命令。</p>
<p>为了避免这种情况，可以明确声明clean是”伪目标”，写法如下。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.PHONY: clean</span><br><span class="line">clean:</span><br><span class="line">        rm *.o temp</span><br></pre></td></tr></table></figure>
<p>如果Make命令运行时没有指定目标，默认会执行Makefile文件的第一个目标。 </p>
</li>
<li><p>prerequisites</p>
<p>前置条件通常是一组文件名，之间用空格分隔。它指定了”目标”是否重新构建的判断标准：只要有一个前置文件不存在，或者有过更新（前置文件的last-modification时间戳比目标的时间戳新），”目标”就需要重新构建。 </p>
<p>没有前置条件，就意味着它跟其他文件都无关，只要这个target文件还不存在 就需要执行命令构建</p>
<p>如果需要生成多个文件，往往采用下面的写法。 </p>
<p><code>source: file1 file2 file3</code>  </p>
<p>无需加上命令，当三个文件不存在时，执行<code>make source</code>就会生成这三个文件。</p>
</li>
<li><p>commands</p>
<p>命令（commands）表示如何更新目标文件，由一行或多行的Shell命令组成。它是构建”目标”的具体指令，它的运行结果通常就是生成目标文件。 </p>
<p>每行命令之前必须有一个tab键 </p>
<p>需要注意的是，每行命令在一个单独的shell中执行。这些Shell之间没有继承关系。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var-lost:</span><br><span class="line">    export foo&#x3D;bar</span><br><span class="line">    echo &quot;foo&#x3D;[$$foo]&quot;</span><br></pre></td></tr></table></figure>
<p>上面代码执行后（<code>make var-lost</code>），取不到foo的值。因为两行命令在两个不同的进程执行。 </p>
<p>解决办法：</p>
<ol>
<li><p>命令写在同1行</p>
</li>
<li><p>换行符前加反斜杠转义</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">var-kept:</span><br><span class="line">    export foo&#x3D;bar; \</span><br><span class="line">    echo &quot;foo&#x3D;[$$foo]&quot;</span><br></pre></td></tr></table></figure>
<ol>
<li>加上<code>.ONESHELL:</code>命令 </li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.ONESHELL:</span><br><span class="line">var-kept:</span><br><span class="line">    export foo&#x3D;bar; </span><br><span class="line">    echo &quot;foo&#x3D;[$$foo]&quot;</span><br></pre></td></tr></table></figure>
<h2 id="makefile的语法"><a href="#makefile的语法" class="headerlink" title="makefile的语法"></a>makefile的语法</h2></li>
</ol>
</li>
<li><p>注释</p>
<p>井号（#）在Makefile中表示注释。 </p>
</li>
<li><p>回声（echoing）</p>
<p>正常情况下，make会打印每条命令，然后再执行，这就叫做回声（echoing）。</p>
<p>在命令的前面加上@，就可以关闭回声。 </p>
<p>由于在构建过程中，需要了解当前在执行哪条命令，所以通常只在注释和纯显示的echo命令前面加上@。 </p>
</li>
<li><p>通配符</p>
<p>由于在构建过程中，需要了解当前在执行哪条命令，所以通常只在注释和纯显示的echo命令前面加上@。 </p>
</li>
<li><p>模式匹配</p>
<p>Make命令允许对文件名，进行类似正则运算的匹配，主要用到的匹配符是%。比如，假定当前目录下有 f1.c 和 f2.c 两个源码文件，需要将它们编译为对应的对象文件。 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">%.o: %.c</span><br></pre></td></tr></table></figure>
<p>等同于</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">f1.o: f1.c</span><br><span class="line">f2.o: f2.c</span><br></pre></td></tr></table></figure>
<p>使用匹配符%，可以将大量同类型的文件，只用一条规则就完成构建。 </p>
</li>
<li><p>变量和赋值符</p>
<p>Makefile 允许使用等号自定义变量。 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">txt &#x3D; Hello World</span><br><span class="line">test:</span><br><span class="line">    @echo $(txt)</span><br></pre></td></tr></table></figure>
<p>上面代码中，变量 txt 等于 Hello World。调用时，变量需要放在 $( ) 之中 </p>
<p>调用Shell变量，需要在美元符号前，再加一个美元符号，这是因为Make命令会对美元符号转义。 </p>
</li>
<li><p>内置变量</p>
<p>Make命令提供一系列内置变量，比如，$(CC) 指向当前使用的编译器，$(MAKE) 指向当前使用的Make工具。这主要是为了跨平台的兼容性 gmake、cmake、dmake等等。</p>
<p>$(AR) ：函数库打包程序,将对应的gcc编译出来的obj文件打包成静态链接库程序。</p>
<p>ar可以集合许多文件，成为单一的备存文件。在备存文件中，所有成员文件皆保有原来的属性与权限。</p>
</li>
<li><p>自动变量</p>
<ol>
<li><p>$@指代当前目标，就是Make命令当前构建的那个目标  target</p>
</li>
<li><p>$&lt;指代第一个前置条件。比如，规则为 t: p1 p2，那么$&lt; 就指代p1 </p>
</li>
<li><p>$？指代比目标更新的所有前置条件，之间以空格分隔。比如，规则为 t: p1 p2，其中 p2 的时间戳比 t 新，$?就指代p2。 </p>
</li>
<li><p>$^指代所有前置条件，之间以空格分隔。比如，规则为 t: p1 p2，那么 $^ 就指代 p1 p2 。 </p>
</li>
<li><p>$<em>指代匹配符 % 匹配的部分， 比如% 匹配 f1.txt 中的f1 ，$</em> 就表示 f1。 </p>
</li>
<li><p>$(@D) 和 $(@F)$(@D) 和 $(@F) 分别指向 $@ 的目录名和文件名。比如，$@是 src/input.c，那么$(@D) 的值为 src ，$(@F) 的值为 input.c。 </p>
</li>
<li><p>$(&lt;D) 和 $(&lt;F)</p>
<p>$(&lt;D) 和 $(&lt;F) 分别指向 $&lt; 的目录名和文件名。</p>
</li>
</ol>
</li>
<li><p>其他</p>
<ol>
<li><p><code>.DEFAULT：</code>表示找不到匹配规则时，就执行该recipe。  </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">default:all</span><br><span class="line">.DEFAULT:</span><br><span class="line">	commands</span><br></pre></td></tr></table></figure>
<p>这里当执行<code>make default</code> 时会转到<code>make all</code> 因为default：all 这个target没有隐式规则。所以最后会执行commands。</p>
</li>
<li><p>忽略命令的出错，可以在Makefile的命令行前加一个减号”-“(在Tab键之后)，标记为不管命令出不出错都认为是成功的。如：     </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">clean:        </span><br><span class="line">	-(rm -f *.o )</span><br></pre></td></tr></table></figure></li>
<li><code>include filename</code> 将filename中的内容导入，如果找不到会停止make， <code>-include filename</code> 则不会停止make。 </li>
</ol>
</li>
</ol>
<h2 id="几种等号"><a href="#几种等号" class="headerlink" title="几种等号"></a>几种等号</h2><p>= 是最基本的赋值<br>:= 是覆盖之前的值<br>?= 是如果没有被赋值过就赋予等号后面的值<br>+= 是添加等号后面的值</p>
<p>=与:= 的区别</p>
<p> =：make会将整个makefile展开后，再决定变量的值。也就是说，变量的值将会是整个makefile中最后被指定的值。例子为：</p>
<figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">x = foo</span><br><span class="line">y = <span class="variable">$(x)</span> bar</span><br><span class="line">x = xyz</span><br></pre></td></tr></table></figure>
<p>y的值将会是 xyz bar ，而不是 foo bar 。因为展开后最终变成的是xyz</p>
<p>:=表示变量的值决定于它在makefile中的位置，而不是整个makefile展开后的最终值。</p>
<figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">x := foo</span><br><span class="line">y := <span class="variable">$(x)</span> bar</span><br><span class="line">x := xyz</span><br></pre></td></tr></table></figure>
<p>y的值将会是 foo bar ，而不是 xyz bar 了。</p>
<h2 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h2><ol>
<li><a href="http://www.ruanyifeng.com/blog/2015/02/make.html">http://www.ruanyifeng.com/blog/2015/02/make.html</a></li>
<li><a href="https://gist.github.com/isaacs/62a2d1825d04437c6f08">https://gist.github.com/isaacs/62a2d1825d04437c6f08</a> makefile文件教程</li>
<li><a href="https://www.gnu.org/software/make/manual/make.html">https://www.gnu.org/software/make/manual/make.html</a> GNUmake手册</li>
<li><a href="https://blog.csdn.net/shouso888/article/details/7226030">https://blog.csdn.net/shouso888/article/details/7226030</a> 等号解释</li>
</ol>
]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>make</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux命令学习之wc</title>
    <url>/Linux%20%E5%91%BD%E4%BB%A4%20%E5%AD%A6%E4%B9%A0.html</url>
    <content><![CDATA[<h1 id="Linux-命令学习wc命令"><a href="#Linux-命令学习wc命令" class="headerlink" title="Linux 命令学习wc命令"></a>Linux 命令学习wc命令</h1><h2 id="wc命令"><a href="#wc命令" class="headerlink" title="wc命令"></a><code>wc</code>命令</h2><a id="more"></a>
<ol>
<li>作用：Word Count 功能为统计指定文件中的字节数、字数、行数，并将统计结果显示输出。 </li>
<li>格式：<ul>
<li><code>wc [option] filepath</code></li>
</ul>
</li>
<li>参数<ul>
<li><code>-c</code> 统计字节数</li>
<li><code>-l</code> 统计行数</li>
<li><code>-m</code> 统计字符数 标志不能与 -c 标志一起使用。 </li>
<li><code>-w</code> 统计字（单词word）数。一个字被定义为由空白、跳格或换行字符分隔的字符串 </li>
<li><code>-L</code>  打印最长行的长度。 </li>
<li><code>-help</code> 显示帮助信息 </li>
<li><code>--version</code> 显示版本信息 </li>
</ul>
</li>
<li>参考网址：<a href="http://www.cnblogs.com/peida/archive/2012/12/18/2822758.html">http://www.cnblogs.com/peida/archive/2012/12/18/2822758.html</a></li>
</ol>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title>tee命令解析</title>
    <url>/tee%E5%91%BD%E4%BB%A4%E8%A7%A3%E6%9E%90.html</url>
    <content><![CDATA[<h1 id="make-2-gt-amp-1-tee-log-txt-命令解析"><a href="#make-2-gt-amp-1-tee-log-txt-命令解析" class="headerlink" title="make 2&gt;&amp;1 | tee log.txt 命令解析"></a>make 2&gt;&amp;1 | tee log.txt 命令解析</h1><p>在安装mpich 的时候遇到了很多这个命令，此处学习下这个命令：<code>2&gt;&amp;1 | tee log.txt</code> </p>
<a id="more"></a>
<p>这个命令共有三个部分： <code>2&gt;&amp;1</code> <code>|</code>  <code>tee log.txt</code></p>
<h2 id="2-gt-amp-1"><a href="#2-gt-amp-1" class="headerlink" title="2&gt;&amp;1"></a>2&gt;&amp;1</h2><p>shell中：最常使用的 FD (file descriptor) 大概有三个 </p>
<p>0表示标准输入Standard Input (STDIN)  </p>
<p>1表示标准输出Standard Output (STDOUT)  </p>
<p> 2表示标准错误输出 Standard Error Output (STDERR)  </p>
<p>‘&gt;’ 默认为标准输出重定向 （类似于c++ 中的 &gt;&gt;？）</p>
<p>在标准情况下, 这些FD分别跟如下设备关联 </p>
<p>stdin(0): keyboard  键盘输入,并返回在前端   </p>
<p>stdout(1): monitor  正确返回值 输出到前端   </p>
<p>stderr(2): monitor 错误返回值 输出到前端  </p>
<p>1&gt;&amp;2  正确返回值传递给2输出通道 &amp;2表示2输出通道   如果此处错写成 1&gt;2, 就表示把1输出重定向到文件2中  2&gt;&amp;1 错误返回值传递给1输出通道, 同样&amp;1表示1输出通道.  </p>
<h2 id="管道"><a href="#管道" class="headerlink" title="|管道"></a>|管道</h2><p>管道的作用是提供一个通道，将上一个程序的标准输出重定向到下一个程序作为下一个程序的标准输入。 </p>
<h2 id="tee-log-txt"><a href="#tee-log-txt" class="headerlink" title="tee log.txt"></a>tee log.txt</h2><p>tee从标准输入中读取，并将读入的内容写到标准输出以及文件中。  此处将数据读入并写入到log.txt中</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个命令将标准错误输出重定向到标准输出，然后再将标准输出重定向到log.txt文件中</p>
<p>常用于make 后面将log信息保存下来。</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title>BigdataBench deploy</title>
    <url>/BigdataBench-deploy.html</url>
    <content><![CDATA[<h1 id="Bigdatabench-4-0-MPI版本-安装"><a href="#Bigdatabench-4-0-MPI版本-安装" class="headerlink" title="Bigdatabench 4.0 MPI版本 安装"></a>Bigdatabench 4.0 MPI版本 安装</h1><a id="more"></a>
<p>官网上面的指南BigDataBench User Manual有一些错误。</p>
<p>本机环境：</p>
<p>​    Centos 6.9</p>
<p>​    gcc (GCC) 4.8.2 20140120 (Red Hat 4.8.2-15)</p>
<p>​    g++ (GCC) 4.8.2 20140120 (Red Hat 4.8.2-15)</p>
<h2 id="mpi的安装"><a href="#mpi的安装" class="headerlink" title="mpi的安装"></a>mpi的安装</h2><p>这部分网上资料很多，而Manual中有一点错误</p>
<ol>
<li><p>需要保证c 编译器 如gcc c++ 编译器 如：g++</p>
</li>
<li><p>基础安装</p>
<ol>
<li>从官网下载安装包解压</li>
</ol>
<ul>
<li><code>wget http://www.mpich.org/static/downloads/3.2.1/mpich-3.2.1.tar.gz</code>  从官网下载安装包</li>
<li><code>tar -zxvf mpich-3.2.1.tar.gz</code>  解压</li>
<li><code>cd mpich-3.2.1</code></li>
</ul>
<ol>
<li>配置安装目录   本机安装在mpich-install目录下</li>
</ol>
<ul>
<li><code>./configure –prefix=/home/mpich-install 2&gt;&amp;1 | tee c.txt</code> 手册中&amp;被错写为$了 <code>2&gt;&amp;1 | tee c.txt</code> 表示将输出的标准出错信息重定向到c.txt中。</li>
</ul>
<ol>
<li>build</li>
</ol>
<ul>
<li><code>make 2&gt;&amp;1 | tee m.txt</code></li>
</ul>
<ol>
<li>安装</li>
</ol>
<ul>
<li><code>make install 2&gt;&amp;1 | tee mi.txt</code></li>
</ul>
<ol>
<li>将安装目录添加到PATH 环境变量中</li>
</ol>
<ul>
<li><code>vim ~/.bashrc</code></li>
<li><code>export PATH=$PATH:/home/mpich-install/bin</code> 在最后一行添加</li>
<li><code>source ~/.bashrc</code> 重启生效</li>
</ul>
</li>
<li><p>检查</p>
<ol>
<li>检查路径<ul>
<li><code>which mpicc</code> </li>
<li><code>which mpic++</code></li>
</ul>
</li>
</ol>
</li>
<li><p>验证 </p>
<p>在mpich的安装包目录下有提供例子程序运行</p>
<ol>
<li><code>cd mpich-3.2.1/examples</code></li>
<li><code>mpicc cpi.c -o cpi</code> 编译cpi.c程序求pi值</li>
<li><code>mpirun -n 4 ./cpi</code> 使用4个进程 注意<code>./</code>否则报错找不到文件</li>
</ol>
<p>如果是集群环境在每个节点将mpich安装在相同的路径然后编辑一个machine_file （里面是各个节点的host）然后<code>mpirun -f machine_file -n 3 ./cpi</code> 在集群上并行运行</p>
</li>
</ol>
<h2 id="boost-安装"><a href="#boost-安装" class="headerlink" title="boost 安装"></a>boost 安装</h2><p>boost当前最新版本是：1.67 但是BigdataBench用的是1.43版本推荐安装这个旧版本</p>
<ol>
<li><p><code>wget https://sourceforge.net/projects/boost/files/boost/1.43.0/boost_1_43_0.tar.gz/download</code> </p>
</li>
<li><p>若下载下来的文件名为：downloads 则使用mv命令重命名在当前文件目录下:</p>
<p><code>mv downloads boost_1_43_0.tar.gz</code>  </p>
</li>
<li><p>解压<code>tar -zxvf boost_1_43_0.tar.gz</code>  之后<code>cd boost_1_43_0</code></p>
</li>
<li><p><code>sh bootstrap.sh</code>  执行这个命令运行脚本后会多出很多配置文件</p>
</li>
<li><p>使用mpi,这一步骤很重要否则后续cmake时会提示找不到：boost_mpi</p>
<ol>
<li><p>对低版本的boost </p>
<ol>
<li><p><code>which mpic++</code> 找mpich的目录</p>
</li>
<li><p><code>vim tools/build/v2/user-config.jam</code></p>
</li>
<li><p>在最后添加： using mpi:后面是mpich的目录</p>
<p><code>#MPI config</code></p>
<p><code>using mpi : /usr/lib64/mpich/bin/mpic++ ;</code></p>
</li>
</ol>
</li>
<li><p>对高版本的boost直接在boost_1_67_0目录下修改project-config.jam即可</p>
</li>
</ol>
</li>
<li><p><code>./bjam</code> 进行编译</p>
</li>
<li><p><code>./bjam install</code> 这一步是必需的但在手册中没有表明。</p>
</li>
</ol>
<h2 id="BigdataBench的配置"><a href="#BigdataBench的配置" class="headerlink" title="BigdataBench的配置"></a>BigdataBench的配置</h2><p>进入BigDataBench的安装根目录：</p>
<ol>
<li><code>vim conf.properties</code> 添加$JAVA_HOME， $MPI_HOME ，$BigdataBench_HOMEMPI的路径</li>
<li><code>sh prepar.sh</code> </li>
</ol>
<p>至此安装理论上已经成功。但仍然遇到了其他问题</p>
<h2 id="Perminsion-denied问题"><a href="#Perminsion-denied问题" class="headerlink" title="Perminsion denied问题"></a>Perminsion denied问题</h2><p>最开始的安装包是从windows下面考过去的结果生成cc的数据后无法运行执行脚本</p>
<p><img src="/BigdataBench-deploy.htm/runcc.png" alt=""></p>
<p>原因是此时的run_connectedComponents已经不是可执行文件了（不是绿色的）需要<code>chmod a+x run_connectedComponents</code>来将文件的权限修改为可执行文件权限（修改后变为绿色）</p>
<p>后面wget下载后解压配置之后直接就是可执行文件！</p>
<h2 id="ldd-程序-动态链接库缺失"><a href="#ldd-程序-动态链接库缺失" class="headerlink" title="ldd 程序 动态链接库缺失"></a>ldd 程序 动态链接库缺失</h2><p><code>[root@hw073 ConnectedComponent]# ldd run_connectedComponents</code><br><code>linux-vdso.so.1 =&gt;  (0x00007ffdfc8d4000)</code><br><code>librt.so.1 =&gt; /lib64/librt.so.1 (0x0000003156e00000)</code><br><code>libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x0000003156a00000)</code></p>
<p><code>libboost_serialization-mt.so.1.43.0 =&gt; not found</code><br><code>libboost_filesystem-mt.so.1.43.0 =&gt; not found</code><br><code>libboost_system-mt.so.1.43.0 =&gt; not found</code><br><code>libstdc++.so.6 =&gt; /usr/lib64/libstdc++.so.6 (0x0000003162200000)</code><br><code>libm.so.6 =&gt; /lib64/libm.so.6 (0x0000003157200000)</code><br><code>libgcc_s.so.1 =&gt; /lib64/libgcc_s.so.1 (0x0000003161a00000)</code><br><code>libc.so.6 =&gt; /lib64/libc.so.6 (0x0000003156600000)</code><br><code>/lib64/ld-linux-x86-64.so.2 (0x0000003155e00000)</code></p>
<p>最开始以为是没有指定LD_LIBRARY_PATH ，因为明明有这个文件的，后面使用find / -name 命令发现还是找不到，仔细一看ldd 的信息，发现上述文件都多了个-mt</p>
<p>解决办法： 在boost安装时的库。本机：<code>/usr/local/lib</code> 有着及其相似的3个文件<code>libboost_filesystem.so.1.43.0</code> 、<code>libboost_filesystem.so.1.43.0</code> ，<code>libboost_system.so.1.43.0</code> 均少了个-mt，因此将上述三个文件均拷贝一份命名为上述缺少的动态库文件。</p>
<p><code>cd /usr/local/lib</code> #切换到对应的目录下</p>
<p><code>cp libboost_system.so.1.43.0 libboost_system-mt.so.1.43.0</code> #拷贝为对应的文件名</p>
]]></content>
      <categories>
        <category>deploy</category>
      </categories>
      <tags>
        <tag>bigdatabench</tag>
      </tags>
  </entry>
  <entry>
    <title>图计算常用算法</title>
    <url>/%E5%9B%BE%E8%AE%A1%E7%AE%97%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95.html</url>
    <content><![CDATA[<h1 id="图算法的典型操作"><a href="#图算法的典型操作" class="headerlink" title="图算法的典型操作"></a>图算法的典型操作</h1><p>关于一些常见图算法的调研与学习。</p>
<a id="more"></a>
<h2 id="常用图算法"><a href="#常用图算法" class="headerlink" title="常用图算法"></a>常用图算法</h2><h3 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h3><ol>
<li><p>背景    </p>
<ol>
<li>既考虑入链数量，又考虑了网页质量因素，二者相结合 数量与权重的结合</li>
<li>算法与主题无关，因为PR值是根据图计算出来的</li>
</ol>
</li>
<li><p>算法原理</p>
<ol>
<li><p>基本思想</p>
<p>A有链接指向B，表明A认为B比A重要。A将自身权重分配一部分给B。</p>
<p>$W(B)=W(A)/N$   W(A) 是A的PR值，W(B)是A 分配的权重，N是A的出链数</p>
</li>
<li><p>PageRank公式修正</p>
<p>存在出链为0的孤立网页，增加阻力系数q ，一般取q=0.85，其意义是用户有1-q的概率不点击此页面上面的所有链接。同时还有随机直接跳转的概率，如直接输入网址，点击书签等。完整公式如下：</p>
<p><img src="/%E5%9B%BE%E8%AE%A1%E7%AE%97%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95.htm/data\ict\docs\page rank.png" alt=""></p>
</li>
</ol>
</li>
</ol>
<h3 id="Connected-component"><a href="#Connected-component" class="headerlink" title="Connected component"></a>Connected component</h3><ol>
<li>定义<ol>
<li>连通分支：图中，某个子图的任意两点有边连接，而子图之间无边连接</li>
<li>问题：cc是寻找连通分支的算法？？</li>
</ol>
</li>
<li>通过BFS、DFS算法的便利就可以找到连通分支，每个白色节点开始的就是一个连通分支。</li>
<li>常见算法<ol>
<li>DFS<ol>
<li>原理：访问某个顶点后只有当某个节点是叶结点后才会访问其余相邻节点。</li>
<li>步骤：<ol>
<li>选择一个结点作为起始结点，标记为灰色</li>
<li>从该节点的邻居结点中选择一个结点，标记为灰色，继续这个操作</li>
<li>当选中的结点时叶子结点时，将其涂黑并返回到上一个父节点。</li>
<li>重复2,3直到所有结点都被访问。</li>
</ol>
</li>
</ol>
</li>
<li>BFS   （DFS，BFS不是图的遍历算法吗）。<ol>
<li>原理：在进一步遍历中顶点之前，先访问当前结点的所有邻接结点。</li>
<li>步骤：<ol>
<li>选择一个顶点作为起始节点，放入队列，标记为灰色，其余标记为白色</li>
<li>寻找队列首部结点的所有邻居节点，将其放入队列中并标记为灰色，将队列首部结点出队，并标记为黑色  </li>
<li>重复2步骤，直到队列中的节点全部为空。</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="SSSP-single-source-shortest-paths"><a href="#SSSP-single-source-shortest-paths" class="headerlink" title="SSSP (single-source shortest paths)"></a>SSSP (single-source shortest paths)</h3><ol>
<li>单独的起点与目标点之间最短路径的计算。起点固定，寻找与其他所有结点之间的最短路径。包括单源单汇，单源多汇</li>
<li>常见算法<ol>
<li>Dijkstra<ol>
<li>步骤<ol>
<li>将所有顶点分成两个集合A、B，其中集合A表示已经求得从V0出发的最短路径的顶点集合，集合B为为待求解的顶点集合。初始时有A={V0}</li>
<li>将集合A与集合B相连的边（A中的所有结点与B中所有的结点形成的边）按照从V0出发的最短权重和递增次序排序，取最短的边，将该条边在集合B中所对应的顶点加入到集合A中</li>
<li>重复第二步，直至B为空集。</li>
</ol>
</li>
<li>总结：<ol>
<li>最短中的最短：每次迭代时比较的是当前状态下以V0为起点，A中顶点为中间点的到各顶点之间的最短路径权重，最后再选择在当前所有最短路径中路径最短的一个顶点加入A。也就是说每次加入A集合的点是最短路径中的最短。</li>
<li>给定目标点，在每次迭代时，并不知道能否到达最后的目标点，所以把到所有结点的最短距离都算出来了。</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="Betweenness-Centrality（中介中心性）"><a href="#Betweenness-Centrality（中介中心性）" class="headerlink" title="Betweenness Centrality（中介中心性）"></a>Betweenness Centrality（中介中心性）</h3><ol>
<li><p>定义 ：中心性用来衡量节结点的重要性。Betweenness Centrality ：考虑的是该节点出现在其他两节点之间的最短路径上的比率。</p>
</li>
<li><p>思想：如果一个成员位于其他成员的多条最短路上，那么该成员就是核心成员，就具有较大的中介中心性。</p>
</li>
<li><p>步骤</p>
<p>其中<img src="https://i2.wp.com/upload.wikimedia.org/math/b/b/7/bb79bd91213d03edf0c8fc04fbd51085.png" alt="\sigma_{st}">表示的是节点s和t之间的最短路径的数量，而<img src="https://i2.wp.com/upload.wikimedia.org/math/4/2/c/42c33f5f78864175f0d9c9af1c492f5d.png" alt="\sigma_{st}(v)">是最短路径中经过节点v的数量。</p>
<ol>
<li><p>计算各个点对之间最短路径的长度和条数，用于计算pair-dependencies: δst(v) =σst(v)/σst</p>
<p><img src="http://static.oschina.net/uploads/img/201305/11121047_IQnh.jpg" alt="clip_image004"></p>
</li>
<li><p>对于每个节点，累积属于自己的pair-dependencies</p>
</li>
</ol>
<p><img src="https://i2.wp.com/upload.wikimedia.org/math/4/c/c/4cc6eaa2dce9d504feeed5bd88b96d73.png" alt=""></p>
</li>
</ol>
<h3 id="LBP算法-Local-Binary-Pattern-局部二值模式"><a href="#LBP算法-Local-Binary-Pattern-局部二值模式" class="headerlink" title="LBP算法(Local Binary Pattern, 局部二值模式)"></a>LBP算法(Local Binary Pattern, 局部二值模式)</h3><ol>
<li><p>定义：LBP是一种用来描述图像局部纹理特征的算子。</p>
<ol>
<li>原始的LBP算子定义为在3*3的窗口内，以窗口中心像素为阈值，将相邻的8个像素的灰度值与其进行比较，若周围像素值大于中心像素值，则该像素点的位置被标记为1，否则为0</li>
</ol>
<p><img src="http://hi.csdn.net/attachment/201104/13/0_1302700245WllL.gif" alt="img"></p>
</li>
<li><p>作用是进行特征提取，而且，提取的特征是图像的纹理特征，并且，是局部的纹理特征.</p>
</li>
<li><p>改进版本</p>
<ol>
<li>原型LBP算子 </li>
<li>LBP等价模式</li>
</ol>
</li>
</ol>
<h3 id="最小生成树"><a href="#最小生成树" class="headerlink" title="最小生成树"></a>最小生成树</h3><ol>
<li>定义：无环连通图，图中所有结点均参与，所有边的权重加起来最小。</li>
<li>算法<ol>
<li>Prim算法  <ol>
<li>步骤：设N=(V,{E})是连通网， TE是N上最小生成树中边的集合 <ol>
<li>初始令U={u0},(u0V), TE=φ</li>
<li>在所有uU,vV-U的边(u,v)E中，找一条代价最小<br>的边(u0,v0), 并保证不形成回路 </li>
<li>将(u0,v0)并入集合TE，同时v0并入U </li>
<li>重复上述操作直至U=V为止，则T=(V,{TE})为N的<br>最小生成树  </li>
</ol>
</li>
<li>总结：每次迭代加入所有连通边中权值最小的。</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="三角计数"><a href="#三角计数" class="headerlink" title="三角计数"></a>三角计数</h3><ol>
<li>定义：寻找无向图中的所有三角形</li>
<li>步骤<ol>
<li>建立邻接表：<ol>
<li>如果A-B &amp; A &lt; B，则将B加入A的邻接表 如果A-B &amp; B &lt; A，则将A加入B的邻接表  A&lt;B比较的是id</li>
</ol>
</li>
<li>遍历每个节点，对于结点A，遍历A邻接表中的结点，如果邻接结点B,C两两之间存在边，则A、B、C三者之间存在三角形</li>
</ol>
</li>
</ol>
<h3 id="社区发现"><a href="#社区发现" class="headerlink" title="社区发现"></a>社区发现</h3><ol>
<li><p>社区定义：同一社区内的节点与节点之间的连接很紧密，而社区与社区之间的连接比较稀疏。社区是一个子图</p>
</li>
<li><p>数学描述：<img src="https://img-blog.csdn.net/20130710080910046" alt=""></p>
</li>
<li><p>衡量标准：模块度</p>
<ol>
<li>计算公式</li>
</ol>
<p><img src="https://img-blog.csdn.net/20130710081032203" alt=""></p>
</li>
<li><p>常见算法</p>
<ol>
<li>GN算法<ol>
<li>思想：在一个网络之中，通过社区内部的边的最短路径相对较少，而通过社区之间的边的最短路径的数目则相对较多。从社区内部走大概率会走很多条边。</li>
<li>步骤<ol>
<li>计算每一条边的边介数。边介数（betweenness）：网络中任意两个节点通过此边的最短路径的数目。</li>
<li>删除边介数最大的边</li>
<li>重复（1）（2），直到网络中的任一顶点作为一个社区为止。</li>
</ol>
</li>
<li>缺陷<ol>
<li>不知道最后会有多少个社区</li>
<li>在计算边介数的时候可能会有很对重复计算最短路径的情况，时间复杂度太高</li>
<li>GN算法不能判断算法终止位置</li>
</ol>
</li>
</ol>
</li>
<li>LPA算法（标签传播算法）<ol>
<li>思路<ol>
<li>自己是什么标签，由邻居决定。邻居中什么标签最多，则此结点是什么标签</li>
</ol>
</li>
<li>步骤<ol>
<li>为所有结点指定一个唯一的标签</li>
<li>逐轮刷新所有结点的标签，直到达到收敛要求位置。刷新规则： 对于某一个节点，考察其所有邻居节点的标签，并进行统计，将出现个数最多的那个标签赋给当前节点。当个数最多的标签不唯一时，随机选一个。</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h3><ol>
<li>定义 ：<strong>拓扑排序（Topological Sorting）</strong>是一个<strong>有向无环图（DAG, Directed Acyclic Graph）</strong>的所有顶点的线性序列。且该序列必须满足下面两个条件：<ol>
<li>每个顶点出现且只出现一次</li>
<li>若存在一条从顶点 A 到顶点 B 的路径，那么在序列中顶点 A 出现在顶点 B 的前面</li>
</ol>
</li>
<li>步骤<ol>
<li>从 DAG 图中选择一个 没有前驱（即入度为0）的顶点并输出</li>
<li>从图中删除该顶点和所有以它为起点的有向边</li>
<li>重复 1 和 2 直到当前的 DAG 图为空或<strong>当前图中不存在无前驱的顶点为止</strong>。后一种情况说明有向图中必然存在环<br><img src="http://img.blog.csdn.net/20150507001759702" alt="img"></li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title>ceph 部署文档</title>
    <url>/ceph%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3.html</url>
    <content><![CDATA[<h1 id="ceph-部署文档"><a href="#ceph-部署文档" class="headerlink" title="ceph 部署文档"></a>ceph 部署文档</h1><a id="more"></a>
<hr>
<h1 id="1-配置所有节点"><a href="#1-配置所有节点" class="headerlink" title="1.配置所有节点"></a>1.配置所有节点</h1><h2 id="创建ceph用户"><a href="#创建ceph用户" class="headerlink" title="创建ceph用户"></a>创建ceph用户</h2><h2 id="安装配置NTP"><a href="#安装配置NTP" class="headerlink" title="安装配置NTP"></a>安装配置NTP</h2><ol>
<li><code>systemctl enable ntp</code>  ubuntu 14.04不可用，感觉已经安装过了，因此跳过。</li>
</ol>
<h2 id="配置hosts文件"><a href="#配置hosts文件" class="headerlink" title="配置hosts文件"></a>配置hosts文件</h2><p><code>172.16.1.93 object1</code><br><code>172.16.1.94 object2</code><br><code>172.16.1.95 object3</code><br><code>172.16.1.66 object4</code><br><code>172.16.1.92 controller</code></p>
<hr>
<h1 id="2-配置ssh服务器"><a href="#2-配置ssh服务器" class="headerlink" title="2. 配置ssh服务器"></a>2. 配置ssh服务器</h1><p>修改ssh的配置文件</p>
<p>Host controller<br>        Hostname gd92<br>        User cephuser<br>Host object1<br>        Hostname gd93<br>        User cephuser<br>Host object2<br>        Hostname hw101<br>        User cephuser<br>Host object3<br>        Hostname gd95<br>        User cephuser<br>Host object4<br>        Hostname gd66<br>        User cephuser</p>
<p>生成密钥并拷贝到4个osd节点上，无需拷贝到controller节点</p>
<hr>
<h1 id="3-安装ceph"><a href="#3-安装ceph" class="headerlink" title="3.安装ceph"></a>3.安装ceph</h1><p>主要参考链接：这些链接的操作大都一致，部分的顺序会有变化。</p>
<p><a href="https://linux.cn/article-8182-1.html#4_10238">https://linux.cn/article-8182-1.html#4_10238</a></p>
<p><a href="https://blog.csdn.net/styshoo/article/details/55471132">https://blog.csdn.net/styshoo/article/details/55471132</a></p>
<p><a href="https://blog.csdn.net/styshoo/article/details/58572816">https://blog.csdn.net/styshoo/article/details/58572816</a></p>
<h2 id="部署监控节点出现的问题"><a href="#部署监控节点出现的问题" class="headerlink" title="部署监控节点出现的问题"></a>部署监控节点出现的问题</h2><p><code>ceph-deploy mon create-initial</code></p>
<ol>
<li><p><code>ceph-mon --cluster ceph --mkfs -i gd92 --keyring /var/lib/ceph/tmp/ceph-gd92.mon.keyring</code></p>
<p>问题：ceph.conf的配置文件中的<code>public network=172.16.1.92/24</code> 掩码前面多打了空格</p>
<p>修改后重新执行命令，并加上<code>--overwrite-conf</code> </p>
</li>
<li><p>[info]Running command: ceph —cluster=ceph —admin-daemon /var/run/ceph/ceph-mon.controller.asok mon_status</p>
<p><code>admin_socket: exception getting command descriptions: [Errno 2] No such file or directory</code></p>
<p>似乎是ceph -deploy 的问题，或者是ubuntu14.04的问题。教程是ubuntu16.04的</p>
<p>此问题非hostname 不对应</p>
<p>非conf 不同步导致。—overwrtie-conf  无作用。</p>
<p>解决办法：按照14.04方法重新安装ceph-deploy</p>
</li>
</ol>
<h2 id="部署osd节点出现的问题"><a href="#部署osd节点出现的问题" class="headerlink" title="部署osd节点出现的问题"></a>部署osd节点出现的问题</h2><ol>
<li>使用<code>ceph-deploy disk list ceph-osd1 ceph-osd2 ceph-osd3</code>检查磁盘可用性时报错，使用<code>ceph-deploy osd prepare ceph-osd1:/dev/sdb ceph-osd2:/dev/sdb ceph-osd3:/dev/sdb</code> 在数据盘上面准备时也报错<br>Running command: fdisk -l  File “/usr/lib/python2.7/distpackages/ceph_deploy/util/decorators.py”, line 69, in newfunc<br>问题：未知<br>解决办法：将osd节点的数据目录放在指定目录，不用整个数据盘</li>
<li>最后部署后集群状况是health -ok，但是4osds，有3个osd up，一个osd down<br>问题：down掉的节点磁盘有问题。<br>解决办法：先卸载磁盘，重新格式化，挂载，重新激活osd节点</li>
</ol>
<h2 id="部署rgw节点出现的问题"><a href="#部署rgw节点出现的问题" class="headerlink" title="部署rgw节点出现的问题"></a>部署rgw节点出现的问题</h2><ol>
<li><p>显示rgw进程在工作，但是使用：<a href="http://controller:7480">http://controller:7480</a> 显示拒绝连接。并且新建S3账号，测试时未返回正确结果。</p>
<p>问题：未知</p>
<p>尝试方法：重新部署</p>
<p>解决办法：重新部署后最开始将端口设置为80，发现可以创建s3账号，但是无法正确测试，显示创建bucket出错，查看rgw的log，发现端口被占用，无法打开，后面重新设置端口为7480问题解决，测试均正确。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>deploy</category>
      </categories>
      <tags>
        <tag>ceph</tag>
      </tags>
  </entry>
</search>

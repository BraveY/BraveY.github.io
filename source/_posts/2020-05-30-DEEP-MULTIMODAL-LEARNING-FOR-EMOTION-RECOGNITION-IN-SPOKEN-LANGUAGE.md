---
title: DEEP MULTIMODAL LEARNING FOR EMOTION RECOGNITION IN SPOKEN LANGUAGE
date: 2020-05-30 14:51:30
categories: 论文阅读
tags:
- 情感识别
copyright: true
---

## 关键词

情感识别，口语，多模态学习

## 摘要

提出了一种新颖的深度多模态框架，用于基于句子级的口头语言来预测人类情感。本文的架构具有两个鲜明的特点。**首先**，它通过混合的深度多模态结构**从文本和音频中提取高级特征**，该结构考虑了文本的空间信息，音频的时间信息以及低层次的人工特征的高级关联。
其次，本文使用三层深度神经网络融合所有特征，以学习各种模态之间的相关性，并将特征提取和融合模块一起训练，从而实现整个结构的最佳全局微调。
在IEMOCAP数据集上评估了提出的框架。五个情感类别的加权准确率达到60.4％。

## 结论

混合的深度框架，包括卷积网络，CNN-LSTM和DNN来从文本和音频中提取空间和时间信息以及升学特征。

## 引言

人类的言语表达了内容和态度。通过语音交流时，人们自然会同时吸收内容和情感，以理解说话者的实际意图。情感识别被定义为从人类中提取一组情感状态，对于在人机交互中自动检测人类的意义是必需的。语音情感识别是情感计算领域中的一种，它从**语音中提取情感状态**，并揭示口头语言下的态度。

和计算机视觉领域的情感识别相比，结合文本和音频形式的**工作相对较少**。为了检测话语中的情绪，人们经常同时考虑文字含义和韵律。因此使用**多模态架构**来考虑文本和音频输入是有必要的。

### **挑战与相关工作**

情感识别**挑战1**是从语音数据中提取有效特征。可以通过OpenSmile软件提取数千个具有功能统计信息的低级声学描述符和派生词（LLD），提取出来的低层次特征很难代表高级关联，被认为不足以区分情感。可以使用CNN从Word embedding中提取高级文本特征以表示文本特征,但是没有考虑时序关联。

**挑战2**多模态的融合，有两种融合策略：特征层融合以及决策层融合。特征层融合在决策之前合并单个特征表示，从而显着提高了性能。然而，直接将连接的特征馈送到分类器中或使用浅层融合模型，这些模型难以学习不同模态之间的复杂相互关系。由三个受限玻尔兹曼机器层组成的深度置信网络通过融合高级视听功能，比浅层融合模型具有更好的性能，但是这种方法分离了特征提取和特征融合的训练阶段。这种方法的最大问题是它不能保证参数的全局调整，即不能通过预测的损失来调整特征提取模块的参数。

### 本文工作

本文建立了一个混合的深度模型结构。使用了CNN来从单词和词类标签中提取文本特征，CNN-LSTM架构从Mel频谱系数（MFSC）能量图捕获声学的时空特征，而三层深度神经网络可从低级手工特征中学习高级声学关联。然后，本文使用三层深度神经网络**将所有提取的特征连接**起来，以学习跨模态的相互关系，并通过softmax分类器对情绪进行分类。
本文直接将特征提取模块和融合模型一起训练，以便最终损失可以适当地用于调整所有参数。
所提出的结构在IEMOCAP多模态数据集上针对五种情绪达到了60.4％的加权准确度。

------

## 方法

网络的的整体架构：

![](C:\Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-05-30 162811.jpg)

框架由三个模块组成：数据预处理，特征提取和特征融合。

数据预处理模块处理输入的语音流，并输出相应的文本语句，词性标签，音频信号和提取的低级手工声学特征。

特征提取层从单词，词类标签，语言型号，低层次人工特征四个分支中各自提取特征。

特征融合模块：将输出特征串联起来作为一个联合特征表示，并通过一个深度神经网络学习相互关系。

### 数据预处理

将输入语音流分为句子级文本和相应的音频片段，使用NLTK来提取每一句的词类标签POS，删除了标点符号，对语言信号同时使用OpenSmile提取低水平的音调和人声相关特征。基本频率，与音调/能量相关的特征，零交叉率（ZCR），抖动，微光，梅尔频率倒谱系数（MFCC），一些统计学特征：平坦度，偏度，四分位数，标准差，均方根,总共6382维的特征。

### 特征提取

**Word embedding**：使用Word2vec（从谷歌新闻预训练），zero padding填充，一个卷积层后一个最大池化层，使用多种尺寸的卷积核（2,3,4,5的尺寸），每种尺寸都有256个卷积核，最终的特征向量1024维。

**POS embedding**：使用自己的POS标签数据进行训练。将POS编码为10维，然后卷积架构同Word embedding，最终也是1024维

**语言信号**：

![](C:\Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-05-31 101851.jpg)

首先提取MFSC系数，使用64个滤波器组来提取MFSC，并提取了增量系数和双增量系数。选择64作为上下文窗口大小，选择15帧作为移动窗口以分割整个MFSC图。4维度的MFSC图$n×64×64×3$ n是窗口数量。8层的ConvNet，来提取MFSC图中的**空间关联**，4个卷积层，4个最大池化层。卷积层的卷积核尺寸$3*3$，池化层的尺寸$2*2$,之后使用全连接层与Dense 层来将特征关联，最后在使用LSTM来提取**时序关联**，选择最后一层LSTM的隐藏层状态作为特征向量，1024维。

**韵律和声音质量的低级特征**

使用三层的神经网络来提取低级特征的关联，使用了Max-min normalization，输入层6382维，隐藏层2048和1024.

### 特征融合

3层神经网络，一个softmax层，进行融合并分类。2个隐藏层维度2048和1024.使用线性SVM来替换Softmax没有明显的提升。

### 网络训练

直接将特征提取和融合模块一起训练。使用ReLU激活函数，以及Dropout来防止过拟合。lr初始为0.01，Adam优化函数，分类交叉熵损失函数。

另一个问题是**内部协变量偏移**，内部协变量偏移定义为由于训练期间网络参数的变化而导致的网络激活分布的变化，在每层中使用batch normalization来更好地学习分布，提高训练效率。

## 结果

### 数据集

在Interactive Emotional Dyadic Motion Capture Database (IEMOCAP)实验，只考虑音频和文本数据。3个注释者对句子情感打标签，包括快乐，悲伤，中立，愤怒，惊讶，激动，沮丧，厌恶，恐惧和其他。仅使用带有至少两个同意的情感标签的句子。最终合并为**4个**：1213 Hap, 1032 Sad (sad), 1084 Ang (anger), 774 Neu (neutral), and 1136 Fru (frustration). 5折交叉验证。

文章各种方法单独使用的精确度比较：

![](C:\Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-05-31 105359.jpg)

本文方法与其他人的工作比较：

![](C:\Users\BraveY\Documents\BraveY\blog\images\论文阅读\批注 2020-05-31 105551.jpg)

------

## 思考

1. 特征那么多是否需要特征降维。
2. Dense Layer 和FNN 的区别？
3. 只有3个注释者来打标签，标签是否一定合理？



